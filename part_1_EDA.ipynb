{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\logo.png\" alt=\"drawing\" style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  \n",
    "\n",
    "<span style=\"background-size: 600px;background:White;color:REd;font-size: 60px;font-family: Comic Sans MS\">Кредитный скоринг Альфа банка</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Задача</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "Кредитный скоринг – важнейшая банковская задача. Стандартным подходом к ее решению   \n",
    "является построение классических моделей машинного обучения, таких как логистическая   \n",
    "регрессия и градиентный бустинг, на табличных данных, в том числе используя агрегации  \n",
    "от каких-нибудь последовательных данных, например, транзакционных историй клиентов.   \n",
    "Альтернативный подход заключается в использовании последовательных данных “как есть”,   \n",
    "подавая их на вход рекуррентной нейронной сети.\n",
    "\n",
    "В этом соревновании участникам предлагается решить задачу кредитного скоринга клиентов   \n",
    "Альфа-Банка, используя только данные кредитных историй. [Источник](https://www.kaggle.com/competitions/alfa-bank-pd-credit-history)\n",
    "\n",
    "**Данные**\n",
    "\n",
    "Датасет соревнования устроен таким образом, что кредиты для тренировочной выборки взяты   \n",
    "за период в М месяцев, а кредиты для тестовой выборки взяты за последующие K месяцев.\n",
    "\n",
    "Каждая запись кредитной истории содержит самую разнообразную информацию о прошлом кредите   \n",
    "клиента, например, сумму, отношение клиента к кредиту, дату открытия и закрытия, информацию   \n",
    "о просрочках по платежам и др. Все публикуемые данные тщательно анонимизированы.\n",
    "\n",
    "Целевая переменная – бинарная величина, принимающая значения 0 и 1, где 1 соответствует   \n",
    "дефолту клиента по кредиту.\n",
    "\n",
    "\n",
    "**Проверка решений**\n",
    "\n",
    "Метрика соревнования – ROC AUC. Подробнее про метрику можно почитать, например, [здесь](https://dyakonov.org/2017/07/28/auc-roc-площадь-под-кривой-ошибок/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Используемые библиотеки</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# работа с регулярными выражениями\n",
    "import re\n",
    "\n",
    "# библиотеки для работы с табличными данными\n",
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "# генерация случайных чисел\n",
    "import random\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# библиотеки для построения графики\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# библиотеки для математических преобразований с массивами данных\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# библиотеки для работы с функциями(частичная передача аргументов в функцию)\n",
    "from functools import partial\n",
    "\n",
    "# библиотеки для работы со статистическими характеристиками\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# библиотеки для работы с pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Импортируем DBSCAN-кластеризацию\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# вставить картинку в Jupiter Notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# линейные модели машинного обучения\n",
    "from sklearn import linear_model\n",
    "\n",
    "# ансамбли моделей машинного обучения\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# поиск гиперпараметров модели\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "import optuna\n",
    "\n",
    " # метрики\n",
    "from sklearn import metrics\n",
    "\n",
    "# библиотека для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# сохранить полученные модели\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "# сборщик мусора\n",
    "import gc\n",
    "\n",
    "# для ограничения времени выполнения функции\n",
    "import signal\n",
    "import func_timeout\n",
    "\n",
    "# для отслеживания времени выполнения функции\n",
    "import time\n",
    "\n",
    "# очистить output\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Разведывательный анализ данных (EDA-Exploratory Data Analysis)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Исходные данные</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузим данные о кредитной истории клиентов\n",
    "td0 = pd.read_parquet('data_source/train_data/train_data_0.pq')\n",
    "td1 = pd.read_parquet('data_source/train_data/train_data_1.pq')\n",
    "td2 = pd.read_parquet('data_source/train_data/train_data_2.pq')\n",
    "td3 = pd.read_parquet('data_source/train_data/train_data_3.pq')\n",
    "td4 = pd.read_parquet('data_source/train_data/train_data_4.pq')\n",
    "td5 = pd.read_parquet('data_source/train_data/train_data_5.pq')\n",
    "td6 = pd.read_parquet('data_source/train_data/train_data_6.pq')\n",
    "td7 = pd.read_parquet('data_source/train_data/train_data_7.pq')\n",
    "td8 = pd.read_parquet('data_source/train_data/train_data_8.pq')\n",
    "td9 = pd.read_parquet('data_source/train_data/train_data_9.pq')\n",
    "td10 = pd.read_parquet('data_source/train_data/train_data_10.pq')\n",
    "td11 = pd.read_parquet('data_source/train_data/train_data_11.pq')\n",
    "# Обьединим данные в один датасет\n",
    "train_data = pd.concat([td0,td1,td2,td3,td4,td5,td6,td7,td8,td9,td10,td11],ignore_index=True)\n",
    "\n",
    "# Подгрузим данные о дефлоте клиента\n",
    "target = pd.read_csv('data_source/train_data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# очистим память от ненужных данных\n",
    "del td0,td1,td2,td3,td4,td5,td6,td7,td8,td9,td10,td11\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26162717, 61)\n",
      "(3000000, 2)\n"
     ]
    }
   ],
   "source": [
    "# посмотрим совпадают ли размерности датасетов\n",
    "print(train_data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rn</th>\n",
       "      <th>pre_since_opened</th>\n",
       "      <th>pre_since_confirmed</th>\n",
       "      <th>pre_pterm</th>\n",
       "      <th>pre_fterm</th>\n",
       "      <th>pre_till_pclose</th>\n",
       "      <th>pre_till_fclose</th>\n",
       "      <th>pre_loans_credit_limit</th>\n",
       "      <th>pre_loans_next_pay_summ</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_paym_21</th>\n",
       "      <th>enc_paym_22</th>\n",
       "      <th>enc_paym_23</th>\n",
       "      <th>enc_paym_24</th>\n",
       "      <th>enc_loans_account_holder_type</th>\n",
       "      <th>enc_loans_credit_status</th>\n",
       "      <th>enc_loans_credit_type</th>\n",
       "      <th>enc_loans_account_cur</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  rn  pre_since_opened  pre_since_confirmed  pre_pterm  pre_fterm  \\\n",
       "0    0   1                18                    9          2          3   \n",
       "1    0   2                18                    9         14         14   \n",
       "2    0   3                18                    9          4          8   \n",
       "3    0   4                 4                    1          9         12   \n",
       "4    0   5                 5                   12         15          2   \n",
       "5    0   6                 5                    0         11          8   \n",
       "6    0   7                 3                    9          1          2   \n",
       "7    0   8                 2                    9          2          3   \n",
       "8    0   9                 1                    9         11         13   \n",
       "9    0  10                 7                    9          2         10   \n",
       "10   1   1                 8                    7         12         15   \n",
       "\n",
       "    pre_till_pclose  pre_till_fclose  pre_loans_credit_limit  \\\n",
       "0                16               10                      11   \n",
       "1                12               12                       0   \n",
       "2                 1               11                      11   \n",
       "3                16                7                      12   \n",
       "4                11               12                      10   \n",
       "5                12               11                       4   \n",
       "6                12               14                      15   \n",
       "7                12               14                      15   \n",
       "8                14                8                       2   \n",
       "9                 8                8                      16   \n",
       "10                9                1                       1   \n",
       "\n",
       "    pre_loans_next_pay_summ  ...  enc_paym_21  enc_paym_22  enc_paym_23  \\\n",
       "0                         3  ...            3            3            3   \n",
       "1                         3  ...            0            0            0   \n",
       "2                         0  ...            0            0            0   \n",
       "3                         2  ...            3            3            3   \n",
       "4                         2  ...            3            3            3   \n",
       "5                         2  ...            3            3            3   \n",
       "6                         5  ...            3            3            3   \n",
       "7                         5  ...            3            3            3   \n",
       "8                         5  ...            3            3            3   \n",
       "9                         4  ...            3            3            3   \n",
       "10                        2  ...            0            3            3   \n",
       "\n",
       "    enc_paym_24  enc_loans_account_holder_type  enc_loans_credit_status  \\\n",
       "0             4                              1                        3   \n",
       "1             4                              1                        3   \n",
       "2             4                              1                        2   \n",
       "3             4                              1                        3   \n",
       "4             4                              1                        3   \n",
       "5             4                              1                        2   \n",
       "6             4                              1                        3   \n",
       "7             4                              1                        3   \n",
       "8             4                              1                        2   \n",
       "9             4                              1                        2   \n",
       "10            4                              1                        3   \n",
       "\n",
       "    enc_loans_credit_type  enc_loans_account_cur  pclose_flag  fclose_flag  \n",
       "0                       4                      1            0            0  \n",
       "1                       4                      1            0            0  \n",
       "2                       3                      1            1            1  \n",
       "3                       1                      1            0            0  \n",
       "4                       4                      1            0            0  \n",
       "5                       3                      1            0            1  \n",
       "6                       4                      1            0            0  \n",
       "7                       4                      1            0            0  \n",
       "8                       4                      1            0            0  \n",
       "9                       4                      1            0            0  \n",
       "10                      4                      1            0            0  \n",
       "\n",
       "[11 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрм на данные о кредитной истории клиентов\n",
    "train_data.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'rn', 'pre_since_opened', 'pre_since_confirmed', 'pre_pterm',\n",
      "       'pre_fterm', 'pre_till_pclose', 'pre_till_fclose',\n",
      "       'pre_loans_credit_limit', 'pre_loans_next_pay_summ',\n",
      "       'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
      "       'pre_loans_max_overdue_sum', 'pre_loans_credit_cost_rate', 'pre_loans5',\n",
      "       'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_loans90',\n",
      "       'is_zero_loans5', 'is_zero_loans530', 'is_zero_loans3060',\n",
      "       'is_zero_loans6090', 'is_zero_loans90', 'pre_util', 'pre_over2limit',\n",
      "       'pre_maxover2limit', 'is_zero_util', 'is_zero_over2limit',\n",
      "       'is_zero_maxover2limit', 'enc_paym_0', 'enc_paym_1', 'enc_paym_2',\n",
      "       'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
      "       'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12',\n",
      "       'enc_paym_13', 'enc_paym_14', 'enc_paym_15', 'enc_paym_16',\n",
      "       'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20',\n",
      "       'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24',\n",
      "       'enc_loans_account_holder_type', 'enc_loans_credit_status',\n",
      "       'enc_loans_credit_type', 'enc_loans_account_cur', 'pclose_flag',\n",
      "       'fclose_flag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\t-\tИдентификатор заявки(id -клиента).  \n",
    "rn\t-\tПорядковый номер кредитного продукта в кредитной истории. Большему номеру соответствует продукт с более поздней датой открытия.\t  \n",
    "\n",
    "**date features:**  \n",
    "1. pre_since_opened\t-\tДней с даты открытия кредита до даты сбора данных (бинаризовано**)\t  \n",
    "2. pre_since_confirmed\t-\tДней с даты подтверждения информации по кредиту до даты сбора данных (бинаризовано**)\t  \n",
    "3. pre_pterm\t-\tПлановое количество дней с даты открытия кредита до даты закрытия (бинаризовано**)\t  \n",
    "4. pre_fterm\t-\tФактическое количество дней с даты открытия кредита до даты закрытия (бинаризовано**)\t  \n",
    "5. pre_till_pclose\t-\tПлановое количество дней с даты сбора данных до даты закрытия кредита (бинаризовано**)\t  \n",
    "6. pre_till_fclose\t-\tФактическое количество дней с даты сбора данных до даты закрытия кредита (бинаризовано**)\n",
    "7. pclose_flag\t-\tФлаг: плановое количество дней с даты открытия кредита до даты закрытия не определено \t  \n",
    "8. fclose_flag\t-\tФлаг: фактическое количество дней с даты открытия кредита до даты закрытия не определено\n",
    "\n",
    "**late payments features:**  \n",
    "1. pre_loans5\t-\tЧисло просрочек до 5 дней (бинаризовано**)\t  \n",
    "2. pre_loans530\t-\tЧисло просрочек от 5 до 30 дней (бинаризовано**)\t  \n",
    "3. pre_loans3060\t-\tЧисло просрочек от 30 до 60 дней (бинаризовано**)\t  \n",
    "4. pre_loans6090\t-\tЧисло просрочек от 60 до 90 дней (бинаризовано**)\t  \n",
    "5. pre_loans90\t-\tЧисло просрочек более, чем на 90 дней (бинаризовано**)\t  \n",
    "6. is_zero_loans5\t-\tФлаг: нет просрочек до 5 дней\t  \n",
    "7. is_zero_loans530\t-\tФлаг: нет просрочек от 5 до 30 дней\t  \n",
    "8. is_zero_loans3060\t-\tФлаг: нет просрочек от 30 до 60 дней\t  \n",
    "9. is_zero_loans6090\t-\tФлаг: нет просрочек от 60 до 90 дней\t  \n",
    "10. is_zero_loans90\t-\tФлаг: нет просрочек более, чем на 90 дней\t  \n",
    "11. pre_loans_total_overdue\t-\tТекущая просроченная задолженность (бинаризовано**)\t  \n",
    "12. pre_loans_max_overdue_sum\t-\tМаксимальная просроченная задолженность (бинаризовано**)\t\n",
    "\n",
    "\n",
    "**credit features:** \n",
    "1. pre_loans_credit_limit\t-\tКредитный лимит (бинаризовано**)\t  \n",
    "2. pre_loans_next_pay_summ\t-\tСумма следующего платежа по кредиту (бинаризовано**)\t  \n",
    "3. pre_loans_outstanding\t-\tОставшаяся невыплаченная сумма кредита (бинаризовано**)\t  \n",
    "4. pre_loans_credit_cost_rate\t-\tПолная стоимость кредита (бинаризовано**)\t  \n",
    "  \n",
    "\n",
    "**relative features:**  \n",
    "1. pre_util\t-\tОтношение оставшейся невыплаченной суммы кредита к кредитному лимиту (бинаризовано**)\t  \n",
    "2. pre_over2limit\t-\tОтношение текущей просроченной задолженности к кредитному лимиту (бинаризовано**)\t  \n",
    "3. pre_maxover2limit\t-\tОтношенение максимальной просроченной задолженности к кредитному лимиту (бинаризовано**)\t  \n",
    "4. is_zero_util\t-\tФлаг: отношение оставшейся невыплаченной суммы кредита к кредитному лимиту равняется 0\t  \n",
    "5. is_zero_over2limit\t-\tФлаг: отношение текущей просроченной задолженности к кредитному лимиту равняется 0\t  \n",
    "6. is_zero_maxover2limit\t-\tФлаг: отношение максимальной просроченной задолженности к кредитному лимиту равняется 0\t  \n",
    "\n",
    "**payments features:**   \n",
    "1. enc_paym_{0..N}\t-\tСтатусы ежемесячных платежей за последние N месяцев (закодировано***)\t  \n",
    "\n",
    "**service features:**  \n",
    "1. enc_loans_account_holder_type\t-\tТип отношения к кредиту (закодировано***)\t  \n",
    "2. enc_loans_credit_status\t-\tСтатус кредита (закодировано***)\t  \n",
    "3. enc_loans_account_cur\t-\tВалюта кредита (закодировано***)\t  \n",
    "4. enc_loans_credit_type\t-\tТип кредита (закодировано***)\t  \n",
    "\n",
    "  ** область значений поля разбивается на N непересекающихся промежутков, каждому промежутку случайным образом ставится в соответствие уникальный номер от 0 до N-1, значение поля заменяется номером промежутка, которому оно принадлежит  \n",
    "  *** каждому уникальному значению поля случайным образом ставится в соответствие уникальный номер от 0 до K, значение поля заменяется номером этого значения\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>2999995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>2999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>2999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>2999998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>2999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  flag\n",
       "0              0     0\n",
       "1              1     0\n",
       "2              2     0\n",
       "3              3     0\n",
       "4              4     0\n",
       "...          ...   ...\n",
       "2999995  2999995     0\n",
       "2999996  2999996     0\n",
       "2999997  2999997     0\n",
       "2999998  2999998     0\n",
       "2999999  2999999     0\n",
       "\n",
       "[3000000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на данные о дефолте клиента\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\t-\tИдентификатор заявки(id -клиента).   \n",
    "flag – Целевая переменная, 1 – факт ухода в дефолт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flag\n",
       "0    0.964519\n",
       "1    0.035481\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим целевую переменную на сбалансированность\n",
    "target['flag'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flag\n",
       "0    2893558\n",
       "1     106442\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тоже в абсолютных значениях\n",
    "target['flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Анализ данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что данные представлены в виде кредитной истории клиента.  \n",
    "У каждого клиента своя \"продолжительность\" кредитной истории.  \n",
    "Посмотрим \"каких\" историй больше для каждого типа клиентов (дефолт/не дефолт)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим рабочую плоскость на котором можно разместить две диаграммы одну под другой с общей осью абсцисс\n",
    "fig_1 = make_subplots(\n",
    "    rows=2, # количество диаграмм по вертикали\n",
    "    cols=1, # количество диаграмм по горизонтали\n",
    "    shared_xaxes=True, # использовать одну ось х для двух диаграмм\n",
    "    vertical_spacing = 0.05, # уменьшим расстояние между диаграммами\n",
    "    x_title= 'Количество клиетских операций',\n",
    ")\n",
    "# Настроем оформление рабочей поверхности\n",
    "fig_1.update_layout(\n",
    "    title ={\n",
    "        'text':'Распределение количества клиентских операций (клиент без дефолта)', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height = 800,# Высота рабочей плоскости\n",
    "    width = 1400, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    template='simple_white', # зададим тему оформления для рабочей поверхности \n",
    "    )\n",
    "\n",
    "# Добавим на рабочую плоскость коробчатую диаграмму\n",
    "fig_1.add_box(\n",
    "    x=train_data.groupby('id')['rn'].count().iloc[target[target['flag']==0].index], # Выбираем данные по оси х\n",
    "    y0=0, # Устанавливаем значение в нуле по оси y\n",
    "    row=1, col=1, # Выбираем в какой координатной плоскости будет строиться диаграмма\n",
    "    showlegend=False, # убираем легенду (обе диаграммы представляют распределение одной величины)\n",
    "    boxmean=True, # добавим на коробчатую диаграмму отметку среднего значения заработной платы\n",
    "    name= '', # при наведение на значение не будет отображаться имя диаграммы\n",
    "    hoverinfo='x' # при наведение отражается только значение заработной платы\n",
    ") \n",
    "\n",
    "# Настроим отображение оси y для коробчатой диаграммы\n",
    "fig_1.update_yaxes(\n",
    "    title={'text':'Коробчатая диаграмма', # добавим название оси \"y\" для коробчатой диаграммы\n",
    "            'font':{'size':20,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "            },\n",
    "        ticklabelposition = 'inside top', # \"спрячем\" значения по оси \"y\"\n",
    "    row=1,col=1) \n",
    "\n",
    "# Добавим на рабочую плоскость гистограмму c графиком оценки плотности ядра KDE\n",
    "# Воспользуемся библиотекой plotly.figure_factory\n",
    "# Формируем графику\n",
    "dist_fig = ff.create_distplot(hist_data=[train_data.groupby('id')['rn'].count().iloc[target[target['flag']==0].index]], \n",
    "                              group_labels=[''],\n",
    "                              bin_size=1,\n",
    "                              show_rug=False,\n",
    "                              show_hist=True,\n",
    "                              histnorm='probability',\n",
    "                              colors=['red'],\n",
    "                              )\n",
    "# Настроим отображение графики\n",
    "dist_fig.update_traces(hovertemplate='Количество: %{x}<br>Плотность вероятности: %{y}', # добавить надписи при наведение\n",
    "                       showlegend=False # уберем значок из легенды\n",
    "                       )\n",
    "# я не нашел как другим способом объединить subplots и figure_factory\n",
    "# на сколько я разобрался, суть метода построения заключается в извлечение данных из figure_factory и добавление фрагментов в subplots\n",
    "for trace in dist_fig.select_traces():\n",
    "    fig_1.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# # Настроим отображение оси y для гистограммы\n",
    "fig_1.update_yaxes(\n",
    "     title ={\n",
    "        'text':'Гистограмма распределения<br>(KDE-плотность вероятности)', # добавим название оси \"y\"\n",
    "        'font':{'size':20,'family':\"Times New Roman\"}, # размер и стиль написания имени\n",
    "        },\n",
    "    ticklabelposition = 'inside top', # \"спрячем\" значения по оси \"y\"\n",
    "    row=2,col=1) \n",
    "\n",
    "fig_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\withoutdefolt.png\" alt=\"drawing\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\"> \n",
    "\n",
    "Вывод:  \n",
    "\n",
    "Распределение количества клиентских операций отлично от нормального.  \n",
    "Из коробчатой диаграммы мы наблюдаем наличие выбросов - клиенты с количеством опреций больше 24.  \n",
    "Так же коробчатая диаграмма показывает, что среднее значение больше медианы распределения. В этом случае, график    \n",
    "распределения \"скошен\" вправо относительно нормального, что и наблюдается на графике распределения плотности вероятности.  \n",
    "В рамках разброса количество клиетских операций принимает значения от 1 до 24, при этом основная часть приходится на   \n",
    "знаечния от 4 до 12. \n",
    "Медиальное значение равно 7, а среднее значение приближается к 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим рабочую плоскость на котором можно разместить две диаграммы одну под другой с общей осью абсцисс\n",
    "fig_1 = make_subplots(\n",
    "    rows=2, # количество диаграмм по вертикали\n",
    "    cols=1, # количество диаграмм по горизонтали\n",
    "    shared_xaxes=True, # использовать одну ось х для двух диаграмм\n",
    "    vertical_spacing = 0.05, # уменьшим расстояние между диаграммами\n",
    "    x_title= 'Количество клиетских операций',\n",
    ")\n",
    "# Настроем оформление рабочей поверхности\n",
    "fig_1.update_layout(\n",
    "    title ={\n",
    "        'text':'Распределение количества клиентских операций (клиент с дефолтом)', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height = 800,# Высота рабочей плоскости\n",
    "    width = 1400, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    template='simple_white', # зададим тему оформления для рабочей поверхности \n",
    "    )\n",
    "\n",
    "# Добавим на рабочую плоскость коробчатую диаграмму\n",
    "fig_1.add_box(\n",
    "    x=train_data.groupby('id')['rn'].count().iloc[target[target['flag']==1].index], # Выбираем данные по оси х\n",
    "    y0=0, # Устанавливаем значение в нуле по оси y\n",
    "    row=1, col=1, # Выбираем в какой координатной плоскости будет строиться диаграмма\n",
    "    showlegend=False, # убираем легенду (обе диаграммы представляют распределение одной величины)\n",
    "    boxmean=True, # добавим на коробчатую диаграмму отметку среднего значения заработной платы\n",
    "    name= '', # при наведение на значение не будет отображаться имя диаграммы\n",
    "    hoverinfo='x' # при наведение отражается только значение заработной платы\n",
    ") \n",
    "\n",
    "# Настроим отображение оси y для коробчатой диаграммы\n",
    "fig_1.update_yaxes(\n",
    "    title={'text':'Коробчатая диаграмма', # добавим название оси \"y\" для коробчатой диаграммы\n",
    "            'font':{'size':20,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "            },\n",
    "        ticklabelposition = 'inside top', # \"спрячем\" значения по оси \"y\"\n",
    "    row=1,col=1) \n",
    "\n",
    "# Добавим на рабочую плоскость гистограмму c графиком оценки плотности ядра KDE\n",
    "# Воспользуемся библиотекой plotly.figure_factory\n",
    "# Формируем графику\n",
    "dist_fig = ff.create_distplot(hist_data=[train_data.groupby('id')['rn'].count().iloc[target[target['flag']==0].index]], \n",
    "                              group_labels=[''],\n",
    "                              bin_size=1,\n",
    "                              show_rug=False,\n",
    "                              show_hist=True,\n",
    "                              histnorm='probability',\n",
    "                              colors=['red'],\n",
    "                              )\n",
    "# Настроим отображение графики\n",
    "dist_fig.update_traces(hovertemplate='Количество: %{x}<br>Плотность вероятности: %{y}', # добавить надписи при наведение\n",
    "                       showlegend=False # уберем значок из легенды\n",
    "                       )\n",
    "# я не нашел как другим способом объединить subplots и figure_factory\n",
    "# на сколько я разобрался, суть метода построения заключается в извлечение данных из figure_factory и добавление фрагментов в subplots\n",
    "for trace in dist_fig.select_traces():\n",
    "    fig_1.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# # Настроим отображение оси y для гистограммы\n",
    "fig_1.update_yaxes(\n",
    "     title ={\n",
    "        'text':'Гистограмма распределения<br>(KDE-плотность вероятности)', # добавим название оси \"y\"\n",
    "        'font':{'size':20,'family':\"Times New Roman\"}, # размер и стиль написания имени\n",
    "        },\n",
    "    ticklabelposition = 'inside top', # \"спрячем\" значения по оси \"y\"\n",
    "    row=2,col=1) \n",
    "\n",
    "fig_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\withdefolt.png\" alt=\"drawing\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\"> \n",
    "\n",
    "Вывод для клиентов с дефолтом:  \n",
    "\n",
    "Распределение количества клиентских операций отлично от нормального.  \n",
    "Из коробчатой диаграммы мы наблюдаем наличие выбросов - клиенты с количеством опреций больше 25.  \n",
    "Так же коробчатая диаграмма показывает, что среднее значение больше медианы распределения. В этом случае, график    \n",
    "распределения \"скошен\" вправо относительно нормального, что и наблюдается на графике распределения плотности вероятности.  \n",
    "В рамках разброса количество клиетских операций принимает значения от 1 до 25, при этом основная часть приходится на   \n",
    "знаечния от 3 до 12. \n",
    "Медиальное значение равно 7, а среднее значение приближается к 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Анализ признаков</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы по внешнему предварительному анализу данных:\n",
    "\n",
    "Все признаки в датасете либо бинаризированы, либо закодированы - соответственно являются категориальными.  \n",
    "Данных в датасете содержащием кредитные истории клиентов больше, чем в датасете содержащем информацию о дефолте клиента.  \n",
    "Это связано с тем, что на каждого клиента существует кредитная история из нескольких кредитов.  \n",
    "Для того, чтобы подать эти данные в модель необходимо агрегировать информацию по каждой клиенту.   \n",
    "В качестве агрегирующих функций примем статистические характеристики случайных величин.   \n",
    "Проаналировав поле признаков - его можно разделить на 6 групп.  \n",
    "Помимо всего небходимо учитывать, что данные кредитных историй имееют хронологическую последовательность.  \n",
    "Необходимо, не потерять информацию об этом при агригации данных.  \n",
    "Так же необходимо не потерять информацию, о количестве  кредитов в истории.  \n",
    "В связи с выше изложенным, примем следующую концепцию решения задачи:\n",
    "\n",
    "Подготовка данных для обучения модели:\n",
    "1. Выделетить часть данных для проверки модели (80/20 %).\n",
    "2. Разбить признаки на 6 групп.\n",
    "3. Внутри каждой группы в границах одного клиента выполниить преобразование признаков:  \n",
    "    3.1. Заменить значения признаков на их статистичекие характеристики без учета хронологии кредитсной истории;  \n",
    "    3.2. Добавить признак количества кредитных историй.  \n",
    "    3.3. Создать признаки учитывающие историю клинетских операций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Постановка задачи в рамках EDA</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные о клиентах представлены в виде заявок, номера которых соотностятся с датами подачи заявки.  \n",
    "Большему номеру соответствует более поздняя дата заявки.\n",
    "\n",
    "Разделим процесс преобразования признаков на три части: \n",
    "\n",
    "1. Разделим признаки на под пространства.\n",
    "\n",
    "2. Преобразуем кредитные операции клиента в его признаки, сохранив последовательность операций.  \n",
    "Такие методы кодирования как Ordinal Encoding, OneHot Encoding не подойдут, так как уничтожится   \n",
    "информация о последовательности операций.\n",
    "\n",
    "3. Преобразование признаков как дискренного ряда.  \n",
    "Для каждого признака в \"границах\" одного \"id\" рассчитать статистические показатели ряда, такие как:  \n",
    "   - среднее значение ряда (математическое ожидание);  \n",
    "   - средне гармоническое значение ряда;   \n",
    "   - стандартное отклонение;  \n",
    "   - минимальное значение;  \n",
    "   - 25% квантиль;  \n",
    "   - 50% квантиль;  \n",
    "   - 75% квантиль;  \n",
    "   - максимальное значение;  \n",
    "   - мода ряда (среднее значение мод);  \n",
    "   - частота появления моды (средненего значения мод);\n",
    "   - и т.д. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Разработка инструментов преобразования данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция torow_transformer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция torow_transformer\n",
    "\n",
    "# Назначение: Преобразование признака столбца в признаки строки\n",
    "#               с сохранением обратной последовательности в признаке.\n",
    "#               (извлечение последних операций клиента)\n",
    "\n",
    "# Внешние переменные функции: DataFrame, n_last\n",
    "#   DataFrame - исоходый DataFrame\n",
    "#   n_last - необходимое счисло послдених операций клиента\n",
    "#   Структура DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: New DataFrame\n",
    "#   Признаки New DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1.1\n",
    "#       3. feature1.2\n",
    "#       4. feature1.3\n",
    "#       ...\n",
    "#       5. feature1.N\n",
    "#       6. feature2.1\n",
    "#       7. feature2.2\n",
    "#       ...\n",
    "#       8. feature2.N\n",
    "#       ...\n",
    "#   где featureX.1 - соотвествует последней операции клиента,\n",
    "#       featureX.2 - соотвествует предпоследней операции клиента,\n",
    "#       .....\n",
    "\n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных DataFrame\n",
    "# 3. формируем карту признаков dict_features:\n",
    "#    3.1. каждый признак кодируется следующим образом: 'fn',\n",
    "#         где n - порядковый номер признака\n",
    "#    3.2. полные имена признаков задаются следующим образом: 'feature_N',\n",
    "#         где N - порядок клиентской операции (большему N соотвествует, более ранняя операция)\n",
    "\n",
    "# 4. Преобразуем данные к массиву\n",
    "# 5. Группируем массив для каждого клиента\n",
    "# 6. К групированному массиву прменяем следующие преобразования:\n",
    "#    6.1. Обращаем порядок клиенских операций\n",
    "#    6.2. выбираем посление n_last операций\n",
    "#    6.3. Если число клиенски операций меньше чем n_last, дополняем их нулями\n",
    "# 7. Преобразуем полученные данные к DataFrame\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. pd_data - исходный DataFrame\n",
    "# 2. n_last - необходимое число последних операций клиента\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 5. dict_features - локальная карта признаков\n",
    "# 6. rn_id - список количества операций для каждого клиента\n",
    "# 6. array_data - данные преобразоанные к numpy-массиву\n",
    "# 6. split_array - сгрупированные по клиентам данные преобразованные\n",
    "\n",
    "# обьявлем функцию\n",
    "def torow_func(dict_params):\n",
    "    pd_data = dict_params['data']\n",
    "    n_last = dict_params['n_last']\n",
    "\n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "    \n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "\n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {}\n",
    "    \n",
    "    # заполним словарь dict_features\n",
    "    num_f=0\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for num_feature in range(1,n_last+1):\n",
    "            dict_features['f'+str(num_feature+num_f)] = feature+'_'+str(num_feature)\n",
    "        num_f+=n_last\n",
    "    \n",
    "\n",
    "    # формируем словарь rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)\n",
    "    \n",
    "    # определим порядок последующих преобразований в функции\n",
    "    def transform_array(array_id):\n",
    "        # обратим порядок клиентских операций \n",
    "        reverse_array_id = array_id[::,::-1]\n",
    "        # выбрем после n операций клиента\n",
    "        list_n_last = reverse_array_id[::,:n_last]\n",
    "        # если клиенских операций было меньше чем n_last\n",
    "        # дополним недастающие нулями и преобразуем данные к строке\n",
    "        if len(list_n_last[0])<n_last:\n",
    "            full_list_n_last = np.hstack((list_n_last,np.zeros((list_n_last.shape[0],n_last-len(list_n_last[0])),dtype='int64')))\n",
    "            # преобразуем список к строке\n",
    "            full_list_n_last = full_list_n_last.reshape(-1)\n",
    "        else:\n",
    "            full_list_n_last = list_n_last.reshape(-1)\n",
    "        return full_list_n_last\n",
    "\n",
    "    # применим transform_array преобразование к списку split_array\n",
    "    list_data = np.array(list(map(transform_array,split_array)))[:-1]\n",
    "    \n",
    "    # преобразуем полученные данные к dataframe\n",
    "    dataframe = pd.DataFrame(data=list_data, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "    \n",
    "    return dataframe, dict_features,rn_id\n",
    "\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "torow_transformer = FunctionTransformer(torow_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция features_from_transform_data_torow\n",
    "\n",
    "# Назначение: Извлечение из данных, над которомы совершено \n",
    "#             row_fich_transformer() преобразование, признаков  \n",
    "#             соотвествующих заданному числу последних  \n",
    "#             опреаций клиента n_last\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. n_last - необходимое число последних операций клиента\n",
    "#       2. n_groups - число групп признаков в transform_data_torow\n",
    "#       3. N_last - число последних операций клиента показанных в transform_data_torow\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. list_n_last_features - список признаков в transform_data_torow\n",
    "#    соотвествующий заданному числу n_last. \n",
    "    \n",
    "\n",
    "# обьявлем функцию\n",
    "def features_from_transform_data_torow(n_last,n_groups,N_last):\n",
    "    # создадим список под необходимые признаки\n",
    "    list_n_last_features = []\n",
    "    \n",
    "    # обьявим начальное значение в группе признаков\n",
    "    n_start = 0\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        for n in range(n_last):\n",
    "            list_n_last_features.append(n+n_start)\n",
    "        n_start+=N_last\n",
    "    \n",
    "    return list_n_last_features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция diff_feature</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция diff_feature\n",
    "\n",
    "# Назначение: Определение дифференциальных характеристик ряда \n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1.Series/np.array/list\n",
    "\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. diff_list - Список из значений:\n",
    "#                   1.1. speed - скорость изменения ряда;\n",
    "#                   1.2. accel - ускорение изменения ряда;\n",
    "#                   1.3. bias - смещение ряда;\n",
    "#                   1.4. pulse - импульс ряда;\n",
    "\n",
    "# обьявлем функцию\n",
    "def diff_feature(data):\n",
    "    # преобразуем данные к numpy массиву\n",
    "    data = np.array(data)\n",
    "    # расчитаем необходимые характеристики\n",
    "    speed = round(float(np.diff(data,1).mean()),2)\n",
    "    accel = round(float(np.diff(data,2).mean()),2)\n",
    "    bias = round(float(np.diff(data,1).sum()),2)\n",
    "    pulse = round(float(np.diff(data,2).sum()),2)\n",
    "    # сформируем из найденных значений в список\n",
    "    diff_list = [speed,accel,bias,pulse]\n",
    "    return diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция statistic_features</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция statistic_features\n",
    "\n",
    "# Назначение: Извлечение основных статистических характеристик\n",
    "#             из признаков в исходном DataFrame.\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. dataframe - таблица с данными. \n",
    "#    Признаки dataframe:\n",
    "#       1. id\n",
    "#       2. feature1_mean\n",
    "#       3. fearture1_hmean\n",
    "#       4. feature1_std\n",
    "#       5. feature1_min\n",
    "#       6. feature1_25%\n",
    "#       7. feature1_50%\n",
    "#       8. feature1_75%\n",
    "#       9. feature1_max\n",
    "#       10. feature1_mode\n",
    "#       11. feature1_frequency_mode\n",
    "#       12. feature2_mean\n",
    "#       ...\n",
    "    \n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных\n",
    "# 2. формируем карту признаков:\n",
    "#    2.1. каждый признак кодируется следующим образом: 'fn' где n - порядковый номер признака\n",
    "#    2.2. полные имена признаков задаются следующим образом:\n",
    "#         2.2.1 если в исходном dataframe признак бинарный, то: \"Исходное имя признака\"+\"binary\"\n",
    "#         2.2.2 если в исходном dataframe признак не бинарный, то: \"Исходное имя признака\"+\"Статистическая характеристика\"\n",
    "# 3. для каждого клиента по каждому признаку из исходного dataframe расчитываем статистические характеристики\n",
    "# 4. записываем полученные значение в новый dataframe\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. dict_agg_function - словарь из агригирующих функций\n",
    "#       keys: имена для обращения к функциям:\n",
    "#       values: lamda-функция, соотвествующей статистической характристики\n",
    "# 2. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. dict_features - локальная карта признаков\n",
    "# 5. k - номер признака в dict_features на текущей итерации\n",
    "# 6. dataframe - результирующий dataframe\n",
    "\n",
    "# обьявлем функцию\n",
    "def statistic_features(pd_data):\n",
    "    # формируем список из функций для статистических преобразований\n",
    "    # предусмотрим работу функций на случай, если в массиве данных всего 1 строка\n",
    "    dict_agg_function = {\n",
    "    'ptp' : lambda x: 0 if len(x) <= 3 else np.ptp(x),\n",
    "    'mean': lambda x: 0 if len(x) <= 3 else x.mean(), \n",
    "    'gmean' : lambda x: stats.gmean(x),   \n",
    "    'hmean': lambda x: stats.gmean(x),\n",
    "    # 'pmean25': lambda x: stats.pmean(x,25),\n",
    "    'pmean50': lambda x: stats.pmean(x,50),\n",
    "    # 'pmean75': lambda x: stats.pmean(x,75),\n",
    "    # 'expectile25': lambda x: stats.expectile(x,0.25),\n",
    "    'expectile50': lambda x: stats.expectile(x),\n",
    "    # 'expectile75': lambda x: stats.expectile(x,0.75),\n",
    "    'moment': lambda x: stats.moment(x),\n",
    "    'std': lambda x: 0 if len(x) <= 3 else np.std(x),\n",
    "    'min': lambda x: min(x),\n",
    "    '20%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=20),\n",
    "    # '30%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=30),\n",
    "    # '40%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=40),\n",
    "    '50%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=50),\n",
    "    # '60%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=60),\n",
    "    '70%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=70),\n",
    "    'max': lambda x: max(x),\n",
    "    'mode': lambda x: statistics.mean(statistics.multimode(x)),\n",
    "    'frequency_mode': lambda x: round(list(x).count(statistics.multimode(x)[0])*len(statistics.multimode(x))/len(x),2),\n",
    "    'cov' : lambda x: 0 if len(x) <= 3 else np.cov(x),\n",
    "    'histogram' : lambda x: 0 if len(x) <= 3 else np.histogram(x)[1].mean(), \n",
    "    # 'speed': lambda x: 0 if len(x) <= 3 else diff_feature(x)[0],\n",
    "    # 'accel': lambda x: 0 if len(x) <= 3 else diff_feature(x)[1],\n",
    "    # 'bias': lambda x: 0 if len(x) <= 3 else diff_feature(x)[2],\n",
    "    # 'pulse': lambda x: 0 if len(x) <= 3  else diff_feature(x)[3]\n",
    "    } \n",
    "\n",
    "    # напишем функцию для преобразования массива до статистических характеристик\n",
    "    def stat_func(array_data): \n",
    "        # сформируем лист под результаты преобразования\n",
    "        list_for_result = []\n",
    "        # запишем все статистические харкетристики из словаря dict_agg_function\n",
    "        for func in dict_agg_function.values():\n",
    "            list_for_result.append(func(array_data))\n",
    "        return np.array(list_for_result).round(3)\n",
    "\n",
    "    # напишем функцию для применения функции stat_func к списку\n",
    "    def submap(list_data):\n",
    "        # расчитаем количество операция клиента\n",
    "        max_rn = len(list_data[0])\n",
    "        # получим статистические характристики массива\n",
    "        list_stat_features = np.array(list(map(stat_func,list_data))).reshape(-1)\n",
    "        return np.hstack((max_rn,list_stat_features))\n",
    "    \n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "\n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "    \n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {'f1':'count'}\n",
    "    k=1 # порядковый номер защифрованного признака\n",
    "\n",
    "    # заполним словарь dict_features\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for key_function in dict_agg_function.keys():\n",
    "            k+=1\n",
    "            dict_features['f'+str(k)] = feature+'_'+key_function\n",
    "\n",
    "    # формируем список rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)[:-1]\n",
    "\n",
    "    # получем статические характеристики признаков\n",
    "    stat_features = np.array(list(map(submap,split_array)))\n",
    "    \n",
    "    # Сформируем dataframe из полученных данных\n",
    "    dataframe = pd.DataFrame(data=stat_features, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "\n",
    "    return dataframe, dict_features\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "stat_transformer = FunctionTransformer(statistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция corr_transform_to_force</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция corr_transform_to_force\n",
    "\n",
    "# Назначение: из матрицы взаимных корреляций\n",
    "#             выделить не корелирующие признаки\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. df.corr() - матрица корреляций\n",
    "#           2. threshold - порог значимости корреляции:\n",
    "#               значение коэффициента корреляции, больше которого\n",
    "#               признаки считаются скоррелированными.\n",
    "\n",
    "# Пояснение: \n",
    "# Под силой корреляции будем понимать следующее: если коэффициент \n",
    "# коррелиции между признаками больше значения threshold, то принимаем,\n",
    "# что между признаками сильная корреляционная связь значение коэффициента \n",
    "# коррелияции заменяем на 1, иначе корреляционная связь слабая и значение \n",
    "# коээфициента корреляции заменяем на 0\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. corr_matrix - матрица корреляций(отражает силу корреляции)\n",
    "# 2. list_ncorr_features - список не скореллированных признаков\n",
    "# 3. corr_force - сила корреляции всей матрицы: отношение числа скоррелированных \n",
    "# признаков к числу всех признаков в матрице\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. coor_force - функция преобазующая значение\n",
    "#        коэффициента корряляции в силу корреляции\n",
    "# 2. corr_matrix - матрица отражающая силу корряляции между признаками\n",
    "# 3. max_corr - максимальное число взаимных корреляций между признаками\n",
    "# 4. list_ncorr_features - список не коррелируемых признаков\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def corr_transform_to_force(matrix,threshold=0.7):\n",
    "    list_features = matrix.index.tolist()\n",
    "    \n",
    "    \n",
    "    # создадим функцию для разметки матрицы корреляции\n",
    "    # 1 - корреляция признаков выше порога значимости threshold\n",
    "    # 0 - корреляция признаков ниже порога значимости threshold\n",
    "    corr_force = lambda x: 1 if x >threshold else 0\n",
    "    # выполним разметку матрицы корреляции\n",
    "    corr_matrix = matrix.map(lambda x: corr_force(x))\n",
    "    \n",
    "    # алгоритм отбора не коррелиарных признаков:\n",
    "    #   1. Найдем признак с наибольшим числом взаимных корреляций\n",
    "    #   2. удалим найденный признак\n",
    "    #   3. составим матрицу корреляций из отсавшися признаков\n",
    "    #   4. повторяем пункты 1-3 до тех пор пока в матрице не останутся \n",
    "    #       не коррелированные признаки\n",
    "\n",
    "    # ищем наибольшее число взаимных корреляций среди признаков\n",
    "    max_corr = corr_matrix.sum().max()\n",
    "\n",
    "    while max_corr > 1:\n",
    "        # определяем признак с наибольшим числом взаимных корреляций\n",
    "        max_corr_feature = corr_matrix.sum()[corr_matrix.sum()==corr_matrix.sum().max()].index[0]\n",
    "        # удалем признак из матрицы корреляций\n",
    "        corr_matrix = corr_matrix.drop(max_corr_feature).drop(max_corr_feature,axis=1)\n",
    "        max_corr = corr_matrix.sum().max()\n",
    "    # запишем не скоррелированные признаки в список\n",
    "    list_ncorr_features = corr_matrix.index.tolist()\n",
    "    # найдем силу корреляции всей матрицы как отношение\n",
    "    # количества скоррелированных признаков к всмеу количеству признаков\n",
    "    corr_force = round(1-len(list_ncorr_features)/len(list_features),3)\n",
    "    return corr_matrix, list_ncorr_features, corr_force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция search_DBSCAN_parameters</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция search_DBSCAN_parameters\n",
    "\n",
    "# Назначение: Для подбора eps и min_samples параметров,\n",
    "#               функция \"прогоняет\" DBSCAN кластеризацию \n",
    "#               с параметрами eps и min_samples\n",
    "#               примающими значения из заданного диапазона.\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data - dataframe для кластеризации\n",
    "#           2. r1 - начало диапазона\n",
    "#           3. r2 - конец диапазона  \n",
    "#           4. n - предпалагамое число кластеров      \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. data_cluster - кластеризация данных при различных \n",
    "#       значениях параметров eps и min_samples\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. parametr_range - диапазон изменения параметров\n",
    "# 2. dataframe_columns - колонки в результирующем dataframe\n",
    "# 3. data_cluster - результрующий dataframe\n",
    "# 4. index_cluster - текущая позиция в data_cluster\n",
    "# 5. clustering - кластеризатор\n",
    "# 6. list_cluster_values - список для заполнения текущими \n",
    "#                          значениями data_cluster\n",
    "\n",
    "# обьявлем функцию\n",
    "def search_DBSCAN_parameters(dataframe,r1,r2,n=3):\n",
    "    # задаем диапозон измениния параметров\n",
    "    parameter_range = range(r1,r2)\n",
    "    # формируем заготовку для результирующего dataframe\n",
    "    dataframe_columns = ['eps','min_samples',-1,0,1]\n",
    "    # проверим что задано не меньше минимального количества кластеров\n",
    "    if n<=3: \n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    else: \n",
    "        for claster in range(4,n+1):\n",
    "            dataframe_columns.append(claster-2)\n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    # задаем начально значение индекса в data_cluster\n",
    "    index_cluster = 0\n",
    "\n",
    "    # для подсчета обьектов в кластерах создадим dataframe\n",
    "    dataframe_count = pd.DataFrame()\n",
    "    \n",
    "    # \"прогоняем\" DBSCAN кластеризациию по диапазону параметров\n",
    "    for eps in parameter_range:\n",
    "        \n",
    "        for min_samples in parameter_range:\n",
    "            print('current eps:',eps,'  current min_samples:', min_samples, end='\\r')\n",
    "            # запускаем кластеризацию с текущими параметрами\n",
    "            clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(dataframe)\n",
    "            # добавлем к данным столбец с разметкой\n",
    "            dataframe_count['clater'] = clustering.labels_\n",
    "            # формируем пустой список для заполнения\n",
    "            list_cluster_values = []\n",
    "            # добавлеям в список текущие параметры\n",
    "            list_cluster_values.append(eps)\n",
    "            list_cluster_values.append(min_samples)\n",
    "            # добавлем в список количество обьктов в каждом кластере\n",
    "            for column in dataframe_columns[2:]:\n",
    "                list_cluster_values.append(len(dataframe_count['clater'][dataframe_count['clater']==column]))\n",
    "                \n",
    "            # заполняем dataframe  текущими данными\n",
    "            data_cluster.loc[index_cluster] = list_cluster_values\n",
    "            index_cluster +=1\n",
    "            # сбрасываем dataframe_count\n",
    "            dataframe_count = pd.DataFrame()\n",
    "    return data_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция generate_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция generate_samples\n",
    "\n",
    "# Назначение: для генерации индексов выборок данных\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. max - определяет максимальное значение множества\n",
    "#               из которого формируются выборки\n",
    "#           2. n - количество выборок\n",
    "#           3. k - мощность одной выборки\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список с выбороками\n",
    "\n",
    "# алгоритм работы:\n",
    "# 1. задаем отрезок натурального ряда N мощностью max и добавлем в него 0.\n",
    "#       In = N U {0}, I = {0,1,2,3,4,5,..,max}\n",
    "# 2. если мощность множества In больше, необходимого количества элементов\n",
    "#       cardo(In) > n x k , то из множества In формируем n случайных выборок\n",
    "# размера k без повторения.\n",
    "# 3. если мощность множества In меньше, необходимого количества элементов\n",
    "#       cardo(In) < n x k , то из множества In формируем случайные выборки\n",
    "# размера k без повторения, до тех пор пока не закончится множество In.\n",
    "# После, добираем недостающее количество выборок случайными выборками \n",
    "# размера k из множества In с повторением (bootstrap метод).\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def generate_samples(max,n,k,random_state = None):\n",
    "    # создадим список под результат\n",
    "    samples_list = []\n",
    "    # формируем множество натуральных числе от 0 до max\n",
    "    In = list(range(max+1))\n",
    "    # Будем выполнять код пока не наберем необходимого количества выборок\n",
    "    # нарушим порядок в множестве\n",
    "    In = shuffle(In,random_state=random_state)\n",
    "    # random.shuffle(In)\n",
    "\n",
    "    # задаим границы извлечения данных из In\n",
    "    In_start = 0\n",
    "    In_end = k\n",
    "    while len(samples_list) < n:\n",
    "        # сформируем список под одну выборку\n",
    "        sample = []\n",
    "        # первые списки будем наполнять значениеми из множества In\n",
    "        # без повторения, до тех пор пока все значения из множества In\n",
    "        # не распределяться по выборкам\n",
    "        if len(In)-In_end >= 0:\n",
    "            sample.extend(In[In_start:In_end])\n",
    "        else:                    \n",
    "            # если элементов во множестве In недостаточно,\n",
    "            # запоняем выборку \"остатками\" \n",
    "            sample.extend(In[In_start:])\n",
    "\n",
    "            # остальные данные заполняем методом bootstrap\n",
    "            # выполнем код пока не заполним выборку k значениями\n",
    "            while len(sample) < k:\n",
    "                # генерируем случайное число из диапазона от 0 до len(In)-1\n",
    "                random_index = randint(0,len(In)-1)\n",
    "                # добавляем значение из множества In с индексом random_index\n",
    "                # в список index_list\n",
    "                sample.append(In[random_index])\n",
    "\n",
    "        # после того как мы набрали значения в выборку отправлем ее в samples_list\n",
    "        samples_list.append(sample)\n",
    "        # переходим к следующим данным в множестве In\n",
    "        In_start+=k\n",
    "        In_end+=k\n",
    "\n",
    "    return samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция my_train_test_split</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X,y,random_state=42,train_size=0.8,):\n",
    "    # если разбиение без стратификации\n",
    "\n",
    "    # зададим число элементов в выборке train\n",
    "    len_train = round(len(y)*train_size)\n",
    "    # формируем множество натуральных чисел от 0 до max\n",
    "    list_random_index = list(range(len(y)))\n",
    "    # нарушим порядок в множестве\n",
    "    list_random_index = shuffle(list_random_index,random_state=random_state)\n",
    "    # формируем список индексов под train выборку\n",
    "    train_samples = list_random_index[:len_train]\n",
    "    # формируем список индексов под test выборку\n",
    "    test_samples = list_random_index[len_train:]\n",
    "    # выполнем код пока не заполним выборку k значениями\n",
    "   \n",
    "    X_train = X.iloc[train_samples]\n",
    "    y_train = y.iloc[train_samples]\n",
    "    X_test = X.iloc[test_samples]\n",
    "    y_test = y.iloc[test_samples]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">функция class_1_percent_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция class_1_percent_samples\n",
    "\n",
    "# Назначение: для генерации индексов сбалансированных выборок\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data_target - массив из id и значений класса\n",
    "#           2. class_1_percent - процент класса 1 в результирующей выборке\n",
    "#           3. random_state - параметр для обеспечения воспроизваодимости функции\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список со сблансированными выбороками\n",
    "\n",
    "# обьявлем функцию\n",
    "def class_1_percent_samples(data_target,class_1_percent,random_state = None):\n",
    "    # приведем данные к нужной форме\n",
    "    data_target = pd.DataFrame(data=np.array(data_target),columns =['id','flag'])\n",
    "    \n",
    "    # разделим клиентов  по признаку flag\n",
    "    flag_0 = data_target[data_target['flag']==0].reset_index(drop=True)\n",
    "    flag_1 = data_target[data_target['flag']==1].reset_index(drop=True)\n",
    "\n",
    "    # определим класс большинства\n",
    "    if flag_1.shape[0] > flag_0.shape[0]:\n",
    "        majority_class = flag_1\n",
    "        minority_class = flag_0\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(class_1_percent)/(1-class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    else:\n",
    "        majority_class = flag_0\n",
    "        minority_class = flag_1\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(1-class_1_percent)/(class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    \n",
    "    # сформируем список выбороки с заданным процентом класс 1\n",
    "    samples_list_id = minority_class['id'].values.tolist()+majority_class['id'].iloc[samples_majority_class[0]].tolist()\n",
    "\n",
    "    return samples_list_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Преобразование признаков и анализ признаков</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Разделим признаки на 6 под пространств</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**date features:**  \n",
    "1. pre_since_opened\t-\tДней с даты открытия кредита до даты сбора данных (бинаризовано**)\t  \n",
    "2. pre_since_confirmed\t-\tДней с даты подтверждения информации по кредиту до даты сбора данных (бинаризовано**)\t  \n",
    "3. pre_pterm\t-\tПлановое количество дней с даты открытия кредита до даты закрытия (бинаризовано**)\t  \n",
    "4. pre_fterm\t-\tФактическое количество дней с даты открытия кредита до даты закрытия (бинаризовано**)\t  \n",
    "5. pre_till_pclose\t-\tПлановое количество дней с даты сбора данных до даты закрытия кредита (бинаризовано**)\t  \n",
    "6. pre_till_fclose\t-\tФактическое количество дней с даты сбора данных до даты закрытия кредита (бинаризовано**)\n",
    "7. pclose_flag\t-\tФлаг: плановое количество дней с даты открытия кредита до даты закрытия не определено \t  \n",
    "8. fclose_flag\t-\tФлаг: фактическое количество дней с даты открытия кредита до даты закрытия не определено\n",
    "\n",
    "**late payments features:**  \n",
    "1. pre_loans5\t-\tЧисло просрочек до 5 дней (бинаризовано**)\t  \n",
    "2. pre_loans530\t-\tЧисло просрочек от 5 до 30 дней (бинаризовано**)\t  \n",
    "3. pre_loans3060\t-\tЧисло просрочек от 30 до 60 дней (бинаризовано**)\t  \n",
    "4. pre_loans6090\t-\tЧисло просрочек от 60 до 90 дней (бинаризовано**)\t  \n",
    "5. pre_loans90\t-\tЧисло просрочек более, чем на 90 дней (бинаризовано**)\t  \n",
    "6. is_zero_loans5\t-\tФлаг: нет просрочек до 5 дней\t  \n",
    "7. is_zero_loans530\t-\tФлаг: нет просрочек от 5 до 30 дней\t  \n",
    "8. is_zero_loans3060\t-\tФлаг: нет просрочек от 30 до 60 дней\t  \n",
    "9. is_zero_loans6090\t-\tФлаг: нет просрочек от 60 до 90 дней\t  \n",
    "10. is_zero_loans90\t-\tФлаг: нет просрочек более, чем на 90 дней\t  \n",
    "11. pre_loans_total_overdue\t-\tТекущая просроченная задолженность (бинаризовано**)\t  \n",
    "12. pre_loans_max_overdue_sum\t-\tМаксимальная просроченная задолженность (бинаризовано**)\t\n",
    "\n",
    "\n",
    "**credit features:** \n",
    "1. pre_loans_credit_limit\t-\tКредитный лимит (бинаризовано**)\t  \n",
    "2. pre_loans_next_pay_summ\t-\tСумма следующего платежа по кредиту (бинаризовано**)\t  \n",
    "3. pre_loans_outstanding\t-\tОставшаяся невыплаченная сумма кредита (бинаризовано**)\t  \n",
    "4. pre_loans_credit_cost_rate\t-\tПолная стоимость кредита (бинаризовано**)\t  \n",
    "  \n",
    "\n",
    "**relative features:**  \n",
    "1. pre_util\t-\tОтношение оставшейся невыплаченной суммы кредита к кредитному лимиту (бинаризовано**)\t  \n",
    "2. pre_over2limit\t-\tОтношение текущей просроченной задолженности к кредитному лимиту (бинаризовано**)\t  \n",
    "3. pre_maxover2limit\t-\tОтношенение максимальной просроченной задолженности к кредитному лимиту (бинаризовано**)\t  \n",
    "4. is_zero_util\t-\tФлаг: отношение оставшейся невыплаченной суммы кредита к кредитному лимиту равняется 0\t  \n",
    "5. is_zero_over2limit\t-\tФлаг: отношение текущей просроченной задолженности к кредитному лимиту равняется 0\t  \n",
    "6. is_zero_maxover2limit\t-\tФлаг: отношение максимальной просроченной задолженности к кредитному лимиту равняется 0\t  \n",
    "\n",
    "**payments features:**   \n",
    "1. enc_paym_{0..N}\t-\tСтатусы ежемесячных платежей за последние N месяцев (закодировано***)\t  \n",
    "\n",
    "**service features:**  \n",
    "1. enc_loans_account_holder_type\t-\tТип отношения к кредиту (закодировано***)\t  \n",
    "2. enc_loans_credit_status\t-\tСтатус кредита (закодировано***)\t  \n",
    "3. enc_loans_account_cur\t-\tВалюта кредита (закодировано***)\t  \n",
    "4. enc_loans_credit_type\t-\tТип кредита (закодировано***)\t  \n",
    "\n",
    "  ** область значений поля разбивается на N непересекающихся промежутков, каждому промежутку случайным образом ставится в соответствие уникальный номер от 0 до N-1, значение поля заменяется номером промежутка, которому оно принадлежит  \n",
    "  *** каждому уникальному значению поля случайным образом ставится в соответствие уникальный номер от 0 до K, значение поля заменяется номером этого значения\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем списки признаков каждого подпространства\n",
    "date_features = ['id','rn','pre_since_opened','pre_since_confirmed','pre_pterm','pre_fterm',\n",
    "                 'pre_till_pclose','pre_till_fclose','pclose_flag','fclose_flag']\n",
    "late_features = ['id','rn','pre_loans5','pre_loans530','pre_loans3060','pre_loans6090',\n",
    "                 'pre_loans90','is_zero_loans5','is_zero_loans530','is_zero_loans3060',\n",
    "                 'is_zero_loans6090','is_zero_loans90','pre_loans_total_overdue','pre_loans_max_overdue_sum']\n",
    "credit_features = ['id','rn','pre_loans_credit_limit','pre_loans_next_pay_summ','pre_loans_outstanding','pre_loans_credit_cost_rate']\n",
    "\n",
    "relative_features = ['id','rn','pre_util','pre_over2limit','pre_maxover2limit','is_zero_util',\n",
    "                 'is_zero_over2limit','is_zero_maxover2limit']\n",
    "\n",
    "payments_features = ['id','rn'] + train_data.columns[30:55].to_list()\n",
    "service_features = ['id','rn'] + train_data.columns[55:59].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Выполним torow-transform преобразование</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current space features: service_torow\n",
      "Start transform\n",
      "Start save\n"
     ]
    }
   ],
   "source": [
    "# составим словарь небходимых данных и их признаков\n",
    "dict_torow_data = {\n",
    "    'date_torow' : date_features,\n",
    "    'late_torow' : late_features,\n",
    "    'credit_torow' : credit_features,\n",
    "    'relative_torow' : relative_features,\n",
    "    'payments_torow': payments_features,\n",
    "    'service_torow': service_features}\n",
    "# сформируем данные за последние 25 операций клиентов\n",
    "for space_features in dict_torow_data.keys():\n",
    "    clear_output()\n",
    "    # добавим индекацию процесса\n",
    "    print('Current space features:',space_features)\n",
    "    # сформируем данные для преобразования\n",
    "    data_to_transform = train_data[dict_torow_data[space_features]]\n",
    "    print('Start transform')\n",
    "    data_torow = torow_transformer.transform({'data':data_to_transform,'n_last':25})[0]\n",
    "    # сохраним преобразованные данные на диск для быстрого воспроизведения\n",
    "    print('Start save')\n",
    "    fp.write('base_models/torow/'+space_features,data_torow)\n",
    "    # удалим использованные данные\n",
    "    del data_to_transform\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Выполним stat-transform преобразование</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current space features: service_stat\n",
      "Start transform\n",
      "Start save\n"
     ]
    }
   ],
   "source": [
    "# составим словарь небходимых данных и их признаков\n",
    "dict_stat_data = {\n",
    "    'date_stat' : date_features,\n",
    "    'late_stat' : late_features,\n",
    "    'credit_stat' : credit_features,\n",
    "    'relative_stat' : relative_features,\n",
    "    'payments_stat': payments_features,\n",
    "    'service_stat': service_features}\n",
    "# сформируем данные за последние 25 операций клиентов\n",
    "for space_features in dict_stat_data.keys():\n",
    "    clear_output()\n",
    "    # добавим индекацию процесса\n",
    "    print('Current space features:',space_features)\n",
    "    # сформируем данные для преобразования\n",
    "    data_to_transform = train_data[dict_stat_data[space_features]]\n",
    "    print('Start transform')\n",
    "    data_stat = stat_transformer.transform(data_to_transform)[0]\n",
    "    # сохраним преобразованные данные на диск для быстрого воспроизведения\n",
    "    print('Start save')\n",
    "    fp.write('base_models/stat/'+space_features,data_stat)\n",
    "    # удалим использованные данные\n",
    "    del data_to_transform, data_stat\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Корреляция признаков</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current space features: service\n",
      "Found corr matrix\n",
      "Start save matrix\n"
     ]
    }
   ],
   "source": [
    "# составим словарь небходимых данных и их признаков\n",
    "list_space_features = ['date','late','credit','relative','payments','service']\n",
    "\n",
    "# сформируем данные за последние 25 операций клиентов\n",
    "for space_features in list_space_features:\n",
    "    clear_output()\n",
    "    # добавим индекацию процесса\n",
    "    print('Current space features:',space_features)\n",
    "    # подгружаем данные\n",
    "    drop_id_stat_data = fp.ParquetFile('base_models/data/'+space_features+'_stat').to_pandas().drop('id',axis=1)\n",
    "    # сформируем матрицу взаимных корреляций\n",
    "    print('Found corr matrix')\n",
    "    corr_matrix = drop_id_stat_data.corr()\n",
    "    # сохраним матрицу взаимных корреляций на диск для быстрого воспроизведения\n",
    "    print('Start save matrix')\n",
    "    fp.write('base_models/data/corr_matrix_'+space_features,corr_matrix)\n",
    "    # удалим использованные данные\n",
    "    del drop_id_stat_data, corr_matrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <span style=\"color:DodgerBlue\">Формирование данных для обучения модели</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">Формирование тренировочного, валидационного, тестового и отложенного набора данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для формирования наборов данных выберем dataset service_stat\n",
    "transform_data_stat = fp.ParquetFile('features/base_models/service_stat').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим отложенный набор данных для проверки результирующей модели\n",
    "X_atrain, y_atrain, X_hold, y_hold = my_train_test_split(transform_data_stat.drop('id',axis=1),target['flag'], train_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставшиеся данные раздедлим на тренировочный и тестовый наборы\n",
    "X_train, y_train, X_test, y_test = my_train_test_split(X_atrain, y_atrain, train_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для валидации модели выделим данные из тренировочного набора\n",
    "X_train, y_train, X_valid, y_valid = my_train_test_split(X_train,y_train, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target flag\n",
      "0    0.964519\n",
      "1    0.035481\n",
      "Name: proportion, dtype: float64\n",
      "train flag\n",
      "0    0.964242\n",
      "1    0.035758\n",
      "Name: proportion, dtype: float64\n",
      "valid flag\n",
      "0    0.96487\n",
      "1    0.03513\n",
      "Name: proportion, dtype: float64\n",
      "test flag\n",
      "0    0.964631\n",
      "1    0.035369\n",
      "Name: proportion, dtype: float64\n",
      "hold flag\n",
      "0    0.963947\n",
      "1    0.036053\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# проверим сбалансированность данных по признаку flag\n",
    "print('target', target['flag'].value_counts(normalize=True))\n",
    "print('train', y_train.value_counts(normalize=True))\n",
    "print('valid', y_valid.value_counts(normalize=True))\n",
    "print('test',y_test.value_counts(normalize=True))\n",
    "print('hold',y_hold.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(641250, 69) (641250,)\n",
      "(71250, 69) (71250,)\n",
      "(2137500, 69) (2137500,)\n",
      "(150000, 69) (150000,)\n"
     ]
    }
   ],
   "source": [
    "# проверим размерности полученных наборов\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_valid.shape,y_valid.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "print(X_hold.shape,y_hold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712500"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]+X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для воспроизводимости кода и корректного сравнения различных моделей  \n",
    "завиксируем id для тренировочного, валидационного и тестового наборов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Спрячем данные в DataDfame для последующего сохраниения в csv\n",
    "train_id = pd.DataFrame(y_train.index)\n",
    "valid_id = pd.DataFrame(y_valid.index)\n",
    "test_id = pd.DataFrame(y_test.index)\n",
    "hold_id = pd.DataFrame(y_hold.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем полученные DataFrame\n",
    "train_id.to_csv('features/base_models/reproduce/train_id.csv',index=False)\n",
    "valid_id.to_csv('features/base_models/reproduce/valid_id.csv',index=False)\n",
    "test_id.to_csv('features/base_models/reproduce/test_id.csv',index=False)\n",
    "hold_id.to_csv('features/base_models/reproduce/hold_id.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгрузим DataFrame с id для тренировочного/валидационного/тестового набора данных\n",
    "train_id = pd.read_csv('features/base_models/reproduce/train_id.csv')['0'].to_list()\n",
    "valid_id = pd.read_csv('features/base_models/reproduce/valid_id.csv')['0'].to_list()\n",
    "test_id = pd.read_csv('features/base_models/reproduce/test_id.csv')['0'].to_list()\n",
    "hold_id = pd.read_csv('features/base_models/reproduce/hold_id.csv')['0'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подгрузим данные целевой переменной\n",
    "target = pd.read_csv('data_source/train_data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним тренировочный валидационный и тестовый целевой переменной\n",
    "# чтобы иметь возможность к ними обратиться\n",
    "target.loc[train_id].to_csv('target/target_train.csv',index=False)\n",
    "target.loc[valid_id].to_csv('target/target_valid.csv',index=False)\n",
    "target.loc[test_id].to_csv('target/target_test.csv',index=False)\n",
    "target.loc[hold_id].to_csv('target/target_hold.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">Формирование тренировочного, валидационного, тестового и отложенного набора на transform_data_torow_25 данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current space features: service_torow\n",
      "Current name_dataset: hold\n",
      "Start save service_torow_hold\n"
     ]
    }
   ],
   "source": [
    "# составим список пространств признаков\n",
    "list_spaces = ['date_torow','late_torow','credit_torow','relative_torow','payments_torow','service_torow']\n",
    "# составим словарь небходимых наборов данных\n",
    "dict_datasets = {\n",
    "    'train' : pd.read_csv('features/base_models/reproduce/train_id.csv')['0'].to_list(),\n",
    "    'valid' : pd.read_csv('features/base_models/reproduce/valid_id.csv')['0'].to_list(),\n",
    "    'test' : pd.read_csv('features/base_models/reproduce/test_id.csv')['0'].to_list(),\n",
    "    'hold' : pd.read_csv('features/base_models/reproduce/hold_id.csv')['0'].to_list()\n",
    "}\n",
    "# разобьем torow данные на обучающий/валидационный/тестовый/отложенный наборы\n",
    "for space_features in list_spaces:\n",
    "    for name_dataset in dict_datasets.keys():\n",
    "        clear_output()\n",
    "        # добавим индекацию процесса\n",
    "        print('Current space features:',space_features)\n",
    "        print('Current name_dataset:',name_dataset)\n",
    "        # загрузим необходимые данные\n",
    "        dataset_id = dict_datasets[name_dataset]\n",
    "        dataset = fp.ParquetFile('features/base_models/torow/'+space_features).to_pandas().set_index('id').iloc[dataset_id]\n",
    "        # сохраним отборанные данные\n",
    "        print('Start save '+space_features+'_'+name_dataset)\n",
    "        fp.write('features/base_models/torow/'+space_features+'_'+name_dataset,dataset)\n",
    "        # удалим использованные данные\n",
    "        del dataset, dataset_id\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <span style=\"color:RoyalBlue\">Формирование тренировочного, валидационного, тестового и отложенного набора на transform_data_stat данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current space features: service_stat\n",
      "Current name_dataset: hold\n",
      "Start save service_stat_hold\n"
     ]
    }
   ],
   "source": [
    "# составим список простраств признаков\n",
    "list_spaces = ['date_stat','late_stat','credit_stat','relative_stat','payments_stat','service_stat']\n",
    "# составим словарь небходимых наборов данных\n",
    "dict_datasets = {\n",
    "    'train' : pd.read_csv('features/base_models/reproduce/train_id.csv')['0'].to_list(),\n",
    "    'valid' : pd.read_csv('features/base_models/reproduce/valid_id.csv')['0'].to_list(),\n",
    "    'test' : pd.read_csv('features/base_models/reproduce/test_id.csv')['0'].to_list(),\n",
    "    'hold' : pd.read_csv('features/base_models/reproduce/hold_id.csv')['0'].to_list()\n",
    "}\n",
    "# разобьем stat данные на обучающий/валидационный/тестовый/отложенный наборы\n",
    "for space_features in list_spaces:\n",
    "    for name_dataset in dict_datasets.keys():\n",
    "        clear_output()\n",
    "        # добавим индекацию процесса\n",
    "        print('Current space features:',space_features)\n",
    "        print('Current name_dataset:',name_dataset)\n",
    "        # загрузим необходимые данные\n",
    "        dataset_id = dict_datasets[name_dataset]\n",
    "        dataset = fp.ParquetFile('features/base_models/stat/'+space_features).to_pandas().set_index('id').iloc[dataset_id]\n",
    "        # сохраним отборанные данные\n",
    "        print('Start save '+space_features+'_'+name_dataset)\n",
    "        fp.write('features/base_models/stat/'+space_features+'_'+name_dataset,dataset)\n",
    "        # удалим использованные данные\n",
    "        del dataset, dataset_id\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
