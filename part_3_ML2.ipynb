{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\logo.png\" alt=\"drawing\" style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  \n",
    "\n",
    "<span style=\"background-size: 600px;background:White;color:REd;font-size: 60px;font-family: Comic Sans MS\">Кредитный скоринг Альфа банка</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Задача</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "Кредитный скоринг – важнейшая банковская задача. Стандартным подходом к ее решению   \n",
    "является построение классических моделей машинного обучения, таких как логистическая   \n",
    "регрессия и градиентный бустинг, на табличных данных, в том числе используя агрегации  \n",
    "от каких-нибудь последовательных данных, например, транзакционных историй клиентов.   \n",
    "Альтернативный подход заключается в использовании последовательных данных “как есть”,   \n",
    "подавая их на вход рекуррентной нейронной сети.\n",
    "\n",
    "В этом соревновании участникам предлагается решить задачу кредитного скоринга клиентов   \n",
    "Альфа-Банка, используя только данные кредитных историй. [Источник](https://www.kaggle.com/competitions/alfa-bank-pd-credit-history)\n",
    "\n",
    "**Данные**\n",
    "\n",
    "Датасет соревнования устроен таким образом, что кредиты для тренировочной выборки взяты   \n",
    "за период в М месяцев, а кредиты для тестовой выборки взяты за последующие K месяцев.\n",
    "\n",
    "Каждая запись кредитной истории содержит самую разнообразную информацию о прошлом кредите   \n",
    "клиента, например, сумму, отношение клиента к кредиту, дату открытия и закрытия, информацию   \n",
    "о просрочках по платежам и др. Все публикуемые данные тщательно анонимизированы.\n",
    "\n",
    "Целевая переменная – бинарная величина, принимающая значения 0 и 1, где 1 соответствует   \n",
    "дефолту клиента по кредиту.\n",
    "\n",
    "\n",
    "**Проверка решений**\n",
    "\n",
    "Метрика соревнования – ROC AUC. Подробнее про метрику можно почитать, например, [здесь](https://dyakonov.org/2017/07/28/auc-roc-площадь-под-кривой-ошибок/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Используемые библиотеки</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# работа с регулярными выражениями\n",
    "import re\n",
    "\n",
    "# библиотеки для работы с табличными данными\n",
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "\n",
    "# генерация случайных чисел\n",
    "import random\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# библиотеки для построения графики\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# библиотеки для математических преобразований с массивами данных\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# библиотеки для работы с функциями(частичная передача аргументов в функцию)\n",
    "from functools import partial\n",
    "\n",
    "# библиотеки для работы со статистическими характеристиками\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# библиотеки для работы с pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Импортируем DBSCAN-кластеризацию\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# вставить картинку в Jupiter Notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# линейные модели машинного обучения\n",
    "from sklearn import linear_model\n",
    "\n",
    "# ансамбли моделей машинного обучения\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# поиск гиперпараметров модели\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "import optuna\n",
    "\n",
    " # метрики\n",
    "from sklearn import metrics\n",
    "\n",
    "# библиотека для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# сохранить полученные модели\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "# сборщик мусора\n",
    "import gc\n",
    "\n",
    "# для ограничения времени выполнения функции\n",
    "import signal\n",
    "import func_timeout\n",
    "\n",
    "# для отслеживания времени выполнения функции\n",
    "import time\n",
    "\n",
    "# очистить output\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:DodgerBlue\">Разработка инструментов преобразования данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция torow_transformer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция torow_transformer\n",
    "\n",
    "# Назначение: Преобразование признака столбца в признаки строки\n",
    "#               с сохранением обратной последовательности в признаке.\n",
    "#               (извлечение последних операций клиента)\n",
    "\n",
    "# Внешние переменные функции: DataFrame, n_last\n",
    "#   DataFrame - исоходый DataFrame\n",
    "#   n_last - необходимое счисло послдених операций клиента\n",
    "#   Структура DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: New DataFrame\n",
    "#   Признаки New DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1.1\n",
    "#       3. feature1.2\n",
    "#       4. feature1.3\n",
    "#       ...\n",
    "#       5. feature1.N\n",
    "#       6. feature2.1\n",
    "#       7. feature2.2\n",
    "#       ...\n",
    "#       8. feature2.N\n",
    "#       ...\n",
    "#   где featureX.1 - соотвествует последней операции клиента,\n",
    "#       featureX.2 - соотвествует предпоследней операции клиента,\n",
    "#       .....\n",
    "\n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных DataFrame\n",
    "# 3. формируем карту признаков dict_features:\n",
    "#    3.1. каждый признак кодируется следующим образом: 'fn',\n",
    "#         где n - порядковый номер признака\n",
    "#    3.2. полные имена признаков задаются следующим образом: 'feature_N',\n",
    "#         где N - порядок клиентской операции (большему N соотвествует, более ранняя операция)\n",
    "\n",
    "# 4. Преобразуем данные к массиву\n",
    "# 5. Группируем массив для каждого клиента\n",
    "# 6. К групированному массиву прменяем следующие преобразования:\n",
    "#    6.1. Обращаем порядок клиенских операций\n",
    "#    6.2. выбираем посление n_last операций\n",
    "#    6.3. Если число клиенски операций меньше чем n_last, дополняем их нулями\n",
    "# 7. Преобразуем полученные данные к DataFrame\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. pd_data - исходный DataFrame\n",
    "# 2. n_last - необходимое число последних операций клиента\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 5. dict_features - локальная карта признаков\n",
    "# 6. rn_id - список количества операций для каждого клиента\n",
    "# 6. array_data - данные преобразоанные к numpy-массиву\n",
    "# 6. split_array - сгрупированные по клиентам данные преобразованные\n",
    "\n",
    "# обьявлем функцию\n",
    "def torow_func(dict_params):\n",
    "    pd_data = dict_params['data']\n",
    "    n_last = dict_params['n_last']\n",
    "\n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "    \n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "\n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {}\n",
    "    \n",
    "    # заполним словарь dict_features\n",
    "    num_f=0\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for num_feature in range(1,n_last+1):\n",
    "            dict_features['f'+str(num_feature+num_f)] = feature+'_'+str(num_feature)\n",
    "        num_f+=n_last\n",
    "    \n",
    "\n",
    "    # формируем словарь rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)\n",
    "    \n",
    "    # определим порядок последующих преобразований в функции\n",
    "    def transform_array(array_id):\n",
    "        # обратим порядок клиентских операций \n",
    "        reverse_array_id = array_id[::,::-1]\n",
    "        # выбрем после n операций клиента\n",
    "        list_n_last = reverse_array_id[::,:n_last]\n",
    "        # если клиенских операций было меньше чем n_last\n",
    "        # дополним недастающие нулями и преобразуем данные к строке\n",
    "        if len(list_n_last[0])<n_last:\n",
    "            full_list_n_last = np.hstack((list_n_last,np.zeros((list_n_last.shape[0],n_last-len(list_n_last[0])),dtype='int64')))\n",
    "            # преобразуем список к строке\n",
    "            full_list_n_last = full_list_n_last.reshape(-1)\n",
    "        else:\n",
    "            full_list_n_last = list_n_last.reshape(-1)\n",
    "        return full_list_n_last\n",
    "\n",
    "    # применим transform_array преобразование к списку split_array\n",
    "    list_data = np.array(list(map(transform_array,split_array)))[:-1]\n",
    "    \n",
    "    # преобразуем полученные данные к dataframe\n",
    "    dataframe = pd.DataFrame(data=list_data, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "    \n",
    "    return dataframe, dict_features,rn_id\n",
    "\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "torow_transformer = FunctionTransformer(torow_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция features_from_transform_data_torow\n",
    "\n",
    "# Назначение: Извлечение из данных, над которомы совершено \n",
    "#             row_fich_transformer() преобразование, признаков  \n",
    "#             соотвествующих заданному числу последних  \n",
    "#             опреаций клиента n_last\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. n_last - необходимое число последних операций клиента\n",
    "#       2. n_groups - число групп признаков в transform_data_torow\n",
    "#       3. N_last - число последних операций клиента показанных в transform_data_torow\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. list_n_last_features - список признаков в transform_data_torow\n",
    "#    соотвествующий заданному числу n_last. \n",
    "    \n",
    "\n",
    "# обьявлем функцию\n",
    "def features_from_transform_data_torow(n_last,n_groups,N_last):\n",
    "    # создадим список под необходимые признаки\n",
    "    list_n_last_features = []\n",
    "    \n",
    "    # обьявим начальное значение в группе признаков\n",
    "    n_start = 0\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        for n in range(n_last):\n",
    "            list_n_last_features.append(n+n_start)\n",
    "        n_start+=N_last\n",
    "    \n",
    "    return list_n_last_features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция diff_feature</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция diff_feature\n",
    "\n",
    "# Назначение: Определение дифференциальных характеристик ряда \n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1.Series/np.array/list\n",
    "\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. diff_list - Список из значений:\n",
    "#                   1.1. speed - скорость изменения ряда;\n",
    "#                   1.2. accel - ускорение изменения ряда;\n",
    "#                   1.3. bias - смещение ряда;\n",
    "#                   1.4. pulse - импульс ряда;\n",
    "\n",
    "# обьявлем функцию\n",
    "def diff_feature(data):\n",
    "    # преобразуем данные к numpy массиву\n",
    "    data = np.array(data)\n",
    "    # расчитаем необходимые характеристики\n",
    "    speed = round(float(np.diff(data,1).mean()),2)\n",
    "    accel = round(float(np.diff(data,2).mean()),2)\n",
    "    bias = round(float(np.diff(data,1).sum()),2)\n",
    "    pulse = round(float(np.diff(data,2).sum()),2)\n",
    "    # сформируем из найденных значений в список\n",
    "    diff_list = [speed,accel,bias,pulse]\n",
    "    return diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция statistic_features</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция statistic_features\n",
    "\n",
    "# Назначение: Извлечение основных статистических характеристик\n",
    "#             из признаков в исходном DataFrame.\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. dataframe - таблица с данными. \n",
    "#    Признаки dataframe:\n",
    "#       1. id\n",
    "#       2. feature1_mean\n",
    "#       3. fearture1_hmean\n",
    "#       4. feature1_std\n",
    "#       5. feature1_min\n",
    "#       6. feature1_25%\n",
    "#       7. feature1_50%\n",
    "#       8. feature1_75%\n",
    "#       9. feature1_max\n",
    "#       10. feature1_mode\n",
    "#       11. feature1_frequency_mode\n",
    "#       12. feature2_mean\n",
    "#       ...\n",
    "    \n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных\n",
    "# 2. формируем карту признаков:\n",
    "#    2.1. каждый признак кодируется следующим образом: 'fn' где n - порядковый номер признака\n",
    "#    2.2. полные имена признаков задаются следующим образом:\n",
    "#         2.2.1 если в исходном dataframe признак бинарный, то: \"Исходное имя признака\"+\"binary\"\n",
    "#         2.2.2 если в исходном dataframe признак не бинарный, то: \"Исходное имя признака\"+\"Статистическая характеристика\"\n",
    "# 3. для каждого клиента по каждому признаку из исходного dataframe расчитываем статистические характеристики\n",
    "# 4. записываем полученные значение в новый dataframe\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. dict_agg_function - словарь из агригирующих функций\n",
    "#       keys: имена для обращения к функциям:\n",
    "#       values: lamda-функция, соотвествующей статистической характристики\n",
    "# 2. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. dict_features - локальная карта признаков\n",
    "# 5. k - номер признака в dict_features на текущей итерации\n",
    "# 6. dataframe - результирующий dataframe\n",
    "\n",
    "# обьявлем функцию\n",
    "def statistic_features(pd_data):\n",
    "    # формируем список из функций для статистических преобразований\n",
    "    # предусмотрим работу функций на случай, если в массиве данных всего 1 строка\n",
    "    dict_agg_function = {\n",
    "    'ptp' : lambda x: 0 if len(x) <= 3 else np.ptp(x),\n",
    "    'mean': lambda x: 0 if len(x) <= 3 else x.mean(), \n",
    "    'gmean' : lambda x: stats.gmean(x),   \n",
    "    'hmean': lambda x: stats.gmean(x),\n",
    "    'pmean25': lambda x: stats.pmean(x,25),\n",
    "    'pmean50': lambda x: stats.pmean(x,50),\n",
    "    'pmean75': lambda x: stats.pmean(x,75),\n",
    "    'expectile25': lambda x: stats.expectile(x,0.25),\n",
    "    'expectile50': lambda x: stats.expectile(x),\n",
    "    'expectile75': lambda x: stats.expectile(x,0.75),\n",
    "    'moment': lambda x: stats.moment(x),\n",
    "    'std': lambda x: 0 if len(x) <= 3 else np.std(x),\n",
    "    'min': lambda x: min(x),\n",
    "    '20%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=20),\n",
    "    '30%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=30),\n",
    "    '40%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=40),\n",
    "    '50%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=50),\n",
    "    '60%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=60),\n",
    "    '70%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=70),\n",
    "    'max': lambda x: max(x),\n",
    "    'mode': lambda x: statistics.mean(statistics.multimode(x)),\n",
    "    'frequency_mode': lambda x: round(list(x).count(statistics.multimode(x)[0])*len(statistics.multimode(x))/len(x),2),\n",
    "    'cov' : lambda x: 0 if len(x) <= 3 else np.cov(x),\n",
    "    'histogram' : lambda x: 0 if len(x) <= 3 else np.histogram(x)[1].mean(), \n",
    "    'speed': lambda x: 0 if len(x) <= 3 else diff_feature(x)[0],\n",
    "    'accel': lambda x: 0 if len(x) <= 3 else diff_feature(x)[1],\n",
    "    'bias': lambda x: 0 if len(x) <= 3 else diff_feature(x)[2],\n",
    "    'pulse': lambda x: 0 if len(x) <= 3  else diff_feature(x)[3]\n",
    "    } \n",
    "\n",
    "    # напишем функцию для преобразования массива до статистических характеристик\n",
    "    def stat_func(array_data): \n",
    "        # сформируем лист под результаты преобразования\n",
    "        list_for_result = []\n",
    "        # запишем все статистические харкетристики из словаря dict_agg_function\n",
    "        for func in dict_agg_function.values():\n",
    "            list_for_result.append(func(array_data))\n",
    "        return np.array(list_for_result).round(3)\n",
    "\n",
    "    # напишем функцию для применения функции stat_func к списку\n",
    "    def submap(list_data):\n",
    "        # расчитаем количество операция клиента\n",
    "        max_rn = len(list_data[0])\n",
    "        # получим статистические характристики массива\n",
    "        list_stat_features = np.array(list(map(stat_func,list_data))).reshape(-1)\n",
    "        return np.hstack((max_rn,list_stat_features))\n",
    "    \n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "\n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "    \n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {'f1':'count'}\n",
    "    k=1 # порядковый номер защифрованного признака\n",
    "\n",
    "    # заполним словарь dict_features\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for key_function in dict_agg_function.keys():\n",
    "            k+=1\n",
    "            dict_features['f'+str(k)] = feature+'_'+key_function\n",
    "\n",
    "    # формируем список rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)[:-1]\n",
    "\n",
    "    # получем статические характеристики признаков\n",
    "    stat_features = np.array(list(map(submap,split_array)))\n",
    "    \n",
    "    # Сформируем dataframe из полученных данных\n",
    "    dataframe = pd.DataFrame(data=stat_features, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "\n",
    "    return dataframe, dict_features\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "stat_transformer = FunctionTransformer(statistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция corr_transform_to_force</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция corr_transform_to_force\n",
    "\n",
    "# Назначение: из матрицы взаимных корреляций\n",
    "#             выделить не корелирующие признаки\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. df.corr() - матрица корреляций\n",
    "#           2. threshold - порог значимости корреляции:\n",
    "#               значение коэффициента корреляции, больше которого\n",
    "#               признаки считаются скоррелированными.\n",
    "\n",
    "# Пояснение: \n",
    "# Под силой корреляции будем понимать следующее: если коэффициент \n",
    "# коррелиции между признаками больше значения threshold, то принимаем,\n",
    "# что между признаками сильная корреляционная связь значение коэффициента \n",
    "# коррелияции заменяем на 1, иначе корреляционная связь слабая и значение \n",
    "# коээфициента корреляции заменяем на 0\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. corr_matrix - матрица корреляций(отражает силу корреляции)\n",
    "# 2. list_ncorr_features - список не скореллированных признаков\n",
    "# 3. corr_force - сила корреляции всей матрицы: отношение числа скоррелированных \n",
    "# признаков к числу всех признаков в матрице\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. coor_force - функция преобазующая значение\n",
    "#        коэффициента корряляции в силу корреляции\n",
    "# 2. corr_matrix - матрица отражающая силу корряляции между признаками\n",
    "# 3. max_corr - максимальное число взаимных корреляций между признаками\n",
    "# 4. list_ncorr_features - список не коррелируемых признаков\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def corr_transform_to_force(matrix,threshold=0.7):\n",
    "    list_features = matrix.index.tolist()\n",
    "    \n",
    "    \n",
    "    # создадим функцию для разметки матрицы корреляции\n",
    "    # 1 - корреляция признаков выше порога значимости threshold\n",
    "    # 0 - корреляция признаков ниже порога значимости threshold\n",
    "    corr_force = lambda x: 1 if x >threshold else 0\n",
    "    # выполним разметку матрицы корреляции\n",
    "    corr_matrix = matrix.map(lambda x: corr_force(x))\n",
    "    \n",
    "    # алгоритм отбора не коррелиарных признаков:\n",
    "    #   1. Найдем признак с наибольшим числом взаимных корреляций\n",
    "    #   2. удалим найденный признак\n",
    "    #   3. составим матрицу корреляций из отсавшися признаков\n",
    "    #   4. повторяем пункты 1-3 до тех пор пока в матрице не останутся \n",
    "    #       не коррелированные признаки\n",
    "\n",
    "    # ищем наибольшее число взаимных корреляций среди признаков\n",
    "    max_corr = corr_matrix.sum().max()\n",
    "\n",
    "    while max_corr > 1:\n",
    "        # определяем признак с наибольшим числом взаимных корреляций\n",
    "        max_corr_feature = corr_matrix.sum()[corr_matrix.sum()==corr_matrix.sum().max()].index[0]\n",
    "        # удалем признак из матрицы корреляций\n",
    "        corr_matrix = corr_matrix.drop(max_corr_feature).drop(max_corr_feature,axis=1)\n",
    "        max_corr = corr_matrix.sum().max()\n",
    "    # запишем не скоррелированные признаки в список\n",
    "    list_ncorr_features = corr_matrix.index.tolist()\n",
    "    # найдем силу корреляции всей матрицы как отношение\n",
    "    # количества скоррелированных признаков к всмеу количеству признаков\n",
    "    corr_force = round(1-len(list_ncorr_features)/len(list_features),3)\n",
    "    return corr_matrix, list_ncorr_features, corr_force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция search_DBSCAN_parameters</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция search_DBSCAN_parameters\n",
    "\n",
    "# Назначение: Для подбора eps и min_samples параметров,\n",
    "#               функция \"прогоняет\" DBSCAN кластеризацию \n",
    "#               с параметрами eps и min_samples\n",
    "#               примающими значения из заданного диапазона.\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data - dataframe для кластеризации\n",
    "#           2. r1 - начало диапазона\n",
    "#           3. r2 - конец диапазона  \n",
    "#           4. n - предпалагамое число кластеров      \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. data_cluster - кластеризация данных при различных \n",
    "#       значениях параметров eps и min_samples\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. parametr_range - диапазон изменения параметров\n",
    "# 2. dataframe_columns - колонки в результирующем dataframe\n",
    "# 3. data_cluster - результрующий dataframe\n",
    "# 4. index_cluster - текущая позиция в data_cluster\n",
    "# 5. clustering - кластеризатор\n",
    "# 6. list_cluster_values - список для заполнения текущими \n",
    "#                          значениями data_cluster\n",
    "\n",
    "# обьявлем функцию\n",
    "def search_DBSCAN_parameters(dataframe,r1,r2,n=3):\n",
    "    # задаем диапозон измениния параметров\n",
    "    parameter_range = range(r1,r2)\n",
    "    # формируем заготовку для результирующего dataframe\n",
    "    dataframe_columns = ['eps','min_samples',-1,0,1]\n",
    "    # проверим что задано не меньше минимального количества кластеров\n",
    "    if n<=3: \n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    else: \n",
    "        for claster in range(4,n+1):\n",
    "            dataframe_columns.append(claster-2)\n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    # задаем начально значение индекса в data_cluster\n",
    "    index_cluster = 0\n",
    "\n",
    "    # для подсчета обьектов в кластерах создадим dataframe\n",
    "    dataframe_count = pd.DataFrame()\n",
    "    \n",
    "    # \"прогоняем\" DBSCAN кластеризациию по диапазону параметров\n",
    "    for eps in parameter_range:\n",
    "        \n",
    "        for min_samples in parameter_range:\n",
    "            print('current eps:',eps,'  current min_samples:', min_samples, end='\\r')\n",
    "            # запускаем кластеризацию с текущими параметрами\n",
    "            clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(dataframe)\n",
    "            # добавлем к данным столбец с разметкой\n",
    "            dataframe_count['clater'] = clustering.labels_\n",
    "            # формируем пустой список для заполнения\n",
    "            list_cluster_values = []\n",
    "            # добавлеям в список текущие параметры\n",
    "            list_cluster_values.append(eps)\n",
    "            list_cluster_values.append(min_samples)\n",
    "            # добавлем в список количество обьктов в каждом кластере\n",
    "            for column in dataframe_columns[2:]:\n",
    "                list_cluster_values.append(len(dataframe_count['clater'][dataframe_count['clater']==column]))\n",
    "                \n",
    "            # заполняем dataframe  текущими данными\n",
    "            data_cluster.loc[index_cluster] = list_cluster_values\n",
    "            index_cluster +=1\n",
    "            # сбрасываем dataframe_count\n",
    "            dataframe_count = pd.DataFrame()\n",
    "    return data_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция generate_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция generate_samples\n",
    "\n",
    "# Назначение: для генерации индексов выборок данных\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. max - определяет максимальное значение множества\n",
    "#               из которого формируются выборки\n",
    "#           2. n - количество выборок\n",
    "#           3. k - мощность одной выборки\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список с выбороками\n",
    "\n",
    "# алгоритм работы:\n",
    "# 1. задаем отрезок натурального ряда N мощностью max и добавлем в него 0.\n",
    "#       In = N U {0}, I = {0,1,2,3,4,5,..,max}\n",
    "# 2. если мощность множества In больше, необходимого количества элементов\n",
    "#       cardo(In) > n x k , то из множества In формируем n случайных выборок\n",
    "# размера k без повторения.\n",
    "# 3. если мощность множества In меньше, необходимого количества элементов\n",
    "#       cardo(In) < n x k , то из множества In формируем случайные выборки\n",
    "# размера k без повторения, до тех пор пока не закончится множество In.\n",
    "# После, добираем недостающее количество выборок случайными выборками \n",
    "# размера k из множества In с повторением (bootstrap метод).\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def generate_samples(max,n,k,random_state = None):\n",
    "    # создадим список под результат\n",
    "    samples_list = []\n",
    "    # формируем множество натуральных числе от 0 до max\n",
    "    In = list(range(max+1))\n",
    "    # Будем выполнять код пока не наберем необходимого количества выборок\n",
    "    # нарушим порядок в множестве\n",
    "    In = shuffle(In,random_state=random_state)\n",
    "    # random.shuffle(In)\n",
    "\n",
    "    # задаим границы извлечения данных из In\n",
    "    In_start = 0\n",
    "    In_end = k\n",
    "    while len(samples_list) < n:\n",
    "        # сформируем список под одну выборку\n",
    "        sample = []\n",
    "        # первые списки будем наполнять значениеми из множества In\n",
    "        # без повторения, до тех пор пока все значения из множества In\n",
    "        # не распределяться по выборкам\n",
    "        if len(In)-In_end >= 0:\n",
    "            sample.extend(In[In_start:In_end])\n",
    "        else:                    \n",
    "            # если элементов во множестве In недостаточно,\n",
    "            # запоняем выборку \"остатками\" \n",
    "            sample.extend(In[In_start:])\n",
    "\n",
    "            # остальные данные заполняем методом bootstrap\n",
    "            # выполнем код пока не заполним выборку k значениями\n",
    "            while len(sample) < k:\n",
    "                # генерируем случайное число из диапазона от 0 до len(In)-1\n",
    "                random_index = randint(0,len(In)-1)\n",
    "                # добавляем значение из множества In с индексом random_index\n",
    "                # в список index_list\n",
    "                sample.append(In[random_index])\n",
    "\n",
    "        # после того как мы набрали значения в выборку отправлем ее в samples_list\n",
    "        samples_list.append(sample)\n",
    "        # переходим к следующим данным в множестве In\n",
    "        In_start+=k\n",
    "        In_end+=k\n",
    "\n",
    "    return samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция my_train_test_split</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X,y,random_state=42,train_size=0.8,):\n",
    "    # если разбиение без стратификации\n",
    "\n",
    "    # зададим число элементов в выборке train\n",
    "    len_train = round(len(y)*train_size)\n",
    "    # формируем множество натуральных чисел от 0 до max\n",
    "    list_random_index = list(range(len(y)))\n",
    "    # нарушим порядок в множестве\n",
    "    list_random_index = shuffle(list_random_index,random_state=random_state)\n",
    "    # формируем список индексов под train выборку\n",
    "    train_samples = list_random_index[:len_train]\n",
    "    # формируем список индексов под test выборку\n",
    "    test_samples = list_random_index[len_train:]\n",
    "    # выполнем код пока не заполним выборку k значениями\n",
    "   \n",
    "    X_train = X.iloc[train_samples]\n",
    "    y_train = y.iloc[train_samples]\n",
    "    X_test = X.iloc[test_samples]\n",
    "    y_test = y.iloc[test_samples]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция class_1_percent_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция class_1_percent_samples\n",
    "\n",
    "# Назначение: для генерации индексов сбалансированных выборок\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data_target - массив из id и значений класса\n",
    "#           2. class_1_percent - процент класса 1 в результирующей выборке\n",
    "#           3. random_state - параметр для обеспечения воспроизваодимости функции\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список со сблансированными выбороками\n",
    "\n",
    "# обьявлем функцию\n",
    "def class_1_percent_samples(data_target,class_1_percent,random_state = None):\n",
    "    # приведем данные к нужной форме\n",
    "    data_target = pd.DataFrame(data=np.array(data_target),columns =['id','flag'])\n",
    "    \n",
    "    # разделим клиентов  по признаку flag\n",
    "    flag_0 = data_target[data_target['flag']==0].reset_index(drop=True)\n",
    "    flag_1 = data_target[data_target['flag']==1].reset_index(drop=True)\n",
    "\n",
    "    # определим класс большинства\n",
    "    if flag_1.shape[0] > flag_0.shape[0]:\n",
    "        majority_class = flag_1\n",
    "        minority_class = flag_0\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(class_1_percent)/(1-class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    else:\n",
    "        majority_class = flag_0\n",
    "        minority_class = flag_1\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(1-class_1_percent)/(class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    \n",
    "    # сформируем список выбороки с заданным процентом класс 1\n",
    "    samples_list_id = minority_class['id'].values.tolist()+majority_class['id'].iloc[samples_majority_class[0]].tolist()\n",
    "\n",
    "    return samples_list_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Процесс машинного обучения (ML-Machine Learning)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи в рамках Machine Learning:\n",
    "1. Для решения задачи построем блендинг моделей. \n",
    "\n",
    "2. В качестве базовых и метамоделей рассмотрим следующие классические модели классификации:\n",
    "    - linear_model.LogisticRegression (Логистическая регрессия);\n",
    "    - RandomForestClassifier (Деревья решений);\n",
    "    - HistGradientBoostingClassifier (Градиентный бустинг).\n",
    "\n",
    "3. В результате, преобразования данных было получено два пространства признаков (torow и stat признаки),  \n",
    "состоящих из 6 подпространств:\n",
    "    - date features;\n",
    "    - late payments features; \n",
    "    - credit features;\n",
    "    - relative features;\n",
    "    - payments features;\n",
    "    - service features.\n",
    "\n",
    "4. На первом этапе построения потроения блендинга, сфокусируем обучение базовых моделей,   \n",
    "на каждом подпространстве в отдельности друго от друга.  \n",
    "\n",
    "5. На втором этапе построения блендинга, обучим несколько групп метамоделей.   \n",
    "Первая группа метамоделей в качестве метапризнаков использует предсказания базовых моделей,    \n",
    "обученных на пространстве признаков torow.  \n",
    "Вторая группа метамоделей в качестве метапризнаков использует предсказания базовых моделей,    \n",
    "обученных на пространстве признаков stat.\n",
    "\n",
    "6. На третьем этапе построения блендинга метамодель обучится на метапризнаках пространства \n",
    "torow и stat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\Blending.jpg\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:DodgerBlue\">Второй этап построения блендинга моделей (first metamodels) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Формирование данных для обучения first meta models</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем улучшить качество модели за счет блендинга.  \n",
    "Сформируем блендинга из рассмотренных базовых модей:\n",
    "- $Logistic$ $Regression$;\n",
    "- $Hist$ $Gradient$ $Boosting$ $Classifier$,  \n",
    "обученных на данных $transform$ $data$ $torow$ и $transform$ $data$ $stat$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Формирование метапризнаков из моделей обученных на transform data torow данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем качество моделей обученных на torow данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим пространство признаков\n",
    "dict_spaces = {\n",
    "    'date' : 8,\n",
    "    'late': 12,\n",
    "    'credit': 4,\n",
    "    'relative' : 6,\n",
    "    'payments': 25,\n",
    "    'service': 4}\n",
    "\n",
    "# сформируем список из базовых моделей\n",
    "list_base_models = ['LogisticRegression','HistGradientBoostingClassifier']\n",
    "\n",
    "# заготовка для общего списка метрик\n",
    "metrics_torow_pd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.585156</td>\n",
       "      <td>0.580831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575883</td>\n",
       "      <td>0.571258</td>\n",
       "      <td>0.067882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.494216</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>0.067875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588762</td>\n",
       "      <td>0.580936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.574669</td>\n",
       "      <td>0.568678</td>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.901318</td>\n",
       "      <td>0.036831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>0.670899</td>\n",
       "      <td>0.636532</td>\n",
       "      <td>0.095426</td>\n",
       "      <td>0.106272</td>\n",
       "      <td>0.086589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>0.656207</td>\n",
       "      <td>0.633204</td>\n",
       "      <td>0.069222</td>\n",
       "      <td>0.990012</td>\n",
       "      <td>0.035865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>0.646931</td>\n",
       "      <td>0.636672</td>\n",
       "      <td>0.067917</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>0.035153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>0.644888</td>\n",
       "      <td>0.634490</td>\n",
       "      <td>0.069238</td>\n",
       "      <td>0.991610</td>\n",
       "      <td>0.035871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.673225</td>\n",
       "      <td>0.636222</td>\n",
       "      <td>0.095333</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.094787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4201 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      roc_train  roc_valid  f1_score  recall_1  precision_1\n",
       "0      0.585156   0.580831  0.000000  0.000000     0.000000\n",
       "1      0.575883   0.571258  0.067882  1.000000     0.035133\n",
       "2      0.494216   0.488417  0.067875  1.000000     0.035130\n",
       "3      0.588762   0.580936  0.000000  0.000000     0.000000\n",
       "4      0.574669   0.568678  0.070770  0.901318     0.036831\n",
       "...         ...        ...       ...       ...          ...\n",
       "4196   0.670899   0.636532  0.095426  0.106272     0.086589\n",
       "4197   0.656207   0.633204  0.069222  0.990012     0.035865\n",
       "4198   0.646931   0.636672  0.067917  0.999201     0.035153\n",
       "4199   0.644888   0.634490  0.069238  0.991610     0.035871\n",
       "4200   0.673225   0.636222  0.095333  0.095885     0.094787\n",
       "\n",
       "[4201 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for base_model in list_base_models:\n",
    "    # сформируем общий список метрик моделей обученных на torow данных\n",
    "    for feature_space in dict_spaces.keys():\n",
    "        # загрузим обьект исcледования optuna\n",
    "        optuna_study_torow = optuna.load_study(study_name=base_model+'_'+feature_space+'_torow',\n",
    "                                storage='sqlite:///optuna_studies.db')\n",
    "        \n",
    "        # из полученного обьекта сформируем Data Frame\n",
    "        optuna_study_torow_pd = optuna_study_torow.trials_dataframe()[['values_0','values_1','values_2','values_3','values_4']].dropna()\n",
    "\n",
    "        # переименуем столбы\n",
    "        optuna_study_torow_pd.rename(columns={\n",
    "            'values_0': 'roc_train', \n",
    "            'values_1': 'roc_valid', \n",
    "            'values_2': 'f1_score', \n",
    "            'values_3': 'recall_1',\n",
    "            'values_4': 'precision_1'\n",
    "        },inplace=True)\n",
    "\n",
    "        # добавим текущще подпростраство в общий список\n",
    "        metrics_torow_pd = pd.concat([metrics_torow_pd,optuna_study_torow_pd],ignore_index=True)\n",
    "\n",
    "# посмотрим на результат\n",
    "metrics_torow_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4201.000000</td>\n",
       "      <td>4201.000000</td>\n",
       "      <td>4201.000000</td>\n",
       "      <td>4201.000000</td>\n",
       "      <td>4201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.638668</td>\n",
       "      <td>0.619358</td>\n",
       "      <td>0.072161</td>\n",
       "      <td>0.428013</td>\n",
       "      <td>0.066453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.050347</td>\n",
       "      <td>0.033692</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.370498</td>\n",
       "      <td>0.058417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.483961</td>\n",
       "      <td>0.485817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.595874</td>\n",
       "      <td>0.597988</td>\n",
       "      <td>0.067865</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>0.037762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.636853</td>\n",
       "      <td>0.623173</td>\n",
       "      <td>0.076725</td>\n",
       "      <td>0.346784</td>\n",
       "      <td>0.052280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.683712</td>\n",
       "      <td>0.635425</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.799041</td>\n",
       "      <td>0.082126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.747136</td>\n",
       "      <td>0.687650</td>\n",
       "      <td>0.141577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_train    roc_valid     f1_score     recall_1  precision_1\n",
       "count  4201.000000  4201.000000  4201.000000  4201.000000  4201.000000\n",
       "mean      0.638668     0.619358     0.072161     0.428013     0.066453\n",
       "std       0.050347     0.033692     0.034689     0.370498     0.058417\n",
       "min       0.483961     0.485817     0.000000     0.000000     0.000000\n",
       "25%       0.595874     0.597988     0.067865     0.062325     0.037762\n",
       "50%       0.636853     0.623173     0.076725     0.346784     0.052280\n",
       "75%       0.683712     0.635425     0.095658     0.799041     0.082126\n",
       "max       0.747136     0.687650     0.141577     1.000000     1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проанализируем метрики всех моделей обученных на torow данных\n",
    "metrics_torow_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для блендинга выберем модели c значениями метрик на валидационном наборе выше верхнего квартиля:\n",
    "- модели с значением метрики $ROC AUC \\geq 0.637$;\n",
    "- модели с значением метрики $precision_1 \\geq 0.081$;\n",
    "- модели с значением метрики $f1_{score} \\geq 0.1$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зададим минимальные значения метрик\n",
    "min_roc_auc = float(metrics_torow_pd.describe()['roc_valid']['75%'])\n",
    "min_precision = float(metrics_torow_pd.describe()['precision_1']['75%'])\n",
    "min_f1 = float(metrics_torow_pd.describe()['f1_score']['75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем заготовку под метаданные для обучения и проверики метамодели \n",
    "features_first_meta_torow = pd.read_csv('target/target_test.csv')\n",
    "\n",
    "# для выбора sceler преобразвания понадобится словарь\n",
    "dict_scalers ={\n",
    "    'MinMaxScaler':MinMaxScaler(),\n",
    "    'RobustScaler':RobustScaler(),\n",
    "    'StandardScaler':StandardScaler(),\n",
    "}\n",
    "\n",
    "# задаим количество отбираемых \"лучших\" моделей\n",
    "best_models_number = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Формирование метапризнаков от модели *Logistic Regression* обученной сбалансированными *transform data torow* данными</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем метапризнаки для каждого подпространства признаков\n",
    "for feature_space in dict_spaces.keys():\n",
    "    # обьявим количество признаков подпространства\n",
    "    count_features = dict_spaces[feature_space]\n",
    "\n",
    "    # загрузим обьект исcледования optuna\n",
    "    optuna_study_lr_torow = optuna.load_study(study_name='LogisticRegression_'+feature_space+'_torow',\n",
    "                               storage='sqlite:///optuna_studies.db')\n",
    "    \n",
    "    # из полученного обьекта сформируем Data Frame\n",
    "    optuna_study_lr_torow_pd = optuna_study_lr_torow.trials_dataframe().dropna()\n",
    "\n",
    "    # переименуем столбы\n",
    "    optuna_study_lr_torow_pd.rename(columns={\n",
    "        'values_0': 'roc_train', \n",
    "        'values_1': 'roc_valid', \n",
    "        'values_2': 'f1_score', \n",
    "        'values_3': 'recall_1',\n",
    "        'values_4': 'precision_1'\n",
    "    },inplace=True)\n",
    "\n",
    "    # удалим ненужные столбцы\n",
    "    optuna_study_lr_torow_pd.drop(['datetime_start','datetime_complete','state','duration'],axis=1, inplace=True)\n",
    "\n",
    "    # выберем точки с лучшими значениями метрики roc auc на валидационном наборе\n",
    "    best_roc_lr_torow = optuna_study_lr_torow_pd[optuna_study_lr_torow_pd['roc_valid']>=min_roc_auc].sort_values(by='roc_valid',ascending=False)[:best_models_number] \n",
    "\n",
    "    # выберем точки с лучшими значениями метрики precion класса 1 на валидационном наборе\n",
    "    best_precion_lr_torow = optuna_study_lr_torow_pd[optuna_study_lr_torow_pd['precision_1']>=min_precision].sort_values(by='precision_1',ascending=False)[:best_models_number]   \n",
    "\n",
    "    # выберем точки в которых максимальна метрика f1-score класса 1 (precision_1=recall_1)\n",
    "    best_f1_lr_torow = optuna_study_lr_torow_pd[optuna_study_lr_torow_pd['f1_score']>=min_f1].sort_values(by='f1_score',ascending=False)[:best_models_number]\n",
    "\n",
    "    # сформируем набор признаков для лучших моделей\n",
    "    best_models_lr_torow = pd.concat([best_roc_lr_torow,best_precion_lr_torow,best_f1_lr_torow]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # сформируем список индексов best_models_lr_torow\n",
    "    list_index = best_models_lr_torow.index\n",
    "\n",
    "    # сформируем  метапризнаки для обучения метамодели\n",
    "    # для этого необходиом обучить модель LogisticRegression с выбранными гиперпараметрами\n",
    "    for index in list_index:\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Current space: ', feature_space)\n",
    "        print('Number of indexes: ', max(list_index))\n",
    "        print('Current index: ', index)\n",
    "        \n",
    "        # сформируем необходимые параметры\n",
    "        C = best_models_lr_torow.loc[index,'params_C']\n",
    "        solver = best_models_lr_torow.loc[index,'params_solver']\n",
    "        class_weight = {0:best_models_lr_torow.loc[index,'params_class_0_weight'],\n",
    "                        1:best_models_lr_torow.loc[index,'params_class_1_weight']}\n",
    "        n_last = int(best_models_lr_torow.loc[index,'params_n_last'])\n",
    "        scaler = best_models_lr_torow.loc[index,'params_scaler']\n",
    "        class_1_percent = best_models_lr_torow.loc[index,'params_class_1_percent']\n",
    "        random_state = int(best_models_lr_torow.loc[index,'params_random_state'])\n",
    "\n",
    "        # с помощью функции features_from_transform_data_torow извлечем  \n",
    "        # из данных transform_data_torow\n",
    "        # признаки сооствествующие n_last последним клиенским операциям\n",
    "        list_n_last_features = features_from_transform_data_torow(n_last,count_features,25)\n",
    "\n",
    "        # сохраним list_n_last_features для дальнейшего воспроизведения\n",
    "        dump(list_n_last_features, 'models/base/list_n_last_features/'+'list_n_last_features_LRTR_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # обьявим scaler\n",
    "        scaler = dict_scalers[scaler]\n",
    "\n",
    "        # загружаем обучующие наборы\n",
    "        print('Loading train data')\n",
    "        X_train_pd = fp.ParquetFile('features/base/torow/'+feature_space+'_torow_train').to_pandas()\n",
    "        y_train_pd = pd.read_csv('target/target_train.csv')\n",
    "\n",
    "        # поготовим данные y_train_pd к работе с функцией class_1_percent_samples\n",
    "        y_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "        # подготовим данные для обучения модели\n",
    "        # с помощью функции class_1_percent_samples зададим долю \n",
    "        # класса 1\n",
    "        list_c1_percent_id = class_1_percent_samples(y_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "        # сформируем данные для обучения базовой модели\n",
    "        X_train_s_balanced = scaler.fit_transform(np.array(X_train_pd.loc[list_c1_percent_id])[:,list_n_last_features])\n",
    "        y_train_balanced = y_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "        # сохраним scaler для дальнейшего воспроизведения\n",
    "        dump(scaler, 'models/base/scalers/'+'scaler_torow_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # освободим память от \"тяжелых\" и ненужных файлов\n",
    "        del X_train_pd, y_train_pd\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('start train')\n",
    "        # обучаем модель LogisticRegression с наилучшеми параметрами\n",
    "        logistic_regression = linear_model.LogisticRegression(\n",
    "                C=C,\n",
    "                solver = solver,\n",
    "                class_weight=class_weight,\n",
    "                random_state=random_state,\n",
    "                max_iter=10000)\n",
    "        logistic_regression.fit(X_train_s_balanced,y_train_balanced)\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('finished train')\n",
    "\n",
    "        # сохраним модель для дальнейшего воспроизведения\n",
    "        dump(logistic_regression, 'models/base/'+'LRTR_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_train_s_balanced,y_train_balanced\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Loading test data')\n",
    "        # подгружаем данные для тестирования\n",
    "        X_test = fp.ParquetFile('features/base/torow/'+feature_space+'_torow_test').to_pandas().to_numpy()[:,list_n_last_features]\n",
    "        # сформируем данные для проверки модели\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "        # для метрик ROC AUC делаем предсказание модели в виде вероятности и записываем в методанные\n",
    "        features_first_meta_torow['LRTR_'+feature_space+'_'+str('0'+str(index))[-2:]] = logistic_regression.predict_proba(X_test_s)[:,1]\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_test_s, X_test\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "\n",
    "# сохраним полученные метапризнаки\n",
    "fp.write('features/firstmeta/features_first_meta_torow',features_first_meta_torow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Формирование метапризнаков от модели *Gradient Boosting Classifier* обученной сбалансированными *transform data torow* данными</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем метапризнаки для каждого подпространства признаков\n",
    "for feature_space in dict_spaces.keys():\n",
    "    # обьявим количество признаков подпространства\n",
    "    count_features = dict_spaces[feature_space]\n",
    "\n",
    "    # загрузим обьект исcледования optuna\n",
    "    optuna_study_gb_torow = optuna.load_study(study_name='HistGradientBoostingClassifier_'+feature_space+'_torow',\n",
    "                               storage='sqlite:///optuna_studies.db')\n",
    "    \n",
    "    # из полученного обьекта сформируем Data Frame\n",
    "    optuna_study_gb_torow_pd = optuna_study_gb_torow.trials_dataframe().dropna()\n",
    "\n",
    "    # переименуем столбы\n",
    "    optuna_study_gb_torow_pd.rename(columns={\n",
    "        'values_0': 'roc_train', \n",
    "        'values_1': 'roc_valid', \n",
    "        'values_2': 'f1_score', \n",
    "        'values_3': 'recall_1',\n",
    "        'values_4': 'precision_1'\n",
    "    },inplace=True)\n",
    "\n",
    "    # удалим ненужные столбцы\n",
    "    optuna_study_gb_torow_pd.drop(['datetime_start','datetime_complete','state','duration'],axis=1, inplace=True)\n",
    "\n",
    "    # выберем точки с лучшими значениями метрики roc auc на валидационном наборе\n",
    "    best_roc_gb_torow = optuna_study_gb_torow_pd[optuna_study_gb_torow_pd['roc_valid']>=min_roc_auc].sort_values(by='roc_valid',ascending=False)[:best_models_number] \n",
    "\n",
    "    # выберем точки с лучшими значениями метрики precion класса 1 на валидационном наборе\n",
    "    best_precion_gb_torow = optuna_study_gb_torow_pd[optuna_study_gb_torow_pd['precision_1']>=min_precision].sort_values(by='precision_1',ascending=False)[:best_models_number]   \n",
    "\n",
    "    # выберем точки в которых максимальна метрика f1-score класса 1 (precision_1=recall_1)\n",
    "    best_f1_gb_torow = optuna_study_gb_torow_pd[optuna_study_gb_torow_pd['f1_score']>=min_f1].sort_values(by='f1_score',ascending=False)[:best_models_number]\n",
    "\n",
    "    # сформируем набор признаков для лучших моделей\n",
    "    best_models_gb_torow = pd.concat([best_roc_gb_torow,best_precion_gb_torow,best_f1_gb_torow]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # сформируем список индексов best_models_gb_torow\n",
    "    list_index = best_models_gb_torow.index\n",
    "\n",
    "    # сформируем  метапризнаки для обучения метамодели\n",
    "    # для этого необходиом обучить модель HistGradientBoostingClassifier с выбранными гиперпараметрами\n",
    "    for index in list_index:\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Current space: ', feature_space)\n",
    "        print('Number of indexes: ', max(list_index))\n",
    "        print('Current index: ', index)\n",
    "        \n",
    "        # сформируем необходимые параметры\n",
    "        learning_rate = best_models_gb_torow.loc[index,'params_learning_rate']\n",
    "        max_iter = best_models_gb_torow.loc[index,'params_max_iter']\n",
    "        max_leaf_nodes = best_models_gb_torow.loc[index,'params_max_leaf_nodes']\n",
    "        max_depth = best_models_gb_torow.loc[index,'params_max_depth']\n",
    "        min_samples_leaf = best_models_gb_torow.loc[index,'params_min_samples_leaf']\n",
    "        l2_regularization = best_models_gb_torow.loc[index,'params_l2_regularization']\n",
    "        max_features = best_models_gb_torow.loc[index,'params_max_features']\n",
    "        class_weight = {0:best_models_gb_torow.loc[index,'params_class_0_weight'],\n",
    "                        1:best_models_gb_torow.loc[index,'params_class_1_weight']}\n",
    "        n_last = best_models_gb_torow.loc[index,'params_n_last']\n",
    "        class_1_percent = best_models_gb_torow.loc[index,'params_class_1_percent']\n",
    "        random_state = best_models_gb_torow.loc[index,'params_random_state']\n",
    "\n",
    "        # с помощью функции features_from_transform_data_torow извлечем  \n",
    "        # из данных transform_data_torow\n",
    "        # признаки сооствествующие n_last последним клиенским операциям\n",
    "        list_n_last_features = features_from_transform_data_torow(n_last,count_features,25)\n",
    "\n",
    "            # сохраним list_n_last_features для дальнейшего воспроизведения\n",
    "        dump(list_n_last_features, 'models/base/list_n_last_features/'+'list_n_last_features_GBTR_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # загружаем обучующие наборы\n",
    "        print('Loading train data')\n",
    "        X_train_pd = fp.ParquetFile('features/base/torow/'+feature_space+'_torow_train').to_pandas()\n",
    "        y_train_pd = pd.read_csv('target/target_train.csv')\n",
    "\n",
    "        # поготовим данные y_train_pd к работе с функцией class_1_percent_samples\n",
    "        y_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "        # подготовим данные для обучения модели\n",
    "        # с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "        list_c1_percent_id = class_1_percent_samples(y_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "        # сформируем данные для обучения модели\n",
    "        X_train_balanced = np.array(X_train_pd.loc[list_c1_percent_id])[:,list_n_last_features]\n",
    "        y_train_balanced = y_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "        # освободим память от \"тяжелых\" и ненужных файлов\n",
    "        del X_train_pd, y_train_pd\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('start train')\n",
    "        # обучаем модель HistGradientBoostingClassifier с наилучшеми параметрами\n",
    "        gradient_boosting = HistGradientBoostingClassifier(\n",
    "                learning_rate = learning_rate,\n",
    "                max_iter = max_iter,\n",
    "                max_leaf_nodes = max_leaf_nodes,\n",
    "                max_depth = max_depth,\n",
    "                min_samples_leaf = min_samples_leaf,\n",
    "                l2_regularization = l2_regularization,\n",
    "                max_features = max_features,\n",
    "                class_weight = class_weight,\n",
    "                random_state=42)\n",
    "        gradient_boosting.fit(X_train_balanced,y_train_balanced)\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('finished train')\n",
    "\n",
    "        # сохраним модель для дальнейшего воспроизведения\n",
    "        dump(gradient_boosting, 'models/base/'+'GBTR_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_train_balanced,y_train_balanced\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Loading test data')\n",
    "        # подгружаем данные для тестирования\n",
    "        X_test = fp.ParquetFile('features/base/torow/'+feature_space+'_torow_test').to_pandas().to_numpy()[:,list_n_last_features]\n",
    "        # для метрик ROC AUC делаем предсказание модели в виде вероятности и записываем в методанные\n",
    "        features_first_meta_torow['GBTR_'+feature_space+'_'+str('0'+str(index))[-2:]] = gradient_boosting.predict_proba(X_test)[:,1]\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_test\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "\n",
    "# сохраним полученные метапризнаки\n",
    "fp.write('features/firstmeta/features_first_meta_torow',features_first_meta_torow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Формирование метапризнаков из моделей обученных на transform data stat данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем качество моделей обученных на stat данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим пространство признаков\n",
    "dict_spaces = {\n",
    "    'date' : 8,\n",
    "    'late': 12,\n",
    "    'credit': 4,\n",
    "    'relative' : 6,\n",
    "    'payments': 25,\n",
    "    'service': 4}\n",
    "\n",
    "# сформируем список из базовых моделей\n",
    "list_base_models = ['LogisticRegression','HistGradientBoostingClassifier']\n",
    "\n",
    "# заготовка для общего списка метрик\n",
    "metrics_stat_pd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.570751</td>\n",
       "      <td>0.566699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586950</td>\n",
       "      <td>0.581612</td>\n",
       "      <td>0.085706</td>\n",
       "      <td>0.241710</td>\n",
       "      <td>0.052088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571209</td>\n",
       "      <td>0.569071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.581467</td>\n",
       "      <td>0.577797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583944</td>\n",
       "      <td>0.577297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>0.658287</td>\n",
       "      <td>0.641242</td>\n",
       "      <td>0.099141</td>\n",
       "      <td>0.536956</td>\n",
       "      <td>0.054612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>0.662682</td>\n",
       "      <td>0.641477</td>\n",
       "      <td>0.110810</td>\n",
       "      <td>0.351578</td>\n",
       "      <td>0.065770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>0.659675</td>\n",
       "      <td>0.641895</td>\n",
       "      <td>0.093336</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.095258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0.658903</td>\n",
       "      <td>0.640088</td>\n",
       "      <td>0.068801</td>\n",
       "      <td>0.989213</td>\n",
       "      <td>0.035640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>0.644104</td>\n",
       "      <td>0.639686</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>0.098816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      roc_train  roc_valid  f1_score  recall_1  precision_1\n",
       "0      0.570751   0.566699  0.000000  0.000000     0.000000\n",
       "1      0.586950   0.581612  0.085706  0.241710     0.052088\n",
       "2      0.571209   0.569071  0.000000  0.000000     0.000000\n",
       "3      0.581467   0.577797  0.000000  0.000000     0.000000\n",
       "4      0.583944   0.577297  0.000000  0.000000     0.000000\n",
       "...         ...        ...       ...       ...          ...\n",
       "3595   0.658287   0.641242  0.099141  0.536956     0.054612\n",
       "3596   0.662682   0.641477  0.110810  0.351578     0.065770\n",
       "3597   0.659675   0.641895  0.093336  0.091490     0.095258\n",
       "3598   0.658903   0.640088  0.068801  0.989213     0.035640\n",
       "3599   0.644104   0.639686  0.086370  0.076708     0.098816\n",
       "\n",
       "[3600 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for base_model in list_base_models:\n",
    "    # сформируем общий список метрик моделей обученных на torow данных\n",
    "    for feature_space in dict_spaces.keys():\n",
    "        # загрузим обьект исcледования optuna\n",
    "        optuna_study_stat = optuna.load_study(study_name=base_model+'_'+feature_space+'_stat',\n",
    "                                storage='sqlite:///optuna_studies.db')\n",
    "        \n",
    "        # из полученного обьекта сформируем Data Frame\n",
    "        optuna_study_stat_pd = optuna_study_stat.trials_dataframe()[['values_0','values_1','values_2','values_3','values_4']].dropna()\n",
    "\n",
    "        # переименуем столбы\n",
    "        optuna_study_stat_pd.rename(columns={\n",
    "            'values_0': 'roc_train', \n",
    "            'values_1': 'roc_valid', \n",
    "            'values_2': 'f1_score', \n",
    "            'values_3': 'recall_1',\n",
    "            'values_4': 'precision_1'\n",
    "        },inplace=True)\n",
    "\n",
    "        # добавим текущще подпростраство в общий список\n",
    "        metrics_stat_pd = pd.concat([metrics_stat_pd,optuna_study_stat_pd],ignore_index=True)\n",
    "\n",
    "# посмотрим на результат\n",
    "metrics_stat_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.645177</td>\n",
       "      <td>0.630109</td>\n",
       "      <td>0.076166</td>\n",
       "      <td>0.270411</td>\n",
       "      <td>0.076349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.039747</td>\n",
       "      <td>0.025895</td>\n",
       "      <td>0.038083</td>\n",
       "      <td>0.274393</td>\n",
       "      <td>0.058625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.569406</td>\n",
       "      <td>0.566111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.616844</td>\n",
       "      <td>0.056656</td>\n",
       "      <td>0.041550</td>\n",
       "      <td>0.052477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.640839</td>\n",
       "      <td>0.626101</td>\n",
       "      <td>0.088602</td>\n",
       "      <td>0.181382</td>\n",
       "      <td>0.067204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.677664</td>\n",
       "      <td>0.657208</td>\n",
       "      <td>0.104749</td>\n",
       "      <td>0.414702</td>\n",
       "      <td>0.092143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.739731</td>\n",
       "      <td>0.671994</td>\n",
       "      <td>0.134240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_train    roc_valid     f1_score     recall_1  precision_1\n",
       "count  3600.000000  3600.000000  3600.000000  3600.000000  3600.000000\n",
       "mean      0.645177     0.630109     0.076166     0.270411     0.076349\n",
       "std       0.039747     0.025895     0.038083     0.274393     0.058625\n",
       "min       0.569406     0.566111     0.000000     0.000000     0.000000\n",
       "25%       0.614292     0.616844     0.056656     0.041550     0.052477\n",
       "50%       0.640839     0.626101     0.088602     0.181382     0.067204\n",
       "75%       0.677664     0.657208     0.104749     0.414702     0.092143\n",
       "max       0.739731     0.671994     0.134240     1.000000     1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проанализируем метрики всех моделей обученных на torow данных\n",
    "metrics_stat_pd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для блендинга выберем модели c значениями метрик на валидационном наборе выше верхнего квартиля:\n",
    "- модели с значением метрики $ROC AUC \\geq 0.652$;\n",
    "- модели с значением метрики $precision_1 \\geq 0.092$;\n",
    "- модели с значением метрики $f1_{score} \\geq 0.1$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зададим минимальные значения метрик\n",
    "min_roc_auc = float(metrics_stat_pd.describe()['roc_valid']['75%'])\n",
    "min_precision = float(metrics_stat_pd.describe()['precision_1']['75%'])\n",
    "min_f1 = float(metrics_stat_pd.describe()['f1_score']['75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем заготовки под метаданные для обучения и проверики метамодели \n",
    "features_first_meta_stat = pd.read_csv('target/target_test.csv')\n",
    "\n",
    "# для выбора scaler преобразвания понадобится словарь\n",
    "dict_scalers ={\n",
    "    'MinMaxScaler':MinMaxScaler(),\n",
    "    'RobustScaler':RobustScaler(),\n",
    "    'StandardScaler':StandardScaler(),\n",
    "}\n",
    "\n",
    "# задаим количество отбираемых \"лучших\" моделей\n",
    "best_models_number = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Формирование метапризнаков от модели *Logistic Regression* обученной сбалансированными *transform data stat* данными</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем метапризнаки для каждого подпространства признаков\n",
    "for feature_space in dict_spaces.keys():\n",
    "    # обьявим количество признаков подпространства\n",
    "    count_features = dict_spaces[feature_space]\n",
    "\n",
    "    # для работы функции corr_transform_to_force\n",
    "    # подгрузим DataFrame с матрицей корреляции признаков\n",
    "    corr_matrix = fp.ParquetFile('features/base/stat/corr_matrix_'+feature_space).to_pandas()\n",
    "\n",
    "    # загрузим обьект исcледования optuna\n",
    "    optuna_study_lr_stat = optuna.load_study(study_name='LogisticRegression_'+feature_space+'_stat',\n",
    "                               storage='sqlite:///optuna_studies.db')\n",
    "    \n",
    "    # из полученного обьекта сформируем Data Frame\n",
    "    optuna_study_lr_stat_pd = optuna_study_lr_stat.trials_dataframe().dropna()\n",
    "\n",
    "    # переименуем столбы\n",
    "    optuna_study_lr_stat_pd.rename(columns={\n",
    "        'values_0': 'roc_train', \n",
    "        'values_1': 'roc_valid', \n",
    "        'values_2': 'f1_score', \n",
    "        'values_3': 'recall_1',\n",
    "        'values_4': 'precision_1'\n",
    "    },inplace=True)\n",
    "\n",
    "    # удалим ненужные столбцы\n",
    "    optuna_study_lr_stat_pd.drop(['datetime_start','datetime_complete','state','duration'],axis=1, inplace=True)\n",
    "\n",
    "    # выберем точки с лучшими значениями метрики roc auc на валидационном наборе\n",
    "    best_roc_lr_stat = optuna_study_lr_stat_pd[optuna_study_lr_stat_pd['roc_valid']>=min_roc_auc].sort_values(by='roc_valid',ascending=False)[:best_models_number] \n",
    "\n",
    "    # выберем точки с лучшими значениями метрики precion класса 1 на валидационном наборе\n",
    "    best_precion_lr_stat = optuna_study_lr_stat_pd[optuna_study_lr_stat_pd['precision_1']>=min_precision].sort_values(by='precision_1',ascending=False)[:best_models_number]   \n",
    "\n",
    "    # выберем точки в которых максимальна метрика f1-score класса 1 (precision_1=recall_1)\n",
    "    best_f1_lr_stat = optuna_study_lr_stat_pd[optuna_study_lr_stat_pd['f1_score']>=min_f1].sort_values(by='f1_score',ascending=False)[:best_models_number]\n",
    "\n",
    "    # сформируем набор признаков для лучших моделей\n",
    "    best_models_lr_stat = pd.concat([best_roc_lr_stat,best_precion_lr_stat,best_f1_lr_stat]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # сформируем список индексов best_models_lr_stat\n",
    "    list_index = best_models_lr_stat.index\n",
    "\n",
    "    # сформируем  метапризнаки для обучения метамодели\n",
    "    # для этого необходиом обучить модель LogisticRegression с выбранными гиперпараметрами\n",
    "    for index in list_index:\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Current space: ', feature_space)\n",
    "        print('Number of indexes: ', max(list_index))\n",
    "        print('Current index: ', index)\n",
    "        \n",
    "        # сформируем необходимые параметры\n",
    "        C = best_models_lr_stat.loc[index,'params_C']\n",
    "        solver = best_models_lr_stat.loc[index,'params_solver']\n",
    "        class_weight = {0:best_models_lr_stat.loc[index,'params_class_0_weight'],\n",
    "                        1:best_models_lr_stat.loc[index,'params_class_1_weight']}\n",
    "        threshold = best_models_lr_stat.loc[index,'params_threshold']\n",
    "        scaler = best_models_lr_stat.loc[index,'params_scaler']\n",
    "        class_1_percent = best_models_lr_stat.loc[index,'params_class_1_percent']\n",
    "        random_state = int(best_models_lr_stat.loc[index,'params_random_state'])\n",
    "\n",
    "        # с помощью функции corr_transform_to_force получим\n",
    "        # список не коррелируемых признаков\n",
    "        list_ncorr_features = corr_transform_to_force(corr_matrix,threshold=threshold)[1]\n",
    "\n",
    "        # сохраним list_ncorr_features для дальнейшего воспроизведения\n",
    "        dump(list_ncorr_features, 'models/base/list_ncorr_features/'+'list_ncorr_features_LRST_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # обьявим scaler\n",
    "        scaler = dict_scalers[scaler]\n",
    "\n",
    "        # сформируем данные для анализа сбалансированности обучающих данных\n",
    "        print('Loading train data')\n",
    "        X_train_pd = fp.ParquetFile('features/base/stat/'+feature_space+'_stat_train').to_pandas()[list_ncorr_features]\n",
    "        y_train_pd = pd.read_csv('target/target_train.csv')\n",
    "\n",
    "        # поготовим данные y_train_pd к работе с функцией class_1_percent_samples\n",
    "        y_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "        # подготовим данные для обучения модели\n",
    "        # с помощью функции class_1_percent_samples зададим долю \n",
    "        # класса 1\n",
    "        list_c1_percent_id = class_1_percent_samples(y_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "        # подготовим данные для обучения\n",
    "        X_train_s_balanced = scaler.fit_transform(X_train_pd.loc[list_c1_percent_id])\n",
    "        y_train_balanced = y_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "        # сохраним scaler для дальнейшего воспроизведения\n",
    "        dump(scaler, 'models/base/scalers/'+'scaler_stat_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # освободим память от \"тяжелых\" и ненужных файлов\n",
    "        del X_train_pd, y_train_pd\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('start train')\n",
    "        # обучаем модель LogisticRegression с наилучшеми параметрами\n",
    "        logistic_regression = linear_model.LogisticRegression(\n",
    "                C=C,\n",
    "                solver = solver,\n",
    "                class_weight=class_weight,\n",
    "                random_state=random_state,\n",
    "                max_iter=10000)\n",
    "        logistic_regression.fit(X_train_s_balanced,y_train_balanced)\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('finished train')\n",
    "\n",
    "        # сохраним модель для дальнейшего воспроизведения\n",
    "        dump(logistic_regression, 'models/base/'+'LRST_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_train_s_balanced,y_train_balanced\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Loading test data')\n",
    "        # подгружаем данные для тестирования\n",
    "        X_test = fp.ParquetFile('features/base/stat/'+feature_space+'_stat_test').to_pandas()[list_ncorr_features]\n",
    "        # сформируем данные для проверки модели\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "        # для метрик ROC AUC делаем предсказание модели в виде вероятности и записываем в методанные\n",
    "        features_first_meta_stat['LRST_'+feature_space+'_'+str('0'+str(index))[-2:]] = logistic_regression.predict_proba(X_test_s)[:,1]\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_test_s, X_test\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "\n",
    "# сохраним полученные метапризнаки\n",
    "fp.write('features/firstmeta/features_first_meta_stat',features_first_meta_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Формирование метапризнаков от модели *Gradient Boosting Classifier* обученной сбалансированными *transform data stat* данными</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем метапризнаки для каждого подпространства признаков\n",
    "for feature_space in dict_spaces.keys():\n",
    "    # обьявим количество признаков подпространства\n",
    "    count_features = dict_spaces[feature_space]\n",
    "\n",
    "    # для работы функции corr_transform_to_force\n",
    "    # подгрузим DataFrame с матрицей корреляции признаков\n",
    "    corr_matrix = fp.ParquetFile('features/base/stat/corr_matrix_'+feature_space).to_pandas()\n",
    "\n",
    "    # загрузим обьект исcледования optuna\n",
    "    optuna_study_gb_stat = optuna.load_study(study_name='HistGradientBoostingClassifier_'+feature_space+'_stat',\n",
    "                               storage='sqlite:///optuna_studies.db')\n",
    "    \n",
    "    # из полученного обьекта сформируем Data Frame\n",
    "    optuna_study_gb_stat_pd = optuna_study_gb_stat.trials_dataframe().dropna()\n",
    "\n",
    "    # переименуем столбы\n",
    "    optuna_study_gb_stat_pd.rename(columns={\n",
    "        'values_0': 'roc_train', \n",
    "        'values_1': 'roc_valid', \n",
    "        'values_2': 'f1_score', \n",
    "        'values_3': 'recall_1',\n",
    "        'values_4': 'precision_1'\n",
    "    },inplace=True)\n",
    "\n",
    "    # удалим ненужные столбцы\n",
    "    optuna_study_gb_stat_pd.drop(['datetime_start','datetime_complete','state','duration'],axis=1, inplace=True)\n",
    "\n",
    "    # выберем точки с лучшими значениями метрики roc auc на валидационном наборе\n",
    "    best_roc_gb_stat = optuna_study_gb_stat_pd[optuna_study_gb_stat_pd['roc_valid']>=min_roc_auc].sort_values(by='roc_valid',ascending=False)[:best_models_number] \n",
    "\n",
    "    # выберем точки с лучшими значениями метрики precion класса 1 на валидационном наборе\n",
    "    best_precion_gb_stat = optuna_study_gb_stat_pd[optuna_study_gb_stat_pd['precision_1']>=min_precision].sort_values(by='precision_1',ascending=False)[:best_models_number]   \n",
    "\n",
    "    # выберем точки в которых максимальна метрика f1-score класса 1 (precision_1=recall_1)\n",
    "    best_f1_gb_stat = optuna_study_gb_stat_pd[optuna_study_gb_stat_pd['f1_score']>=min_f1].sort_values(by='f1_score',ascending=False)[:best_models_number]\n",
    "\n",
    "    # сформируем набор признаков для лучших моделей\n",
    "    best_models_gb_stat = pd.concat([best_roc_gb_stat,best_precion_gb_stat,best_f1_gb_stat]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # сформируем список индексов best_models_gb_stat\n",
    "    list_index = best_models_gb_stat.index\n",
    "\n",
    "    # сформируем  метапризнаки для обучения метамодели\n",
    "    # для этого необходиом обучить модель HistGradientBoostingClassifier с выбранными гиперпараметрами\n",
    "    for index in list_index:\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Current space: ', feature_space)\n",
    "        print('Number of indexes: ', max(list_index))\n",
    "        print('Current index: ', index)\n",
    "        \n",
    "        # сформируем необходимые параметры\n",
    "        learning_rate = best_models_gb_stat.loc[index,'params_learning_rate']\n",
    "        max_iter = best_models_gb_stat.loc[index,'params_max_iter']\n",
    "        max_leaf_nodes = best_models_gb_stat.loc[index,'params_max_leaf_nodes']\n",
    "        max_depth = best_models_gb_stat.loc[index,'params_max_depth']\n",
    "        min_samples_leaf = best_models_gb_stat.loc[index,'params_min_samples_leaf']\n",
    "        l2_regularization = best_models_gb_stat.loc[index,'params_l2_regularization']\n",
    "        max_features = best_models_gb_stat.loc[index,'params_max_features']\n",
    "        class_weight = {0:best_models_gb_stat.loc[index,'params_class_0_weight'],\n",
    "                        1:best_models_gb_stat.loc[index,'params_class_1_weight']}\n",
    "        threshold = best_models_gb_stat.loc[index,'params_threshold']\n",
    "        class_1_percent = best_models_gb_stat.loc[index,'params_class_1_percent']\n",
    "        random_state = best_models_gb_stat.loc[index,'params_random_state']\n",
    "\n",
    "        # с помощью функции corr_transform_to_force получим\n",
    "        # список не коррелируемых признаков\n",
    "        list_ncorr_features = corr_transform_to_force(corr_matrix,threshold=threshold)[1]\n",
    "\n",
    "        # сохраним list_ncorr_features для дальнейшего воспроизведения\n",
    "        dump(list_ncorr_features, 'models/base/list_ncorr_features/'+'list_ncorr_features_GBST_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # сформируем данные для анализа сбалансированности обучающих данных\n",
    "        print('Loading train data')\n",
    "        X_train_pd = fp.ParquetFile('features/base/stat/'+feature_space+'_stat_train').to_pandas()[list_ncorr_features]\n",
    "        y_train_pd = pd.read_csv('target/target_train.csv')\n",
    "\n",
    "        # поготовим данные y_train_pd к работе с функцией class_1_percent_samples\n",
    "        y_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "        # подготовим данные для обучения модели\n",
    "        # с помощью функции class_1_percent_samples зададим долю \n",
    "        # класса 1\n",
    "        list_c1_percent_id = class_1_percent_samples(y_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "        # подготовим данные для обучения\n",
    "        X_train_balanced = X_train_pd.loc[list_c1_percent_id]\n",
    "        y_train_balanced = y_train_pd.loc[list_c1_percent_id]['flag']\n",
    "\n",
    "        # освободим память от \"тяжелых\" и ненужных файлов\n",
    "        del X_train_pd, y_train_pd\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('start train')\n",
    "        # обучаем модель HistGradientBoostingClassifier с наилучшеми параметрами\n",
    "        gradient_boosting = HistGradientBoostingClassifier(\n",
    "                learning_rate = learning_rate,\n",
    "                max_iter = max_iter,\n",
    "                max_leaf_nodes = max_leaf_nodes,\n",
    "                max_depth = max_depth,\n",
    "                min_samples_leaf = min_samples_leaf,\n",
    "                l2_regularization = l2_regularization,\n",
    "                max_features = max_features,\n",
    "                class_weight = class_weight,\n",
    "                random_state=42)\n",
    "        gradient_boosting.fit(X_train_balanced,y_train_balanced)\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('finished train')\n",
    "\n",
    "        # сохраним модель для дальнейшего воспроизведения\n",
    "        dump(gradient_boosting, 'models/base/'+'GBST_'+feature_space+'_'+str('0'+str(index))[-2:]+'.joblib')\n",
    "\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_train_balanced,y_train_balanced\n",
    "        gc.collect()\n",
    "\n",
    "        # для того чтобы следить за выполнением цикла введем индикатор\n",
    "        print('Loading test data')\n",
    "        # подгружаем данные для тестирования\n",
    "        X_test = fp.ParquetFile('features/base/stat/'+feature_space+'_stat_test').to_pandas()[list_ncorr_features]\n",
    "        # для метрик ROC AUC делаем предсказание модели в виде вероятности и записываем в методанные\n",
    "        features_first_meta_stat['GBST_'+feature_space+'_'+str('0'+str(index))[-2:]] = gradient_boosting.predict_proba(X_test)[:,1]\n",
    "        # удаляем крупные файлы чтобы высвободить память \n",
    "        del X_test\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "\n",
    "# сохраним полученные метапризнаки\n",
    "fp.write('features/firstmeta/features_first_meta_stat',features_first_meta_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "      <th>LRST_date_00</th>\n",
       "      <th>LRST_date_01</th>\n",
       "      <th>LRST_date_02</th>\n",
       "      <th>LRST_date_03</th>\n",
       "      <th>LRST_date_04</th>\n",
       "      <th>LRST_date_05</th>\n",
       "      <th>LRST_date_06</th>\n",
       "      <th>LRST_date_07</th>\n",
       "      <th>...</th>\n",
       "      <th>GBST_service_20</th>\n",
       "      <th>GBST_service_21</th>\n",
       "      <th>GBST_service_22</th>\n",
       "      <th>GBST_service_23</th>\n",
       "      <th>GBST_service_24</th>\n",
       "      <th>GBST_service_25</th>\n",
       "      <th>GBST_service_26</th>\n",
       "      <th>GBST_service_27</th>\n",
       "      <th>GBST_service_28</th>\n",
       "      <th>GBST_service_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1444093</td>\n",
       "      <td>0</td>\n",
       "      <td>0.228598</td>\n",
       "      <td>0.286011</td>\n",
       "      <td>0.306310</td>\n",
       "      <td>0.316349</td>\n",
       "      <td>0.284873</td>\n",
       "      <td>0.305438</td>\n",
       "      <td>0.294386</td>\n",
       "      <td>0.263173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513262</td>\n",
       "      <td>0.486053</td>\n",
       "      <td>0.516937</td>\n",
       "      <td>0.492899</td>\n",
       "      <td>0.493076</td>\n",
       "      <td>0.509266</td>\n",
       "      <td>0.497604</td>\n",
       "      <td>0.533499</td>\n",
       "      <td>0.521207</td>\n",
       "      <td>0.524906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1254992</td>\n",
       "      <td>0</td>\n",
       "      <td>0.207357</td>\n",
       "      <td>0.258086</td>\n",
       "      <td>0.254560</td>\n",
       "      <td>0.263676</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>0.262898</td>\n",
       "      <td>0.287953</td>\n",
       "      <td>0.223825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442961</td>\n",
       "      <td>0.433920</td>\n",
       "      <td>0.476977</td>\n",
       "      <td>0.394242</td>\n",
       "      <td>0.423567</td>\n",
       "      <td>0.432123</td>\n",
       "      <td>0.409902</td>\n",
       "      <td>0.443103</td>\n",
       "      <td>0.462180</td>\n",
       "      <td>0.477327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792358</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397426</td>\n",
       "      <td>0.444097</td>\n",
       "      <td>0.454559</td>\n",
       "      <td>0.470724</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.492430</td>\n",
       "      <td>0.469526</td>\n",
       "      <td>0.400029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496953</td>\n",
       "      <td>0.504692</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>0.503799</td>\n",
       "      <td>0.503056</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.503217</td>\n",
       "      <td>0.531735</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.536485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>712455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.218732</td>\n",
       "      <td>0.291654</td>\n",
       "      <td>0.294577</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.284281</td>\n",
       "      <td>0.319613</td>\n",
       "      <td>0.289171</td>\n",
       "      <td>0.257887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378542</td>\n",
       "      <td>0.378537</td>\n",
       "      <td>0.388864</td>\n",
       "      <td>0.374779</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.377065</td>\n",
       "      <td>0.367994</td>\n",
       "      <td>0.411746</td>\n",
       "      <td>0.408434</td>\n",
       "      <td>0.400087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1955754</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274674</td>\n",
       "      <td>0.335005</td>\n",
       "      <td>0.351431</td>\n",
       "      <td>0.371115</td>\n",
       "      <td>0.340883</td>\n",
       "      <td>0.375659</td>\n",
       "      <td>0.351630</td>\n",
       "      <td>0.297714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455626</td>\n",
       "      <td>0.461780</td>\n",
       "      <td>0.465002</td>\n",
       "      <td>0.460135</td>\n",
       "      <td>0.466949</td>\n",
       "      <td>0.461478</td>\n",
       "      <td>0.456959</td>\n",
       "      <td>0.493403</td>\n",
       "      <td>0.489076</td>\n",
       "      <td>0.493711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137495</th>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214208</td>\n",
       "      <td>0.295340</td>\n",
       "      <td>0.297255</td>\n",
       "      <td>0.300528</td>\n",
       "      <td>0.311420</td>\n",
       "      <td>0.301392</td>\n",
       "      <td>0.316672</td>\n",
       "      <td>0.262315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294878</td>\n",
       "      <td>0.271255</td>\n",
       "      <td>0.294946</td>\n",
       "      <td>0.294923</td>\n",
       "      <td>0.319590</td>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.304694</td>\n",
       "      <td>0.311352</td>\n",
       "      <td>0.282347</td>\n",
       "      <td>0.313741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137496</th>\n",
       "      <td>2278100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.259074</td>\n",
       "      <td>0.313675</td>\n",
       "      <td>0.311566</td>\n",
       "      <td>0.323382</td>\n",
       "      <td>0.353013</td>\n",
       "      <td>0.310781</td>\n",
       "      <td>0.322435</td>\n",
       "      <td>0.292823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218474</td>\n",
       "      <td>0.230880</td>\n",
       "      <td>0.221046</td>\n",
       "      <td>0.240721</td>\n",
       "      <td>0.241160</td>\n",
       "      <td>0.242898</td>\n",
       "      <td>0.227068</td>\n",
       "      <td>0.241012</td>\n",
       "      <td>0.263821</td>\n",
       "      <td>0.261778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137497</th>\n",
       "      <td>211942</td>\n",
       "      <td>0</td>\n",
       "      <td>0.378021</td>\n",
       "      <td>0.449593</td>\n",
       "      <td>0.461322</td>\n",
       "      <td>0.481972</td>\n",
       "      <td>0.473538</td>\n",
       "      <td>0.467989</td>\n",
       "      <td>0.462929</td>\n",
       "      <td>0.413047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511684</td>\n",
       "      <td>0.510308</td>\n",
       "      <td>0.515278</td>\n",
       "      <td>0.513717</td>\n",
       "      <td>0.510441</td>\n",
       "      <td>0.509326</td>\n",
       "      <td>0.502068</td>\n",
       "      <td>0.539162</td>\n",
       "      <td>0.541887</td>\n",
       "      <td>0.545609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137498</th>\n",
       "      <td>2272240</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244668</td>\n",
       "      <td>0.312287</td>\n",
       "      <td>0.319336</td>\n",
       "      <td>0.332818</td>\n",
       "      <td>0.335232</td>\n",
       "      <td>0.331659</td>\n",
       "      <td>0.323031</td>\n",
       "      <td>0.292594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207959</td>\n",
       "      <td>0.235565</td>\n",
       "      <td>0.231229</td>\n",
       "      <td>0.220327</td>\n",
       "      <td>0.220996</td>\n",
       "      <td>0.230447</td>\n",
       "      <td>0.230896</td>\n",
       "      <td>0.238040</td>\n",
       "      <td>0.240353</td>\n",
       "      <td>0.243872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137499</th>\n",
       "      <td>2649258</td>\n",
       "      <td>0</td>\n",
       "      <td>0.210047</td>\n",
       "      <td>0.255736</td>\n",
       "      <td>0.280470</td>\n",
       "      <td>0.282874</td>\n",
       "      <td>0.273899</td>\n",
       "      <td>0.269251</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>0.239966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349793</td>\n",
       "      <td>0.319250</td>\n",
       "      <td>0.330279</td>\n",
       "      <td>0.360142</td>\n",
       "      <td>0.324089</td>\n",
       "      <td>0.345974</td>\n",
       "      <td>0.332169</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.383021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2137500 rows × 344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  flag  LRST_date_00  LRST_date_01  LRST_date_02  \\\n",
       "0        1444093     0      0.228598      0.286011      0.306310   \n",
       "1        1254992     0      0.207357      0.258086      0.254560   \n",
       "2        1792358     0      0.397426      0.444097      0.454559   \n",
       "3         712455     0      0.218732      0.291654      0.294577   \n",
       "4        1955754     0      0.274674      0.335005      0.351431   \n",
       "...          ...   ...           ...           ...           ...   \n",
       "2137495      423     0      0.214208      0.295340      0.297255   \n",
       "2137496  2278100     0      0.259074      0.313675      0.311566   \n",
       "2137497   211942     0      0.378021      0.449593      0.461322   \n",
       "2137498  2272240     0      0.244668      0.312287      0.319336   \n",
       "2137499  2649258     0      0.210047      0.255736      0.280470   \n",
       "\n",
       "         LRST_date_03  LRST_date_04  LRST_date_05  LRST_date_06  LRST_date_07  \\\n",
       "0            0.316349      0.284873      0.305438      0.294386      0.263173   \n",
       "1            0.263676      0.261145      0.262898      0.287953      0.223825   \n",
       "2            0.470724      0.459195      0.492430      0.469526      0.400029   \n",
       "3            0.302511      0.284281      0.319613      0.289171      0.257887   \n",
       "4            0.371115      0.340883      0.375659      0.351630      0.297714   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "2137495      0.300528      0.311420      0.301392      0.316672      0.262315   \n",
       "2137496      0.323382      0.353013      0.310781      0.322435      0.292823   \n",
       "2137497      0.481972      0.473538      0.467989      0.462929      0.413047   \n",
       "2137498      0.332818      0.335232      0.331659      0.323031      0.292594   \n",
       "2137499      0.282874      0.273899      0.269251      0.263053      0.239966   \n",
       "\n",
       "         ...  GBST_service_20  GBST_service_21  GBST_service_22  \\\n",
       "0        ...         0.513262         0.486053         0.516937   \n",
       "1        ...         0.442961         0.433920         0.476977   \n",
       "2        ...         0.496953         0.504692         0.506555   \n",
       "3        ...         0.378542         0.378537         0.388864   \n",
       "4        ...         0.455626         0.461780         0.465002   \n",
       "...      ...              ...              ...              ...   \n",
       "2137495  ...         0.294878         0.271255         0.294946   \n",
       "2137496  ...         0.218474         0.230880         0.221046   \n",
       "2137497  ...         0.511684         0.510308         0.515278   \n",
       "2137498  ...         0.207959         0.235565         0.231229   \n",
       "2137499  ...         0.349793         0.319250         0.330279   \n",
       "\n",
       "         GBST_service_23  GBST_service_24  GBST_service_25  GBST_service_26  \\\n",
       "0               0.492899         0.493076         0.509266         0.497604   \n",
       "1               0.394242         0.423567         0.432123         0.409902   \n",
       "2               0.503799         0.503056         0.507792         0.503217   \n",
       "3               0.374779         0.370453         0.377065         0.367994   \n",
       "4               0.460135         0.466949         0.461478         0.456959   \n",
       "...                  ...              ...              ...              ...   \n",
       "2137495         0.294923         0.319590         0.294372         0.304694   \n",
       "2137496         0.240721         0.241160         0.242898         0.227068   \n",
       "2137497         0.513717         0.510441         0.509326         0.502068   \n",
       "2137498         0.220327         0.220996         0.230447         0.230896   \n",
       "2137499         0.360142         0.324089         0.345974         0.332169   \n",
       "\n",
       "         GBST_service_27  GBST_service_28  GBST_service_29  \n",
       "0               0.533499         0.521207         0.524906  \n",
       "1               0.443103         0.462180         0.477327  \n",
       "2               0.531735         0.540401         0.536485  \n",
       "3               0.411746         0.408434         0.400087  \n",
       "4               0.493403         0.489076         0.493711  \n",
       "...                  ...              ...              ...  \n",
       "2137495         0.311352         0.282347         0.313741  \n",
       "2137496         0.241012         0.263821         0.261778  \n",
       "2137497         0.539162         0.541887         0.545609  \n",
       "2137498         0.238040         0.240353         0.243872  \n",
       "2137499         0.368714         0.362868         0.383021  \n",
       "\n",
       "[2137500 rows x 344 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_first_meta_stat = fp.ParquetFile('features/firstmeta/features_first_meta_stat').to_pandas()\n",
    "features_first_meta_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Формирование тренировочного, валидационного и тестового набора данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_first_meta_stat = fp.ParquetFile('features/firstmeta/features_first_meta_stat').to_pandas().iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим из features_first_meta_stat тренировочную часть\n",
    "MX_train, my_train, MX_test, my_test = my_train_test_split(features_first_meta_stat.set_index('id').drop(['flag'],axis=1),\n",
    "                                                           features_first_meta_stat.set_index('id')['flag'], train_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раздедлим обучающий набор на тренировочную и валидационную части\n",
    "MX_train, my_train, MX_valid, my_valid = my_train_test_split(MX_train,my_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для воспроизводимости кода и корректного сравнения   \n",
    "различных моделей завиксируем id для тренировочногоб валидационного   \n",
    "и тестосого наьоров данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Спрячем данные в DataDfame для последующего сохраниения в csv\n",
    "mf_train_id = pd.DataFrame(my_train.index)\n",
    "mf_valid_id = pd.DataFrame(my_valid.index)\n",
    "mf_test_id = pd.DataFrame(my_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем полученные DataFrame\n",
    "mf_train_id.to_csv('features/firstmeta/mf_train_id.csv')\n",
    "mf_valid_id.to_csv('features/firstmeta/mf_valid_id.csv')\n",
    "mf_test_id.to_csv('features/firstmeta/mf_test_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598500\n",
      "149625\n",
      "1389375\n"
     ]
    }
   ],
   "source": [
    "# подгрузим DataFrame с id для тренировочного/валидационного/тестового набора данных\n",
    "mf_train_id = pd.read_csv('features/firstmeta/mf_train_id.csv')['id'].to_list()\n",
    "mf_valid_id = pd.read_csv('features/firstmeta/mf_valid_id.csv')['id'].to_list()\n",
    "mf_test_id = pd.read_csv('features/firstmeta/mf_test_id.csv')['id'].to_list()\n",
    "\n",
    "print(len(mf_train_id))\n",
    "print(len(mf_valid_id))\n",
    "print(len(mf_test_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним тренировочный валидационный и тестовый целевой переменной\n",
    "# чтобы иметь возможность к ними обратиться\n",
    "my_train.to_csv('features/firstmeta/mfy_train.csv')\n",
    "my_valid.to_csv('features/firstmeta/mfy_valid.csv')\n",
    "my_test.to_csv('features/firstmeta/mfy_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Формирование тренировочного, валидационного и тестового набора на features_first_meta_torow данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LRTR_date_00</th>\n",
       "      <th>LRTR_date_01</th>\n",
       "      <th>LRTR_date_02</th>\n",
       "      <th>LRTR_date_03</th>\n",
       "      <th>LRTR_date_04</th>\n",
       "      <th>LRTR_date_05</th>\n",
       "      <th>LRTR_date_06</th>\n",
       "      <th>LRTR_date_07</th>\n",
       "      <th>LRTR_date_08</th>\n",
       "      <th>LRTR_late_00</th>\n",
       "      <th>...</th>\n",
       "      <th>GBTR_service_32</th>\n",
       "      <th>GBTR_service_33</th>\n",
       "      <th>GBTR_service_34</th>\n",
       "      <th>GBTR_service_35</th>\n",
       "      <th>GBTR_service_36</th>\n",
       "      <th>GBTR_service_37</th>\n",
       "      <th>GBTR_service_38</th>\n",
       "      <th>GBTR_service_39</th>\n",
       "      <th>GBTR_service_40</th>\n",
       "      <th>GBTR_service_41</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1444093</th>\n",
       "      <td>0.186788</td>\n",
       "      <td>0.209557</td>\n",
       "      <td>0.170073</td>\n",
       "      <td>0.193175</td>\n",
       "      <td>0.159265</td>\n",
       "      <td>0.195228</td>\n",
       "      <td>0.246425</td>\n",
       "      <td>0.204595</td>\n",
       "      <td>0.176149</td>\n",
       "      <td>0.028055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491388</td>\n",
       "      <td>0.455937</td>\n",
       "      <td>0.472278</td>\n",
       "      <td>0.488735</td>\n",
       "      <td>0.444002</td>\n",
       "      <td>0.499014</td>\n",
       "      <td>0.544666</td>\n",
       "      <td>0.510633</td>\n",
       "      <td>0.485606</td>\n",
       "      <td>0.458330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254992</th>\n",
       "      <td>0.239044</td>\n",
       "      <td>0.259862</td>\n",
       "      <td>0.221379</td>\n",
       "      <td>0.244904</td>\n",
       "      <td>0.189888</td>\n",
       "      <td>0.223594</td>\n",
       "      <td>0.284012</td>\n",
       "      <td>0.298423</td>\n",
       "      <td>0.217998</td>\n",
       "      <td>0.035261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440126</td>\n",
       "      <td>0.461601</td>\n",
       "      <td>0.385527</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.479083</td>\n",
       "      <td>0.387934</td>\n",
       "      <td>0.472670</td>\n",
       "      <td>0.409566</td>\n",
       "      <td>0.366930</td>\n",
       "      <td>0.375410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792358</th>\n",
       "      <td>0.334716</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.290805</td>\n",
       "      <td>0.342053</td>\n",
       "      <td>0.294882</td>\n",
       "      <td>0.341959</td>\n",
       "      <td>0.405733</td>\n",
       "      <td>0.380906</td>\n",
       "      <td>0.320389</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503452</td>\n",
       "      <td>0.531439</td>\n",
       "      <td>0.528214</td>\n",
       "      <td>0.529280</td>\n",
       "      <td>0.523508</td>\n",
       "      <td>0.512008</td>\n",
       "      <td>0.512459</td>\n",
       "      <td>0.514849</td>\n",
       "      <td>0.485838</td>\n",
       "      <td>0.503537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712455</th>\n",
       "      <td>0.202599</td>\n",
       "      <td>0.212664</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>0.205899</td>\n",
       "      <td>0.177517</td>\n",
       "      <td>0.207779</td>\n",
       "      <td>0.271352</td>\n",
       "      <td>0.237662</td>\n",
       "      <td>0.191038</td>\n",
       "      <td>0.024818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374689</td>\n",
       "      <td>0.389504</td>\n",
       "      <td>0.427483</td>\n",
       "      <td>0.408035</td>\n",
       "      <td>0.421883</td>\n",
       "      <td>0.404411</td>\n",
       "      <td>0.384824</td>\n",
       "      <td>0.394359</td>\n",
       "      <td>0.375932</td>\n",
       "      <td>0.388628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955754</th>\n",
       "      <td>0.299312</td>\n",
       "      <td>0.325026</td>\n",
       "      <td>0.262535</td>\n",
       "      <td>0.299460</td>\n",
       "      <td>0.261249</td>\n",
       "      <td>0.298108</td>\n",
       "      <td>0.370666</td>\n",
       "      <td>0.336419</td>\n",
       "      <td>0.283721</td>\n",
       "      <td>0.039503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459335</td>\n",
       "      <td>0.480053</td>\n",
       "      <td>0.486514</td>\n",
       "      <td>0.490217</td>\n",
       "      <td>0.471711</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>0.494188</td>\n",
       "      <td>0.460956</td>\n",
       "      <td>0.433987</td>\n",
       "      <td>0.459799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 356 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LRTR_date_00  LRTR_date_01  LRTR_date_02  LRTR_date_03  LRTR_date_04  \\\n",
       "id                                                                              \n",
       "1444093      0.186788      0.209557      0.170073      0.193175      0.159265   \n",
       "1254992      0.239044      0.259862      0.221379      0.244904      0.189888   \n",
       "1792358      0.334716      0.373801      0.290805      0.342053      0.294882   \n",
       "712455       0.202599      0.212664      0.188178      0.205899      0.177517   \n",
       "1955754      0.299312      0.325026      0.262535      0.299460      0.261249   \n",
       "\n",
       "         LRTR_date_05  LRTR_date_06  LRTR_date_07  LRTR_date_08  LRTR_late_00  \\\n",
       "id                                                                              \n",
       "1444093      0.195228      0.246425      0.204595      0.176149      0.028055   \n",
       "1254992      0.223594      0.284012      0.298423      0.217998      0.035261   \n",
       "1792358      0.341959      0.405733      0.380906      0.320389      0.054827   \n",
       "712455       0.207779      0.271352      0.237662      0.191038      0.024818   \n",
       "1955754      0.298108      0.370666      0.336419      0.283721      0.039503   \n",
       "\n",
       "         ...  GBTR_service_32  GBTR_service_33  GBTR_service_34  \\\n",
       "id       ...                                                      \n",
       "1444093  ...         0.491388         0.455937         0.472278   \n",
       "1254992  ...         0.440126         0.461601         0.385527   \n",
       "1792358  ...         0.503452         0.531439         0.528214   \n",
       "712455   ...         0.374689         0.389504         0.427483   \n",
       "1955754  ...         0.459335         0.480053         0.486514   \n",
       "\n",
       "         GBTR_service_35  GBTR_service_36  GBTR_service_37  GBTR_service_38  \\\n",
       "id                                                                            \n",
       "1444093         0.488735         0.444002         0.499014         0.544666   \n",
       "1254992         0.448575         0.479083         0.387934         0.472670   \n",
       "1792358         0.529280         0.523508         0.512008         0.512459   \n",
       "712455          0.408035         0.421883         0.404411         0.384824   \n",
       "1955754         0.490217         0.471711         0.470499         0.494188   \n",
       "\n",
       "         GBTR_service_39  GBTR_service_40  GBTR_service_41  \n",
       "id                                                          \n",
       "1444093         0.510633         0.485606         0.458330  \n",
       "1254992         0.409566         0.366930         0.375410  \n",
       "1792358         0.514849         0.485838         0.503537  \n",
       "712455          0.394359         0.375932         0.388628  \n",
       "1955754         0.460956         0.433987         0.459799  \n",
       "\n",
       "[5 rows x 356 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_first_meta_torow = fp.ParquetFile('features/firstmeta/features_first_meta_torow').to_pandas().set_index('id').drop(['flag'],axis=1)\n",
    "features_first_meta_torow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним тренировочный валидационный и тестовый наборы данных\n",
    "# чтобы иметь возможность к ними обратиться\n",
    "fp.write('features/firstmeta/features_first_meta_torow_train',features_first_meta_torow.loc[mf_train_id])\n",
    "fp.write('features/firstmeta/features_first_meta_torow_valid',features_first_meta_torow.loc[mf_valid_id])\n",
    "fp.write('features/firstmeta/features_first_meta_torow_test',features_first_meta_torow.loc[mf_test_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Формирование тренировочного, валидационного и тестового набора на features_first_meta_stat данных</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LRST_date_00</th>\n",
       "      <th>LRST_date_01</th>\n",
       "      <th>LRST_date_02</th>\n",
       "      <th>LRST_date_03</th>\n",
       "      <th>LRST_date_04</th>\n",
       "      <th>LRST_date_05</th>\n",
       "      <th>LRST_date_06</th>\n",
       "      <th>LRST_date_07</th>\n",
       "      <th>LRST_late_00</th>\n",
       "      <th>LRST_late_01</th>\n",
       "      <th>...</th>\n",
       "      <th>GBST_service_20</th>\n",
       "      <th>GBST_service_21</th>\n",
       "      <th>GBST_service_22</th>\n",
       "      <th>GBST_service_23</th>\n",
       "      <th>GBST_service_24</th>\n",
       "      <th>GBST_service_25</th>\n",
       "      <th>GBST_service_26</th>\n",
       "      <th>GBST_service_27</th>\n",
       "      <th>GBST_service_28</th>\n",
       "      <th>GBST_service_29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1444093</th>\n",
       "      <td>0.228598</td>\n",
       "      <td>0.286011</td>\n",
       "      <td>0.306310</td>\n",
       "      <td>0.316349</td>\n",
       "      <td>0.284873</td>\n",
       "      <td>0.305438</td>\n",
       "      <td>0.294386</td>\n",
       "      <td>0.263173</td>\n",
       "      <td>0.087456</td>\n",
       "      <td>0.091904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513262</td>\n",
       "      <td>0.486053</td>\n",
       "      <td>0.516937</td>\n",
       "      <td>0.492899</td>\n",
       "      <td>0.493076</td>\n",
       "      <td>0.509266</td>\n",
       "      <td>0.497604</td>\n",
       "      <td>0.533499</td>\n",
       "      <td>0.521207</td>\n",
       "      <td>0.524906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254992</th>\n",
       "      <td>0.207357</td>\n",
       "      <td>0.258086</td>\n",
       "      <td>0.254560</td>\n",
       "      <td>0.263676</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>0.262898</td>\n",
       "      <td>0.287953</td>\n",
       "      <td>0.223825</td>\n",
       "      <td>0.143654</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442961</td>\n",
       "      <td>0.433920</td>\n",
       "      <td>0.476977</td>\n",
       "      <td>0.394242</td>\n",
       "      <td>0.423567</td>\n",
       "      <td>0.432123</td>\n",
       "      <td>0.409902</td>\n",
       "      <td>0.443103</td>\n",
       "      <td>0.462180</td>\n",
       "      <td>0.477327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792358</th>\n",
       "      <td>0.397426</td>\n",
       "      <td>0.444097</td>\n",
       "      <td>0.454559</td>\n",
       "      <td>0.470724</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.492430</td>\n",
       "      <td>0.469526</td>\n",
       "      <td>0.400029</td>\n",
       "      <td>0.105415</td>\n",
       "      <td>0.114457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496953</td>\n",
       "      <td>0.504692</td>\n",
       "      <td>0.506555</td>\n",
       "      <td>0.503799</td>\n",
       "      <td>0.503056</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.503217</td>\n",
       "      <td>0.531735</td>\n",
       "      <td>0.540401</td>\n",
       "      <td>0.536485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712455</th>\n",
       "      <td>0.218732</td>\n",
       "      <td>0.291654</td>\n",
       "      <td>0.294577</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.284281</td>\n",
       "      <td>0.319613</td>\n",
       "      <td>0.289171</td>\n",
       "      <td>0.257887</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.076709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378542</td>\n",
       "      <td>0.378537</td>\n",
       "      <td>0.388864</td>\n",
       "      <td>0.374779</td>\n",
       "      <td>0.370453</td>\n",
       "      <td>0.377065</td>\n",
       "      <td>0.367994</td>\n",
       "      <td>0.411746</td>\n",
       "      <td>0.408434</td>\n",
       "      <td>0.400087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955754</th>\n",
       "      <td>0.274674</td>\n",
       "      <td>0.335005</td>\n",
       "      <td>0.351431</td>\n",
       "      <td>0.371115</td>\n",
       "      <td>0.340883</td>\n",
       "      <td>0.375659</td>\n",
       "      <td>0.351630</td>\n",
       "      <td>0.297714</td>\n",
       "      <td>0.105584</td>\n",
       "      <td>0.114975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455626</td>\n",
       "      <td>0.461780</td>\n",
       "      <td>0.465002</td>\n",
       "      <td>0.460135</td>\n",
       "      <td>0.466949</td>\n",
       "      <td>0.461478</td>\n",
       "      <td>0.456959</td>\n",
       "      <td>0.493403</td>\n",
       "      <td>0.489076</td>\n",
       "      <td>0.493711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 342 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LRST_date_00  LRST_date_01  LRST_date_02  LRST_date_03  LRST_date_04  \\\n",
       "id                                                                              \n",
       "1444093      0.228598      0.286011      0.306310      0.316349      0.284873   \n",
       "1254992      0.207357      0.258086      0.254560      0.263676      0.261145   \n",
       "1792358      0.397426      0.444097      0.454559      0.470724      0.459195   \n",
       "712455       0.218732      0.291654      0.294577      0.302511      0.284281   \n",
       "1955754      0.274674      0.335005      0.351431      0.371115      0.340883   \n",
       "\n",
       "         LRST_date_05  LRST_date_06  LRST_date_07  LRST_late_00  LRST_late_01  \\\n",
       "id                                                                              \n",
       "1444093      0.305438      0.294386      0.263173      0.087456      0.091904   \n",
       "1254992      0.262898      0.287953      0.223825      0.143654      0.138028   \n",
       "1792358      0.492430      0.469526      0.400029      0.105415      0.114457   \n",
       "712455       0.319613      0.289171      0.257887      0.071303      0.076709   \n",
       "1955754      0.375659      0.351630      0.297714      0.105584      0.114975   \n",
       "\n",
       "         ...  GBST_service_20  GBST_service_21  GBST_service_22  \\\n",
       "id       ...                                                      \n",
       "1444093  ...         0.513262         0.486053         0.516937   \n",
       "1254992  ...         0.442961         0.433920         0.476977   \n",
       "1792358  ...         0.496953         0.504692         0.506555   \n",
       "712455   ...         0.378542         0.378537         0.388864   \n",
       "1955754  ...         0.455626         0.461780         0.465002   \n",
       "\n",
       "         GBST_service_23  GBST_service_24  GBST_service_25  GBST_service_26  \\\n",
       "id                                                                            \n",
       "1444093         0.492899         0.493076         0.509266         0.497604   \n",
       "1254992         0.394242         0.423567         0.432123         0.409902   \n",
       "1792358         0.503799         0.503056         0.507792         0.503217   \n",
       "712455          0.374779         0.370453         0.377065         0.367994   \n",
       "1955754         0.460135         0.466949         0.461478         0.456959   \n",
       "\n",
       "         GBST_service_27  GBST_service_28  GBST_service_29  \n",
       "id                                                          \n",
       "1444093         0.533499         0.521207         0.524906  \n",
       "1254992         0.443103         0.462180         0.477327  \n",
       "1792358         0.531735         0.540401         0.536485  \n",
       "712455          0.411746         0.408434         0.400087  \n",
       "1955754         0.493403         0.489076         0.493711  \n",
       "\n",
       "[5 rows x 342 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_first_meta_stat = fp.ParquetFile('features/firstmeta/features_first_meta_stat').to_pandas().set_index('id').drop(['flag'],axis=1)\n",
    "features_first_meta_stat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним тренировочный валидационный и тестовый наборы данных\n",
    "# чтобы иметь возможность к ними обратиться\n",
    "fp.write('features/firstmeta/features_first_meta_stat_train',features_first_meta_stat.loc[mf_train_id])\n",
    "fp.write('features/firstmeta/features_first_meta_stat_valid',features_first_meta_stat.loc[mf_valid_id])\n",
    "fp.write('features/firstmeta/features_first_meta_stat_test',features_first_meta_stat.loc[mf_test_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Построение блендинга из базовых моделей обученных на torow данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Построение метамодели Logistic Regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Baseline обучение метамодели LogisticRegression</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv').set_index('id')['flag']\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv').set_index('id')['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.745\n",
      "ROC AUC на тестовом наборе 0.737\n",
      "Основные метрики на тестовом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    144390\n",
      "           1       0.20      0.00      0.00      5235\n",
      "\n",
      "    accuracy                           0.96    149625\n",
      "   macro avg       0.58      0.50      0.49    149625\n",
      "weighted avg       0.94      0.96      0.95    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель логистической регрессии\n",
    "logistic_regression = linear_model.LogisticRegression(random_state=42, max_iter=10000)\n",
    "logistic_regression.fit(MX_train,my_train)\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_LR_pred_proba = logistic_regression.predict_proba(MX_train)[:,1]\n",
    "my_valid_LR_pred_proba = logistic_regression.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_LR_pred = logistic_regression.predict(MX_valid)\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_LR_pred_proba),3))\n",
    "print('ROC AUC на тестовом наборе', round(metrics.roc_auc_score(my_valid, my_valid_LR_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на тестовом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_LR_pred,zero_division=0))\n",
    "\n",
    "# time: 3m 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Подбор гиперпараметров модели</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроим оптимизацию гипер параметров\n",
    "def optuna_lg_meta(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  C = trial.suggest_float('C',0.01,1)\n",
    "  solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky'])\n",
    "  class_0_weight = trial.suggest_float('class_0_weight',0.01,1)\n",
    "  class_1_weight = trial.suggest_float('class_1_weight',0.01,1)\n",
    "  k_features = trial.suggest_int('k_features', 20, 356,step = 1)\n",
    "  class_1_percent = trial.suggest_float('class_1_percent',0.1,1)\n",
    "  random_state = trial.suggest_int('random_state', 1, 1000000)\n",
    "\n",
    "  # создаем модель\n",
    "  optuna_log_reg_meta = linear_model.LogisticRegression(\n",
    "      C=C,\n",
    "      solver = solver,\n",
    "      class_weight={0:class_0_weight,1:class_1_weight},\n",
    "      random_state=random_state,\n",
    "      max_iter=10000)\n",
    "\n",
    "  # Загружаем обучающие наборы\n",
    "  MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "  my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "  # поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "  my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "  # с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "  list_c1_percent_id = class_1_percent_samples(my_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "  # подготовим данные для обучения\n",
    "  MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "  my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "  # освободим память от \"тяжелых\" и ненужных файлов\n",
    "  del MX_train_pd, my_train_pd\n",
    "  gc.collect()\n",
    "  \n",
    "  # с помощью класса SelectKBest получим список лучших признаков\n",
    "  selector = SelectKBest(f_classif, k=k_features)\n",
    "  selector.fit(MX_train_balanced, my_train_balanced)\n",
    "  list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "  MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "  # я не воспользовался параметром timeout метода optimize потому, что  \n",
    "  # optimize отставливает поиск параметров, если время обучения \n",
    "  # превышает timeout. Мне нужно чтобы поиск параметров продолжился.\n",
    "  try:\n",
    "    # для модуля func_time_out упакуем обучение модели в функцию try_func\n",
    "    def try_func():\n",
    "            return optuna_log_reg_meta.fit(MX_train_balanced, my_train_balanced)\n",
    "    # обучаем модель с ограничением по времени 10 минут (600 секунд)\n",
    "    optuna_log_reg_meta = func_timeout.func_timeout(600, try_func)\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "\n",
    "    # загружаем тестовые наборы\n",
    "    MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()[list_best_features].to_numpy()\n",
    "    MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()[list_best_features].to_numpy()\n",
    "    my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "    my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "    \n",
    "    # делаем предсказание на обучающем и валидационном наборе и считаем метрики\n",
    "    roc_train = metrics.roc_auc_score(my_train, optuna_log_reg_meta.predict_proba(MX_train)[:,1])\n",
    "    roc_valid = metrics.roc_auc_score(my_valid, optuna_log_reg_meta.predict_proba(MX_valid)[:,1])\n",
    "    f1_score = metrics.f1_score(my_valid, optuna_log_reg_meta.predict(MX_valid))\n",
    "    recall_1 = metrics.recall_score(my_valid, optuna_log_reg_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "    precision_1 = metrics.precision_score(my_valid, optuna_log_reg_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train, MX_valid, my_train, my_valid\n",
    "    gc.collect()\n",
    "\n",
    "  except func_timeout.FunctionTimedOut:\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "    \n",
    "    # фиксируем пустые значения метрик\n",
    "    roc_train = 0\n",
    "    roc_valid = 0\n",
    "    f1_score = 0\n",
    "    recall_1 = 0\n",
    "    precision_1 = 0\n",
    "    pass\n",
    "\n",
    "  return roc_train, roc_valid, f1_score, recall_1, precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:36:24,723] A new study created in RDB with name: LogisticRegression_meta_first_torow\n"
     ]
    }
   ],
   "source": [
    "# optuna.delete_study(study_name=\"LogisticRegression_meta_first_torow\", storage='sqlite:///optuna_studies.db')\n",
    "# cоздаем объект исследования\n",
    "# чтобы модель не переобучалась минимизируем roc_train \n",
    "# и максимизируем roc_valid\n",
    "optuna_study_lg_meta = optuna.create_study(study_name=\"LogisticRegression_meta_first_torow\", \n",
    "                               directions=['maximize','maximize','maximize','maximize','maximize'], \n",
    "                               sampler=optuna.samplers.TPESampler(),\n",
    "                               pruner='Hyperband',\n",
    "                               storage='sqlite:///optuna_studies.db',\n",
    "                               load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "optuna_study_lg_meta.optimize(optuna_lg_meta, n_trials=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Анализ гиперпараметров</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_class_0_weight</th>\n",
       "      <th>params_class_1_percent</th>\n",
       "      <th>params_class_1_weight</th>\n",
       "      <th>params_k_features</th>\n",
       "      <th>params_random_state</th>\n",
       "      <th>params_solver</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>0.750848</td>\n",
       "      <td>0.740206</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.404776</td>\n",
       "      <td>0.098403</td>\n",
       "      <td>2025-04-20 18:01:26.242190</td>\n",
       "      <td>2025-04-20 18:01:33.366617</td>\n",
       "      <td>0 days 00:00:07.124427</td>\n",
       "      <td>0.802815</td>\n",
       "      <td>0.256365</td>\n",
       "      <td>0.395373</td>\n",
       "      <td>0.212852</td>\n",
       "      <td>341</td>\n",
       "      <td>611801</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>0.742962</td>\n",
       "      <td>0.730683</td>\n",
       "      <td>0.088609</td>\n",
       "      <td>0.923782</td>\n",
       "      <td>0.046536</td>\n",
       "      <td>2025-04-20 18:01:33.371641</td>\n",
       "      <td>2025-04-20 18:01:38.538392</td>\n",
       "      <td>0 days 00:00:05.166751</td>\n",
       "      <td>0.815527</td>\n",
       "      <td>0.230913</td>\n",
       "      <td>0.429195</td>\n",
       "      <td>0.777775</td>\n",
       "      <td>190</td>\n",
       "      <td>238543</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>0.751009</td>\n",
       "      <td>0.739289</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>0.939828</td>\n",
       "      <td>0.044897</td>\n",
       "      <td>2025-04-20 18:01:38.544440</td>\n",
       "      <td>2025-04-20 18:01:46.703088</td>\n",
       "      <td>0 days 00:00:08.158648</td>\n",
       "      <td>0.858042</td>\n",
       "      <td>0.149071</td>\n",
       "      <td>0.366052</td>\n",
       "      <td>0.754079</td>\n",
       "      <td>328</td>\n",
       "      <td>584230</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0.750584</td>\n",
       "      <td>0.738551</td>\n",
       "      <td>0.121917</td>\n",
       "      <td>0.714231</td>\n",
       "      <td>0.066646</td>\n",
       "      <td>2025-04-20 18:01:46.708757</td>\n",
       "      <td>2025-04-20 18:01:53.852193</td>\n",
       "      <td>0 days 00:00:07.143436</td>\n",
       "      <td>0.792399</td>\n",
       "      <td>0.172989</td>\n",
       "      <td>0.397806</td>\n",
       "      <td>0.304821</td>\n",
       "      <td>322</td>\n",
       "      <td>242512</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0.750508</td>\n",
       "      <td>0.738972</td>\n",
       "      <td>0.151633</td>\n",
       "      <td>0.480229</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>2025-04-20 18:01:53.857429</td>\n",
       "      <td>2025-04-20 18:02:00.498482</td>\n",
       "      <td>0 days 00:00:06.641053</td>\n",
       "      <td>0.815738</td>\n",
       "      <td>0.245214</td>\n",
       "      <td>0.415227</td>\n",
       "      <td>0.220710</td>\n",
       "      <td>342</td>\n",
       "      <td>584885</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  roc_train  roc_valid  f1_score  recall_1  precision_1  \\\n",
       "295     295   0.750848   0.740206  0.158317  0.404776     0.098403   \n",
       "296     296   0.742962   0.730683  0.088609  0.923782     0.046536   \n",
       "297     297   0.751009   0.739289  0.085700  0.939828     0.044897   \n",
       "298     298   0.750584   0.738551  0.121917  0.714231     0.066646   \n",
       "299     299   0.750508   0.738972  0.151633  0.480229     0.090030   \n",
       "\n",
       "                datetime_start          datetime_complete  \\\n",
       "295 2025-04-20 18:01:26.242190 2025-04-20 18:01:33.366617   \n",
       "296 2025-04-20 18:01:33.371641 2025-04-20 18:01:38.538392   \n",
       "297 2025-04-20 18:01:38.544440 2025-04-20 18:01:46.703088   \n",
       "298 2025-04-20 18:01:46.708757 2025-04-20 18:01:53.852193   \n",
       "299 2025-04-20 18:01:53.857429 2025-04-20 18:02:00.498482   \n",
       "\n",
       "                  duration  params_C  params_class_0_weight  \\\n",
       "295 0 days 00:00:07.124427  0.802815               0.256365   \n",
       "296 0 days 00:00:05.166751  0.815527               0.230913   \n",
       "297 0 days 00:00:08.158648  0.858042               0.149071   \n",
       "298 0 days 00:00:07.143436  0.792399               0.172989   \n",
       "299 0 days 00:00:06.641053  0.815738               0.245214   \n",
       "\n",
       "     params_class_1_percent  params_class_1_weight  params_k_features  \\\n",
       "295                0.395373               0.212852                341   \n",
       "296                0.429195               0.777775                190   \n",
       "297                0.366052               0.754079                328   \n",
       "298                0.397806               0.304821                322   \n",
       "299                0.415227               0.220710                342   \n",
       "\n",
       "     params_random_state params_solver     state  \n",
       "295               611801     liblinear  COMPLETE  \n",
       "296               238543     liblinear  COMPLETE  \n",
       "297               584230     liblinear  COMPLETE  \n",
       "298               242512     liblinear  COMPLETE  \n",
       "299               584885     liblinear  COMPLETE  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из полученного результат соврмируем Data Frame\n",
    "optuna_study_lg_meta_pd = optuna_study_lg_meta.trials_dataframe().dropna()\n",
    "# переименуем столбы\n",
    "optuna_study_lg_meta_pd.rename(columns={\n",
    "    'values_0': 'roc_train',\n",
    "    'values_1': 'roc_valid',\n",
    "    'values_2': 'f1_score',\n",
    "    'values_3': 'recall_1',\n",
    "    'values_4': 'precision_1'\n",
    "},inplace=True)\n",
    "# Выделим время потраченное на обучение модели\n",
    "optuna_study_lg_meta_pd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение метрики ROC AUC на валидационном наборе: 0.741\n",
      "Среднее значение метрики ROC AUC на валидационном наборе: 0.737\n",
      "Максимальное значение метрики f1_score на валидационном наборе: 0.166\n",
      "Среднее значение метрики f1_score на валидационном наборе: 0.124\n",
      "Максимальное значение метрики recall_1 на валидационном наборе: 1.0\n",
      "Среднее значение метрики recall_1 на валидационном наборе: 0.466\n",
      "Максимальное значение метрики precision_1 на валидационном наборе: 0.368\n",
      "Среднее значение метрики precision_1 на валидационном наборе: 0.101\n"
     ]
    }
   ],
   "source": [
    "# покаждем статистику обучения\n",
    "print('Максимальное значение метрики ROC AUC на валидационном наборе:',round(optuna_study_lg_meta_pd['roc_valid'].max(),3))\n",
    "print('Среднее значение метрики ROC AUC на валидационном наборе:',round(optuna_study_lg_meta_pd['roc_valid'].mean(),3))\n",
    "print('Максимальное значение метрики f1_score на валидационном наборе:',round(optuna_study_lg_meta_pd['f1_score'].max(),3))\n",
    "print('Среднее значение метрики f1_score на валидационном наборе:',round(optuna_study_lg_meta_pd['f1_score'].mean(),3))\n",
    "print('Максимальное значение метрики recall_1 на валидационном наборе:',round(optuna_study_lg_meta_pd['recall_1'].max(),3))\n",
    "print('Среднее значение метрики recall_1 на валидационном наборе:',round(optuna_study_lg_meta_pd['recall_1'].mean(),3))\n",
    "print('Максимальное значение метрики precision_1 на валидационном наборе:',round(optuna_study_lg_meta_pd['precision_1'].max(),3))\n",
    "print('Среднее значение метрики precision_1 на валидационном наборе:',round(optuna_study_lg_meta_pd['precision_1'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим график важности гиперпарметров\n",
    "optuna.visualization.plot_param_importances(optuna_study_lg_meta, target_name='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\1.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрики precision_1 от гипер параметров\n",
    "\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_lg_meta_pd,\n",
    "    x='precision_1', #ось абсцисс\n",
    "    y=['f1_score','roc_train', 'roc_valid', 'recall_1'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision_1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\2.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем метрики полученные при optuna оптимизации\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['roc_valid']>0.9999*optuna_study_lg_meta_pd['roc_valid'].max()],\n",
    "    x='number', #ось абсцисс\n",
    "    y=['roc_train','roc_valid','recall_1','precision_1'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость качества модели от trials optuna', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='trials optuna',\n",
    "    yaxis_title='Величина метрики')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\3.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Вывод:  \n",
    "\n",
    "Их всех выбираем точку $number =213$.   \n",
    "В этой точке самые оптимальные значения $recall$ и $precision$.  \n",
    "Посмотрим каким значениям гиперпараметров соотвествует выбранная точка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best solver: liblinear\n",
      "best C: 0.501\n",
      "best class 0 weight: 0.775\n",
      "best class 1 weight: 0.781\n",
      "best class 1 percent: 0.358\n",
      "best k features: 350\n",
      "best random state: 332173\n",
      "time for best train: 0 minutes\n",
      "\n",
      "ROC AUC на обучающем наборе: 0.752\n",
      "ROC AUC на валидационном наборе: 0.741\n",
      "precision класса 1: 0.098\n",
      "recall класса 1: 0.428\n"
     ]
    }
   ],
   "source": [
    "# определим номер лучшге варианта\n",
    "best_optuna_number = 213\n",
    "\n",
    "# сформируем словарь лучших гипирпарметров RandomForestClassifier\n",
    "best_param_lr = {\n",
    "    'solver' : optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_solver'].iloc[0],\n",
    "    'C' : round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_C'].iloc[0],3),\n",
    "    'class_weight' : {0:round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_class_0_weight'].iloc[0],3),\n",
    "                      1:round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_class_1_weight'].iloc[0],3)},\n",
    "    }\n",
    "\n",
    "# создадим перменные\n",
    "best_k_features = optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_k_features'].iloc[0]\n",
    "best_random_state = optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_random_state'].iloc[0]\n",
    "best_class_1_percent = optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['params_class_1_percent'].iloc[0]\n",
    "\n",
    "# Выведем принятые наилучшие праметры\n",
    "print('best solver:',best_param_lr['solver'])\n",
    "print('best C:',best_param_lr['C'])\n",
    "print('best class 0 weight:',best_param_lr['class_weight'][0])\n",
    "print('best class 1 weight:',best_param_lr['class_weight'][1])\n",
    "\n",
    "print('best class 1 percent:',round(best_class_1_percent,3))\n",
    "print('best k features:',best_k_features)\n",
    "print('best random state:',best_random_state)\n",
    "print('time for best train:',round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['duration'].iloc[0].seconds/60),'minutes')\n",
    "\n",
    "print()\n",
    "print('ROC AUC на обучающем наборе:', round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['roc_train'].iloc[0],3))\n",
    "print('ROC AUC на валидационном наборе:', round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['roc_valid'].iloc[0],3))\n",
    "print('precision класса 1:', round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['precision_1'].iloc[0],3))\n",
    "print('recall класса 1:', round(optuna_study_lg_meta_pd[optuna_study_lg_meta_pd['number']==best_optuna_number]['recall_1'].iloc[0],3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Обучение модели с лучшими параметрами</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.752\n",
      "ROC AUC на валидационном наборе 0.741\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91    144390\n",
      "           1       0.10      0.43      0.16      5235\n",
      "\n",
      "    accuracy                           0.84    149625\n",
      "   macro avg       0.54      0.64      0.54    149625\n",
      "weighted avg       0.95      0.84      0.89    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем обучающие наборы\n",
    "MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "# поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "# с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "list_c1_percent_id = class_1_percent_samples(my_train_pd,best_class_1_percent,random_state=best_random_state)[:1000000]\n",
    "\n",
    "# подготовим данные для обучения\n",
    "MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "# освободим память от \"тяжелых\" и ненужных файлов\n",
    "del MX_train_pd, my_train_pd\n",
    "gc.collect()\n",
    "\n",
    "# с помощью класса SelectKBest получим список лучших признаков\n",
    "selector = SelectKBest(f_classif, k=best_k_features)\n",
    "selector.fit(MX_train_balanced, my_train_balanced)\n",
    "list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "\n",
    "# обучаем модель LogisticRegression с наилучшеми параметрами\n",
    "logistic_regression = linear_model.LogisticRegression(\n",
    "        **best_param_lr,\n",
    "        random_state=best_random_state,\n",
    "        max_iter=10000)\n",
    "logistic_regression.fit(MX_train_balanced, my_train_balanced)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train_balanced, my_train_balanced\n",
    "gc.collect()\n",
    "\n",
    "# загружаем тестовые наборы\n",
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()[list_best_features].to_numpy()\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()[list_best_features].to_numpy()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_lr_pred_proba = logistic_regression.predict_proba(MX_train)[:,1]\n",
    "my_valid_lr_pred_proba = logistic_regression.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_lr_pred = logistic_regression.predict(MX_valid)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train, MX_valid\n",
    "gc.collect()\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_lr_pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_lr_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_lr_pred,zero_division=0))\n",
    "\n",
    "# time: 2m 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Построение модели Gradient Boosting Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Baseline обучение модели Hist Gradient Boosting Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv').set_index('id')['flag']\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv').set_index('id')['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.792\n",
      "ROC AUC на валидационном наборе 0.743\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    144390\n",
      "           1       0.00      0.00      0.00      5235\n",
      "\n",
      "    accuracy                           0.96    149625\n",
      "   macro avg       0.48      0.50      0.49    149625\n",
      "weighted avg       0.93      0.96      0.95    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель Gradient Boosting Classifier\n",
    "gradient_boosting = HistGradientBoostingClassifier(random_state=42)\n",
    "gradient_boosting.fit(MX_train,my_train)\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_rf__pred_proba = gradient_boosting.predict_proba(MX_train)[:,1]\n",
    "my_valid_rf_pred_proba = gradient_boosting.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_rf_pred = gradient_boosting.predict(MX_valid)\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_rf__pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_rf_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_rf_pred))\n",
    "\n",
    "# time: 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Выводы:\n",
    "\n",
    "В сравнение с моделью $HistGradientBoostingClassifier$ построенной   \n",
    "на несбалансированных данных $transform$ $data$ $stat$:\n",
    "1. Качество модели по метрике $ROC AUC$ не изменилась.\n",
    "2. Качество модели по метрике $recall_1$ значительно  \n",
    "улучшилось. Правда, $recall_0$ уменьшилось.\n",
    "3. Качество модели по метрике $precision_1$ уменьшилось.   \n",
    "4. На порядок уменьшилось время обучения модели. Это логично,  \n",
    "велична сбалансированной выборки всего $107000$ $samples$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Подбор гиперпараметров модели</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроим оптимизацию гипер параметров\n",
    "def optuna_gb_meta(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  learning_rate = trial.suggest_float('learning_rate',0.01,0.1)\n",
    "  max_iter = trial.suggest_int('max_iter', 150, 250,step = 1)\n",
    "  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 60,step = 1)\n",
    "  max_depth = trial.suggest_int('max_depth', 1, 10,step = 1)\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 60,step = 1)\n",
    "  max_features = trial.suggest_float('max_features',0.1,1)\n",
    "  l2_regularization = trial.suggest_float('l2_regularization',0.01,1)\n",
    "  class_0_weight = trial.suggest_float('class_0_weight',0.01,1)\n",
    "  class_1_weight = trial.suggest_float('class_1_weight',0.01,1)\n",
    "  k_features = trial.suggest_int('k_features', 20, 356,step = 1)\n",
    "  class_1_percent = trial.suggest_float('class_1_percent',0.01,1)\n",
    "  random_state = trial.suggest_int('random_state', 1, 1000000)\n",
    "\n",
    "  # создаем модель\n",
    "  optuna_gradient_boosting_meta = HistGradientBoostingClassifier(\n",
    "      learning_rate= learning_rate,\n",
    "      max_iter = max_iter,\n",
    "      max_leaf_nodes =max_leaf_nodes,\n",
    "      max_depth = max_depth,\n",
    "      min_samples_leaf = min_samples_leaf,\n",
    "      max_features=max_features,\n",
    "      l2_regularization = l2_regularization,\n",
    "      class_weight={0:class_0_weight,1:class_1_weight},\n",
    "      random_state=random_state)\n",
    "\n",
    "  # Загружаем обучающие наборы\n",
    "  MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "  my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "  # поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "  my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "  # с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "  list_c1_percent_id = class_1_percent_samples(my_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "  # подготовим данные для обучения\n",
    "  MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "  my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "  # освободим память от \"тяжелых\" и ненужных файлов\n",
    "  del MX_train_pd, my_train_pd\n",
    "  gc.collect()\n",
    "  \n",
    "  # с помощью класса SelectKBest получим список лучших признаков\n",
    "  selector = SelectKBest(f_classif, k=k_features)\n",
    "  selector.fit(MX_train_balanced, my_train_balanced)\n",
    "  list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "  MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "  # я не воспользовался параметром timeout метода optimize потому, что  \n",
    "  # optimize отставливает поиск параметров, если время обучения \n",
    "  # превышает timeout. Мне нужно чтобы поиск параметров продолжился.\n",
    "  try:\n",
    "    # для модуля func_time_out упакуем обучение модели в функцию try_func\n",
    "    def try_func():\n",
    "            return optuna_gradient_boosting_meta.fit(MX_train_balanced,my_train_balanced)\n",
    "    # обучаем модель с ограничением по времени 10 минут (600 секунд)\n",
    "    optuna_gradient_boosting_meta = func_timeout.func_timeout(600, try_func)\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "\n",
    "    # загружаем тестовые наборы\n",
    "    MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()[list_best_features].to_numpy()\n",
    "    MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()[list_best_features].to_numpy()\n",
    "    my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "    my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "    # делаем предсказание на обучающем и валидационном наборе\n",
    "    #Считаем метрики для класса 1 добавляем их в список\n",
    "    roc_train = metrics.roc_auc_score(my_train, optuna_gradient_boosting_meta.predict_proba(MX_train)[:,1])\n",
    "    roc_valid = metrics.roc_auc_score(my_valid, optuna_gradient_boosting_meta.predict_proba(MX_valid)[:,1])\n",
    "    f1_score = metrics.f1_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid))\n",
    "    recall_1 = metrics.recall_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "    precision_1 = metrics.precision_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train, MX_valid, my_train, my_valid\n",
    "    gc.collect()\n",
    "\n",
    "  except func_timeout.FunctionTimedOut:\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "    \n",
    "    # фиксируем пустые значения метрик\n",
    "    roc_train = 0\n",
    "    roc_valid = 0\n",
    "    f1_score = 0\n",
    "    recall_1 = 0\n",
    "    precision_1 = 0\n",
    "    pass\n",
    "\n",
    "  return roc_train, roc_valid, f1_score, recall_1, precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 18:02:29,826] A new study created in RDB with name: HistGradientBoostingClassifier_meta_first_torow\n"
     ]
    }
   ],
   "source": [
    "# ну случай удаления обучения\n",
    "# optuna.delete_study(study_name=\"HistGradientBoostingClassifier_meta_first_torow\", storage='sqlite:///optuna_studies.db')\n",
    "\n",
    "# cоздаем объект исследования\n",
    "# чтобы модель не переобучалась минимизируем roc_train \n",
    "# и максимизируем roc_valid\n",
    "optuna_study_gb_meta = optuna.create_study(study_name=\"HistGradientBoostingClassifier_meta_first_torow\", \n",
    "                               directions=['maximize','maximize','maximize','maximize','maximize'], \n",
    "                               sampler=optuna.samplers.TPESampler(),\n",
    "                               pruner='Hyperband',\n",
    "                               storage='sqlite:///optuna_studies.db',\n",
    "                               load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "optuna_study_gb_meta.optimize(optuna_gb_meta, n_trials=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Анализ гиперпараметров</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_class_0_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>params_k_features</th>\n",
       "      <th>params_l2_regularization</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_max_depth</th>\n",
       "      <th>params_max_features</th>\n",
       "      <th>params_max_iter</th>\n",
       "      <th>params_max_leaf_nodes</th>\n",
       "      <th>params_min_samples_leaf</th>\n",
       "      <th>params_random_state</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>0.795564</td>\n",
       "      <td>0.744243</td>\n",
       "      <td>0.149774</td>\n",
       "      <td>0.164852</td>\n",
       "      <td>0.137224</td>\n",
       "      <td>2025-04-20 18:51:03.333111</td>\n",
       "      <td>2025-04-20 18:51:16.581460</td>\n",
       "      <td>0 days 00:00:13.248349</td>\n",
       "      <td>0.651024</td>\n",
       "      <td>...</td>\n",
       "      <td>339</td>\n",
       "      <td>0.277669</td>\n",
       "      <td>0.081731</td>\n",
       "      <td>8</td>\n",
       "      <td>0.328420</td>\n",
       "      <td>194</td>\n",
       "      <td>59</td>\n",
       "      <td>38</td>\n",
       "      <td>76466</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>0.761505</td>\n",
       "      <td>0.742570</td>\n",
       "      <td>0.163037</td>\n",
       "      <td>0.225215</td>\n",
       "      <td>0.127763</td>\n",
       "      <td>2025-04-20 18:51:16.587624</td>\n",
       "      <td>2025-04-20 18:51:30.921314</td>\n",
       "      <td>0 days 00:00:14.333690</td>\n",
       "      <td>0.632323</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0.255832</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>4</td>\n",
       "      <td>0.317473</td>\n",
       "      <td>203</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>76061</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>0.798146</td>\n",
       "      <td>0.746495</td>\n",
       "      <td>0.168336</td>\n",
       "      <td>0.282521</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>2025-04-20 18:51:30.927423</td>\n",
       "      <td>2025-04-20 18:51:50.543477</td>\n",
       "      <td>0 days 00:00:19.616054</td>\n",
       "      <td>0.669912</td>\n",
       "      <td>...</td>\n",
       "      <td>341</td>\n",
       "      <td>0.346893</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>8</td>\n",
       "      <td>0.337752</td>\n",
       "      <td>204</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>59535</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0.801353</td>\n",
       "      <td>0.746268</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.186246</td>\n",
       "      <td>0.139047</td>\n",
       "      <td>2025-04-20 18:51:50.549528</td>\n",
       "      <td>2025-04-20 18:52:13.807980</td>\n",
       "      <td>0 days 00:00:23.258452</td>\n",
       "      <td>0.662894</td>\n",
       "      <td>...</td>\n",
       "      <td>329</td>\n",
       "      <td>0.340077</td>\n",
       "      <td>0.025337</td>\n",
       "      <td>8</td>\n",
       "      <td>0.619916</td>\n",
       "      <td>209</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>47402</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0.783417</td>\n",
       "      <td>0.745474</td>\n",
       "      <td>0.166339</td>\n",
       "      <td>0.258453</td>\n",
       "      <td>0.122632</td>\n",
       "      <td>2025-04-20 18:52:13.814542</td>\n",
       "      <td>2025-04-20 18:52:37.945596</td>\n",
       "      <td>0 days 00:00:24.131054</td>\n",
       "      <td>0.627800</td>\n",
       "      <td>...</td>\n",
       "      <td>339</td>\n",
       "      <td>0.343936</td>\n",
       "      <td>0.016333</td>\n",
       "      <td>8</td>\n",
       "      <td>0.165393</td>\n",
       "      <td>203</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>68738</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  roc_train  roc_valid  f1_score  recall_1  precision_1  \\\n",
       "295     295   0.795564   0.744243  0.149774  0.164852     0.137224   \n",
       "296     296   0.761505   0.742570  0.163037  0.225215     0.127763   \n",
       "297     297   0.798146   0.746495  0.168336  0.282521     0.119883   \n",
       "298     298   0.801353   0.746268  0.159223  0.186246     0.139047   \n",
       "299     299   0.783417   0.745474  0.166339  0.258453     0.122632   \n",
       "\n",
       "                datetime_start          datetime_complete  \\\n",
       "295 2025-04-20 18:51:03.333111 2025-04-20 18:51:16.581460   \n",
       "296 2025-04-20 18:51:16.587624 2025-04-20 18:51:30.921314   \n",
       "297 2025-04-20 18:51:30.927423 2025-04-20 18:51:50.543477   \n",
       "298 2025-04-20 18:51:50.549528 2025-04-20 18:52:13.807980   \n",
       "299 2025-04-20 18:52:13.814542 2025-04-20 18:52:37.945596   \n",
       "\n",
       "                  duration  params_class_0_weight  ...  params_k_features  \\\n",
       "295 0 days 00:00:13.248349               0.651024  ...                339   \n",
       "296 0 days 00:00:14.333690               0.632323  ...                349   \n",
       "297 0 days 00:00:19.616054               0.669912  ...                341   \n",
       "298 0 days 00:00:23.258452               0.662894  ...                329   \n",
       "299 0 days 00:00:24.131054               0.627800  ...                339   \n",
       "\n",
       "     params_l2_regularization  params_learning_rate  params_max_depth  \\\n",
       "295                  0.277669              0.081731                 8   \n",
       "296                  0.255832              0.019345                 4   \n",
       "297                  0.346893              0.032010                 8   \n",
       "298                  0.340077              0.025337                 8   \n",
       "299                  0.343936              0.016333                 8   \n",
       "\n",
       "     params_max_features  params_max_iter  params_max_leaf_nodes  \\\n",
       "295             0.328420              194                     59   \n",
       "296             0.317473              203                     59   \n",
       "297             0.337752              204                     60   \n",
       "298             0.619916              209                     60   \n",
       "299             0.165393              203                     59   \n",
       "\n",
       "     params_min_samples_leaf  params_random_state     state  \n",
       "295                       38                76466  COMPLETE  \n",
       "296                       34                76061  COMPLETE  \n",
       "297                       35                59535  COMPLETE  \n",
       "298                       36                47402  COMPLETE  \n",
       "299                       32                68738  COMPLETE  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из полученного результат соврмируем Data Frame\n",
    "optuna_study_gb_meta_pd = optuna_study_gb_meta.trials_dataframe()\n",
    "# переименуем столбы\n",
    "optuna_study_gb_meta_pd.rename(columns={\n",
    "    'values_0': 'roc_train',\n",
    "    'values_1': 'roc_valid',\n",
    "    'values_2': 'f1_score',\n",
    "    'values_3': 'recall_1',\n",
    "    'values_4': 'precision_1'\n",
    "},inplace=True)\n",
    "optuna_study_gb_meta_pd.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение метрики ROC AUC на валидационном наборе: 0.747\n",
      "Среднее значение метрики ROC AUC на валидационном наборе: 0.741\n",
      "Максимальное значение метрики f1_score на валидационном наборе: 0.169\n",
      "Среднее значение метрики f1_score на валидационном наборе: 0.124\n",
      "Максимальное значение метрики recall_1 на валидационном наборе: 1.0\n",
      "Среднее значение метрики recall_1 на валидационном наборе: 0.436\n",
      "Максимальное значение метрики precision_1 на валидационном наборе: 0.75\n",
      "Среднее значение метрики precision_1 на валидационном наборе: 0.111\n"
     ]
    }
   ],
   "source": [
    "# покаждем статистику обучения\n",
    "print('Максимальное значение метрики ROC AUC на валидационном наборе:',round(optuna_study_gb_meta_pd['roc_valid'].max(),3))\n",
    "print('Среднее значение метрики ROC AUC на валидационном наборе:',round(optuna_study_gb_meta_pd['roc_valid'].mean(),3))\n",
    "print('Максимальное значение метрики f1_score на валидационном наборе:',round(optuna_study_gb_meta_pd['f1_score'].max(),3))\n",
    "print('Среднее значение метрики f1_score на валидационном наборе:',round(optuna_study_gb_meta_pd['f1_score'].mean(),3))\n",
    "print('Максимальное значение метрики recall_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['recall_1'].max(),3))\n",
    "print('Среднее значение метрики recall_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['recall_1'].mean(),3))\n",
    "print('Максимальное значение метрики precision_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['precision_1'].max(),3))\n",
    "print('Среднее значение метрики precision_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['precision_1'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим график важности гиперпарметров\n",
    "optuna.visualization.plot_param_importances(optuna_study_gb_meta, target_name='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\4.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрики precision_1 от гиперпараметров\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_gb_meta_pd,\n",
    "    x='precision_1', #ось абсцисс\n",
    "    y=['roc_train', 'roc_valid', 'recall_1','f1_score'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision класса 1 от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision класса 1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\5.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрик качества модели от number\n",
    "\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['roc_valid']>0.9999*(optuna_study_gb_meta_pd['roc_valid'].max())],\n",
    "    x='number', #ось абсцисс\n",
    "    y=['roc_train','roc_valid','recall_1','precision_1'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision_1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\6.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Вывод:  \n",
    "\n",
    "Их всех выбираем точку $number =272$.   \n",
    "В этой точке оптимальные значения метрик $recall_1$ и $precision_1$, а  \n",
    "также относительно высокая метрика $ROC AUC$.   \n",
    "Посмотрим каким значениям гиперпараметров соотвествует выбранная точка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning rate: 0.025\n",
      "best max iter: 232\n",
      "best max leaf nodes: 58\n",
      "best max depth: 9\n",
      "best min samples leaf: 7\n",
      "best max features: 0.346\n",
      "best l2 regularization: 0.688\n",
      "best class 0 weight: 0.233\n",
      "best class 1 weight: 0.616\n",
      "best class 1 percent: 0.15\n",
      "best k features: 334\n",
      "best random state: 975267\n",
      "time for best train: 0 minutes\n",
      "\n",
      "ROC AUC на обучающем наборе: 0.788\n",
      "ROC AUC на валидационном наборе: 0.736\n",
      "precision класса 1: 0.103\n",
      "recall класса 1: 0.333\n"
     ]
    }
   ],
   "source": [
    "# определим номер лучшге варианта\n",
    "best_optuna_number = 272\n",
    "\n",
    "# сформируем словарь лучших гипирпарметров HistGradientBoostingClassifier\n",
    "best_param_hgbc = {\n",
    "    'learning_rate' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_learning_rate'].iloc[0],\n",
    "    'max_iter' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_iter'].iloc[0],\n",
    "    'max_leaf_nodes' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_leaf_nodes'].iloc[0],\n",
    "    'max_depth' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_depth'].iloc[0],\n",
    "    'min_samples_leaf' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_min_samples_leaf'].iloc[0],\n",
    "    'max_features' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_features'].iloc[0],\n",
    "    'l2_regularization' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_l2_regularization'].iloc[0],\n",
    "    'class_weight' : {0: optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_0_weight'].iloc[0],\n",
    "                      1: optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_1_weight'].iloc[0],}\n",
    "    }\n",
    "\n",
    "# определим перменные для лучших значений параметров\n",
    "best_k_features = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_k_features'].iloc[0]\n",
    "best_random_state = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_random_state'].iloc[0]\n",
    "best_class_1_percent = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_1_percent'].iloc[0]\n",
    "\n",
    "\n",
    "# Выведем принятые наилучшие параметры\n",
    "print('best learning rate:',round(best_param_hgbc['learning_rate'],3))\n",
    "print('best max iter:',best_param_hgbc['max_iter'])\n",
    "print('best max leaf nodes:',best_param_hgbc['max_leaf_nodes'])\n",
    "print('best max depth:',best_param_hgbc['max_depth'])\n",
    "print('best min samples leaf:',best_param_hgbc['min_samples_leaf'])\n",
    "print('best max features:',round(best_param_hgbc['max_features'],3))\n",
    "print('best l2 regularization:',round(best_param_hgbc['l2_regularization'],3))\n",
    "print('best class 0 weight:',round(best_param_hgbc['class_weight'][0],3))\n",
    "print('best class 1 weight:',round(best_param_hgbc['class_weight'][1],3))\n",
    "\n",
    "print('best class 1 percent:',round(best_class_1_percent,3))\n",
    "print('best k features:',best_k_features)\n",
    "print('best random state:',best_random_state)\n",
    "print('time for best train:',round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['duration'].iloc[0].seconds/60),'minutes')\n",
    "\n",
    "print()\n",
    "print('ROC AUC на обучающем наборе:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['roc_train'].iloc[0],3))\n",
    "print('ROC AUC на валидационном наборе:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['roc_valid'].iloc[0],3))\n",
    "print('precision класса 1:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['precision_1'].iloc[0],3))\n",
    "print('recall класса 1:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['recall_1'].iloc[0],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Обучение модели с лучшими параметрами</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.801\n",
      "ROC AUC на валидационном наборе 0.747\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93    144390\n",
      "           1       0.11      0.37      0.16      5235\n",
      "\n",
      "    accuracy                           0.87    149625\n",
      "   macro avg       0.54      0.63      0.55    149625\n",
      "weighted avg       0.94      0.87      0.90    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем обучающие наборы\n",
    "MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()\n",
    "my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "# поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "# с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "list_c1_percent_id = class_1_percent_samples(my_train_pd,best_class_1_percent,random_state=best_random_state)[:1000000]\n",
    "\n",
    "# подготовим данные для обучения\n",
    "MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "# освободим память от \"тяжелых\" и ненужных файлов\n",
    "del MX_train_pd, my_train_pd\n",
    "gc.collect()\n",
    "\n",
    "# с помощью класса SelectKBest получим список лучших признаков\n",
    "selector = SelectKBest(f_classif, k=best_k_features)\n",
    "selector.fit(MX_train_balanced, my_train_balanced)\n",
    "list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "# обучаем модель HistGradientBoostingClassifier с наилучшеми параметрами\n",
    "gradient_boosting = HistGradientBoostingClassifier(\n",
    "        **best_param_hgbc,\n",
    "        random_state=best_random_state)\n",
    "gradient_boosting.fit(MX_train_balanced,my_train_balanced)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train_balanced, my_train_balanced\n",
    "gc.collect()\n",
    "\n",
    "# загружаем тестовые наборы\n",
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_torow_train').to_pandas()[list_best_features].to_numpy()\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_torow_valid').to_pandas()[list_best_features].to_numpy()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_gb_pred_proba = gradient_boosting.predict_proba(MX_train)[:,1]\n",
    "my_valid_gb_pred_proba = gradient_boosting.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_gb_pred = gradient_boosting.predict(MX_valid)\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_gb_pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_gb_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_gb_pred,zero_division=0))\n",
    "\n",
    "# time: 1m 40s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">Построение блендинга из базовых моделей обученных на transform data stat данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Построение метамодели Logistic Regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Baseline обучение метамодели LogisticRegression</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv').set_index('id')['flag']\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv').set_index('id')['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.736\n",
      "ROC AUC на тестовом наборе 0.727\n",
      "Основные метрики на тестовом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    144390\n",
      "           1       0.13      0.00      0.00      5235\n",
      "\n",
      "    accuracy                           0.96    149625\n",
      "   macro avg       0.55      0.50      0.49    149625\n",
      "weighted avg       0.94      0.96      0.95    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель логистической регрессии\n",
    "logistic_regression = linear_model.LogisticRegression(random_state=42, max_iter=10000)\n",
    "logistic_regression.fit(MX_train,my_train)\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_LR_pred_proba = logistic_regression.predict_proba(MX_train)[:,1]\n",
    "my_valid_LR_pred_proba = logistic_regression.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_LR_pred = logistic_regression.predict(MX_valid)\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_LR_pred_proba),3))\n",
    "print('ROC AUC на тестовом наборе', round(metrics.roc_auc_score(my_valid, my_valid_LR_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на тестовом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_LR_pred,zero_division=0))\n",
    "\n",
    "# time: 3m 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Подбор гиперпараметров модели</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроим оптимизацию гипер параметров\n",
    "def optuna_lg_meta(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  C = trial.suggest_float('C',0.01,1)\n",
    "  class_0_weight = trial.suggest_float('class_0_weight',0.01,1)\n",
    "  class_1_weight = trial.suggest_float('class_1_weight',0.01,1)\n",
    "  k_features = trial.suggest_int('k_features', 20, 342,step = 1)\n",
    "  class_1_percent = trial.suggest_float('class_1_percent',0.1,1)\n",
    "  random_state = trial.suggest_int('random_state', 1, 1000000)\n",
    "\n",
    "\n",
    "  # создаем модель\n",
    "  optuna_log_reg_meta = linear_model.LogisticRegression(\n",
    "      C=C,\n",
    "      class_weight={0:class_0_weight,1:class_1_weight},\n",
    "      random_state=random_state,\n",
    "      max_iter=10000)\n",
    "\n",
    "  # Загружаем обучающие наборы\n",
    "  MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "  my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "  # поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "  my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "  # с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "  list_c1_percent_id = class_1_percent_samples(my_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "  # подготовим данные для обучения\n",
    "  MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "  my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "  # освободим память от \"тяжелых\" и ненужных файлов\n",
    "  del MX_train_pd, my_train_pd\n",
    "  gc.collect()\n",
    "  \n",
    "  # с помощью класса SelectKBest получим список лучших признаков\n",
    "  selector = SelectKBest(f_classif, k=k_features)\n",
    "  selector.fit(MX_train_balanced, my_train_balanced)\n",
    "  list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "  MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "  # я не воспользовался параметром timeout метода optimize потому, что  \n",
    "  # optimize отставливает поиск параметров, если время обучения \n",
    "  # превышает timeout. Мне нужно чтобы поиск параметров продолжился.\n",
    "  try:\n",
    "    # для модуля func_time_out упакуем обучение модели в функцию try_func\n",
    "    def try_func():\n",
    "            return optuna_log_reg_meta.fit(MX_train_balanced, my_train_balanced)\n",
    "    # обучаем модель с ограничением по времени 10 минут (600 секунд)\n",
    "    optuna_log_reg_meta = func_timeout.func_timeout(600, try_func)\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "\n",
    "    # загружаем тестовые наборы\n",
    "    MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()[list_best_features].to_numpy()\n",
    "    MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()[list_best_features].to_numpy()\n",
    "    my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "    my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "    \n",
    "    # делаем предсказание на обучающем и валидационном наборе и считаем метрики\n",
    "    roc_train = metrics.roc_auc_score(my_train, optuna_log_reg_meta.predict_proba(MX_train)[:,1])\n",
    "    roc_valid = metrics.roc_auc_score(my_valid, optuna_log_reg_meta.predict_proba(MX_valid)[:,1])\n",
    "    f1_score = metrics.f1_score(my_valid, optuna_log_reg_meta.predict(MX_valid))\n",
    "    recall_1 = metrics.recall_score(my_valid, optuna_log_reg_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "    precision_1 = metrics.precision_score(my_valid, optuna_log_reg_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train, MX_valid, my_train, my_valid\n",
    "    gc.collect()\n",
    "\n",
    "  except func_timeout.FunctionTimedOut:\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "    \n",
    "    # фиксируем пустые значения метрик\n",
    "    roc_train = 0\n",
    "    roc_valid = 0\n",
    "    f1_score = 0\n",
    "    recall_1 = 0\n",
    "    precision_1 = 0\n",
    "    pass\n",
    "\n",
    "  return roc_train, roc_valid, f1_score, recall_1, precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 18:53:11,260] A new study created in RDB with name: LogisticRegression_meta_first_stat\n"
     ]
    }
   ],
   "source": [
    "# на случай удаления\n",
    "# optuna.delete_study(study_name=\"LogisticRegression_meta_first_stat\", storage='sqlite:///optuna_studies.db')\n",
    "\n",
    "# cоздаем объект исследования\n",
    "# чтобы модель не переобучалась минимизируем roc_train \n",
    "# и максимизируем roc_valid\n",
    "optuna_study_lg_meta = optuna.create_study(study_name=\"LogisticRegression_meta_first_stat\", \n",
    "                               directions=['maximize','maximize','maximize','maximize','maximize'], \n",
    "                               sampler=optuna.samplers.TPESampler(),\n",
    "                               pruner='Hyperband',\n",
    "                               storage='sqlite:///optuna_studies.db',\n",
    "                               load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "optuna_study_lg_meta.optimize(optuna_lg_meta, n_trials=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Анализ гиперпараметров</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_class_0_weight</th>\n",
       "      <th>params_class_1_percent</th>\n",
       "      <th>params_class_1_weight</th>\n",
       "      <th>params_k_features</th>\n",
       "      <th>params_random_state</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>0.738451</td>\n",
       "      <td>0.727707</td>\n",
       "      <td>0.144222</td>\n",
       "      <td>0.158548</td>\n",
       "      <td>0.132271</td>\n",
       "      <td>2025-04-20 19:21:52.633749</td>\n",
       "      <td>2025-04-20 19:22:01.731859</td>\n",
       "      <td>0 days 00:00:09.098110</td>\n",
       "      <td>0.978021</td>\n",
       "      <td>0.366845</td>\n",
       "      <td>0.132793</td>\n",
       "      <td>0.694424</td>\n",
       "      <td>308</td>\n",
       "      <td>471344</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>0.728412</td>\n",
       "      <td>0.719076</td>\n",
       "      <td>0.126981</td>\n",
       "      <td>0.126266</td>\n",
       "      <td>0.127705</td>\n",
       "      <td>2025-04-20 19:22:01.737773</td>\n",
       "      <td>2025-04-20 19:22:07.879444</td>\n",
       "      <td>0 days 00:00:06.141671</td>\n",
       "      <td>0.967208</td>\n",
       "      <td>0.364108</td>\n",
       "      <td>0.138004</td>\n",
       "      <td>0.637043</td>\n",
       "      <td>182</td>\n",
       "      <td>470174</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>0.738185</td>\n",
       "      <td>0.727341</td>\n",
       "      <td>0.121309</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.137856</td>\n",
       "      <td>2025-04-20 19:22:07.884179</td>\n",
       "      <td>2025-04-20 19:22:15.867755</td>\n",
       "      <td>0 days 00:00:07.983576</td>\n",
       "      <td>0.982616</td>\n",
       "      <td>0.401983</td>\n",
       "      <td>0.126507</td>\n",
       "      <td>0.690340</td>\n",
       "      <td>296</td>\n",
       "      <td>489837</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0.734438</td>\n",
       "      <td>0.726052</td>\n",
       "      <td>0.135716</td>\n",
       "      <td>0.140019</td>\n",
       "      <td>0.131669</td>\n",
       "      <td>2025-04-20 19:22:15.873917</td>\n",
       "      <td>2025-04-20 19:22:23.792801</td>\n",
       "      <td>0 days 00:00:07.918884</td>\n",
       "      <td>0.044228</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>0.153891</td>\n",
       "      <td>0.601334</td>\n",
       "      <td>303</td>\n",
       "      <td>437409</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0.738595</td>\n",
       "      <td>0.728156</td>\n",
       "      <td>0.126694</td>\n",
       "      <td>0.637440</td>\n",
       "      <td>0.070337</td>\n",
       "      <td>2025-04-20 19:22:23.798719</td>\n",
       "      <td>2025-04-20 19:22:31.879695</td>\n",
       "      <td>0 days 00:00:08.080976</td>\n",
       "      <td>0.972004</td>\n",
       "      <td>0.136477</td>\n",
       "      <td>0.167092</td>\n",
       "      <td>0.651310</td>\n",
       "      <td>308</td>\n",
       "      <td>504329</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  roc_train  roc_valid  f1_score  recall_1  precision_1  \\\n",
       "295     295   0.738451   0.727707  0.144222  0.158548     0.132271   \n",
       "296     296   0.728412   0.719076  0.126981  0.126266     0.127705   \n",
       "297     297   0.738185   0.727341  0.121309  0.108309     0.137856   \n",
       "298     298   0.734438   0.726052  0.135716  0.140019     0.131669   \n",
       "299     299   0.738595   0.728156  0.126694  0.637440     0.070337   \n",
       "\n",
       "                datetime_start          datetime_complete  \\\n",
       "295 2025-04-20 19:21:52.633749 2025-04-20 19:22:01.731859   \n",
       "296 2025-04-20 19:22:01.737773 2025-04-20 19:22:07.879444   \n",
       "297 2025-04-20 19:22:07.884179 2025-04-20 19:22:15.867755   \n",
       "298 2025-04-20 19:22:15.873917 2025-04-20 19:22:23.792801   \n",
       "299 2025-04-20 19:22:23.798719 2025-04-20 19:22:31.879695   \n",
       "\n",
       "                  duration  params_C  params_class_0_weight  \\\n",
       "295 0 days 00:00:09.098110  0.978021               0.366845   \n",
       "296 0 days 00:00:06.141671  0.967208               0.364108   \n",
       "297 0 days 00:00:07.983576  0.982616               0.401983   \n",
       "298 0 days 00:00:07.918884  0.044228               0.379585   \n",
       "299 0 days 00:00:08.080976  0.972004               0.136477   \n",
       "\n",
       "     params_class_1_percent  params_class_1_weight  params_k_features  \\\n",
       "295                0.132793               0.694424                308   \n",
       "296                0.138004               0.637043                182   \n",
       "297                0.126507               0.690340                296   \n",
       "298                0.153891               0.601334                303   \n",
       "299                0.167092               0.651310                308   \n",
       "\n",
       "     params_random_state     state  \n",
       "295               471344  COMPLETE  \n",
       "296               470174  COMPLETE  \n",
       "297               489837  COMPLETE  \n",
       "298               437409  COMPLETE  \n",
       "299               504329  COMPLETE  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из полученного результат соврмируем Data Frame\n",
    "optuna_study_lg_pd = optuna_study_lg_meta.trials_dataframe().dropna()\n",
    "# переименуем столбы\n",
    "optuna_study_lg_pd.rename(columns={\n",
    "    'values_0': 'roc_train',\n",
    "    'values_1': 'roc_valid',\n",
    "    'values_2': 'f1_score',\n",
    "    'values_3': 'recall_1',\n",
    "    'values_4': 'precision_1'\n",
    "},inplace=True)\n",
    "# Выделим время потраченное на обучение модели\n",
    "optuna_study_lg_pd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение метрики ROC AUC на валидационном наборе: 0.73\n",
      "Среднее значение метрики ROC AUC на валидационном наборе: 0.725\n",
      "Максимальное значение метрики recall_1 на валидационном наборе: 1.0\n",
      "Среднее значение метрики recall_1 на валидационном наборе: 0.462\n",
      "Максимальное значение метрики precision_1 на валидационном наборе: 0.214\n",
      "Среднее значение метрики precision_1 на валидационном наборе: 0.091\n"
     ]
    }
   ],
   "source": [
    "# покаждем статистику обучения\n",
    "print('Максимальное значение метрики ROC AUC на валидационном наборе:',round(optuna_study_lg_pd['roc_valid'].max(),3))\n",
    "print('Среднее значение метрики ROC AUC на валидационном наборе:',round(optuna_study_lg_pd['roc_valid'].mean(),3))\n",
    "print('Максимальное значение метрики recall_1 на валидационном наборе:',round(optuna_study_lg_pd['recall_1'].max(),3))\n",
    "print('Среднее значение метрики recall_1 на валидационном наборе:',round(optuna_study_lg_pd['recall_1'].mean(),3))\n",
    "print('Максимальное значение метрики precision_1 на валидационном наборе:',round(optuna_study_lg_pd['precision_1'].max(),3))\n",
    "print('Среднее значение метрики precision_1 на валидационном наборе:',round(optuna_study_lg_pd['precision_1'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим график важности гиперпарметров\n",
    "optuna.visualization.plot_param_importances(optuna_study_lg_meta, target_name='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\7.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрики precision_1 от гипер параметров\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_lg_pd,\n",
    "    x='precision_1', #ось абсцисс\n",
    "    y=['roc_train', 'roc_valid', 'recall_1','f1_score'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision_1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\8.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем метрики полученные при optuna оптимизации\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_lg_pd[(optuna_study_lg_pd['roc_valid']>0.9999*optuna_study_lg_pd['roc_valid'].max())],\n",
    "    x='number', #ось абсцисс\n",
    "    y=['roc_train','roc_valid','recall_1','precision_1'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость качества модели от trials optuna', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='trials optuna',\n",
    "    yaxis_title='Величина метрики')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\9.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Вывод:  \n",
    "\n",
    "Их всех выбираем точку $number =127$.   \n",
    "В этой точке самые оптимальные значения $recall$ и $precision$.  \n",
    "Посмотрим каким значениям гиперпараметров соотвествует выбранная точка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best C: 0.912\n",
      "best class 0 weight: 0.269\n",
      "best class 1 weight: 0.678\n",
      "best class 1 percent: 0.194\n",
      "best k features: 334\n",
      "best random state: 671463\n",
      "time for best train: 0 minutes\n",
      "\n",
      "ROC AUC на обучающем наборе: 0.741\n",
      "ROC AUC на валидационном наборе: 0.73\n",
      "precision класса 1: 0.089\n",
      "recall класса 1: 0.436\n"
     ]
    }
   ],
   "source": [
    "# определим номер лучшге варианта\n",
    "best_optuna_number = 127\n",
    "\n",
    "# сформируем словарь лучших гипирпарметров RandomForestClassifier\n",
    "best_param_lr = {\n",
    "    'C' : round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_C'].iloc[0],3),\n",
    "    'class_weight' : {0:round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_class_0_weight'].iloc[0],3),\n",
    "                      1:round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_class_1_weight'].iloc[0],3)},\n",
    "    }\n",
    "\n",
    "# создадим перменные\n",
    "best_class_1_percent = optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_class_1_percent'].iloc[0]\n",
    "best_k_features = optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_k_features'].iloc[0]\n",
    "best_random_state = optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['params_random_state'].iloc[0]\n",
    "# Выведем принятые наилучшие праметры\n",
    "print('best C:',best_param_lr['C'])\n",
    "print('best class 0 weight:',best_param_lr['class_weight'][0])\n",
    "print('best class 1 weight:',best_param_lr['class_weight'][1])\n",
    "print('best class 1 percent:',round(best_class_1_percent,3))\n",
    "print('best k features:',best_k_features)\n",
    "print('best random state:',best_random_state)\n",
    "print('time for best train:',round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['duration'].iloc[0].seconds/60),'minutes')\n",
    "\n",
    "print()\n",
    "print('ROC AUC на обучающем наборе:', round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['roc_train'].iloc[0],3))\n",
    "print('ROC AUC на валидационном наборе:', round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['roc_valid'].iloc[0],3))\n",
    "print('precision класса 1:', round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['precision_1'].iloc[0],3))\n",
    "print('recall класса 1:', round(optuna_study_lg_pd[optuna_study_lg_pd['number']==best_optuna_number]['recall_1'].iloc[0],3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Обучение модели с лучшими параметрами</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.741\n",
      "ROC AUC на валидационном наборе 0.73\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90    144390\n",
      "           1       0.09      0.44      0.15      5235\n",
      "\n",
      "    accuracy                           0.82    149625\n",
      "   macro avg       0.53      0.64      0.52    149625\n",
      "weighted avg       0.95      0.82      0.87    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем обучающие наборы\n",
    "MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "# поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "# с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "list_c1_percent_id = class_1_percent_samples(my_train_pd,best_class_1_percent,random_state=best_random_state)[:1000000]\n",
    "\n",
    "# подготовим данные для обучения\n",
    "MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "# освободим память от \"тяжелых\" и ненужных файлов\n",
    "del MX_train_pd, my_train_pd\n",
    "gc.collect()\n",
    "\n",
    "# с помощью класса SelectKBest получим список лучших признаков\n",
    "selector = SelectKBest(f_classif, k=best_k_features)\n",
    "selector.fit(MX_train_balanced, my_train_balanced)\n",
    "list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "\n",
    "# обучаем модель LogisticRegression с наилучшеми параметрами\n",
    "logistic_regression = linear_model.LogisticRegression(\n",
    "        **best_param_lr,\n",
    "        random_state=42,\n",
    "        max_iter=10000)\n",
    "logistic_regression.fit(MX_train_balanced, my_train_balanced)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train_balanced, my_train_balanced\n",
    "gc.collect()\n",
    "\n",
    "# загружаем тестовые наборы\n",
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()[list_best_features].to_numpy()\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()[list_best_features].to_numpy()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_lr__pred_proba = logistic_regression.predict_proba(MX_train)[:,1]\n",
    "my_valid_lr_pred_proba = logistic_regression.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_lr_pred = logistic_regression.predict(MX_valid)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train, MX_valid\n",
    "gc.collect()\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_lr__pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_lr_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_lr_pred,zero_division=0))\n",
    "\n",
    "# time: 2m 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:MediumBlue\">Построение модели Gradient Boosting Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Baseline обучение модели Hist Gradient Boosting Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv').set_index('id')['flag']\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv').set_index('id')['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.781\n",
      "ROC AUC на валидационном наборе 0.732\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    144390\n",
      "           1       0.00      0.00      0.00      5235\n",
      "\n",
      "    accuracy                           0.96    149625\n",
      "   macro avg       0.48      0.50      0.49    149625\n",
      "weighted avg       0.93      0.96      0.95    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель Gradient Boosting Classifier\n",
    "gradient_boosting = HistGradientBoostingClassifier(random_state=42)\n",
    "gradient_boosting.fit(MX_train,my_train)\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_rf__pred_proba = gradient_boosting.predict_proba(MX_train)[:,1]\n",
    "my_valid_rf_pred_proba = gradient_boosting.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_rf_pred = gradient_boosting.predict(MX_valid)\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_rf__pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_rf_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_rf_pred))\n",
    "\n",
    "# time: 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Выводы:\n",
    "\n",
    "В сравнение с моделью $HistGradientBoostingClassifier$ построенной   \n",
    "на несбалансированных данных $transform$ $data$ $stat$:\n",
    "1. Качество модели по метрике $ROC AUC$ не изменилась.\n",
    "2. Качество модели по метрике $recall_1$ значительно  \n",
    "улучшилось. Правда, $recall_0$ уменьшилось.\n",
    "3. Качество модели по метрике $precision_1$ уменьшилось.   \n",
    "4. На порядок уменьшилось время обучения модели. Это логично,  \n",
    "велична сбалансированной выборки всего $107000$ $samples$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Подбор гиперпараметров модели</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроим оптимизацию гипер параметров\n",
    "def optuna_gb_meta(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  learning_rate = trial.suggest_float('learning_rate',0.01,0.1)\n",
    "  max_iter = trial.suggest_int('max_iter', 150, 250,step = 1)\n",
    "  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 60,step = 1)\n",
    "  max_depth = trial.suggest_int('max_depth', 1, 10,step = 1)\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 60,step = 1)\n",
    "  max_features = trial.suggest_float('max_features',0.1,1)\n",
    "  l2_regularization = trial.suggest_float('l2_regularization',0.01,1)\n",
    "  class_0_weight = trial.suggest_float('class_0_weight',0.01,1)\n",
    "  class_1_weight = trial.suggest_float('class_1_weight',0.01,1)\n",
    "  k_features = trial.suggest_int('k_features', 20, 342,step = 1)\n",
    "  class_1_percent = trial.suggest_float('class_1_percent',0.01,1)\n",
    "  random_state = trial.suggest_int('random_state', 1, 1000000)\n",
    "\n",
    "  # создаем модель\n",
    "  optuna_gradient_boosting_meta = HistGradientBoostingClassifier(\n",
    "      learning_rate= learning_rate,\n",
    "      max_iter = max_iter,\n",
    "      max_leaf_nodes =max_leaf_nodes,\n",
    "      max_depth = max_depth,\n",
    "      min_samples_leaf = min_samples_leaf,\n",
    "      max_features=max_features,\n",
    "      l2_regularization = l2_regularization,\n",
    "      class_weight={0:class_0_weight,1:class_1_weight},\n",
    "      random_state=random_state)\n",
    "\n",
    "  # Загружаем обучающие наборы\n",
    "  MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "  my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "  # поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "  my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "  # с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "  list_c1_percent_id = class_1_percent_samples(my_train_pd,class_1_percent,random_state=random_state)[:1000000]\n",
    "\n",
    "  # подготовим данные для обучения\n",
    "  MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "  my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "  # освободим память от \"тяжелых\" и ненужных файлов\n",
    "  del MX_train_pd, my_train_pd\n",
    "  gc.collect()\n",
    "  \n",
    "  # с помощью класса SelectKBest получим список лучших признаков\n",
    "  selector = SelectKBest(f_classif, k=k_features)\n",
    "  selector.fit(MX_train_balanced, my_train_balanced)\n",
    "  list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "  MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "  # я не воспользовался параметром timeout метода optimize потому, что  \n",
    "  # optimize отставливает поиск параметров, если время обучения \n",
    "  # превышает timeout. Мне нужно чтобы поиск параметров продолжился.\n",
    "  try:\n",
    "    # для модуля func_time_out упакуем обучение модели в функцию try_func\n",
    "    def try_func():\n",
    "            return optuna_gradient_boosting_meta.fit(MX_train_balanced,my_train_balanced)\n",
    "    # обучаем модель с ограничением по времени 10 минут (600 секунд)\n",
    "    optuna_gradient_boosting_meta = func_timeout.func_timeout(600, try_func)\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "\n",
    "    # загружаем тестовые наборы\n",
    "    MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()[list_best_features].to_numpy()\n",
    "    MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()[list_best_features].to_numpy()\n",
    "    my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "    my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "    # делаем предсказание на обучающем и валидационном наборе\n",
    "    # cчитаем метрики для класса 1 добавляем их в список\n",
    "    roc_train = metrics.roc_auc_score(my_train, optuna_gradient_boosting_meta.predict_proba(MX_train)[:,1])\n",
    "    roc_valid = metrics.roc_auc_score(my_valid, optuna_gradient_boosting_meta.predict_proba(MX_valid)[:,1])\n",
    "    f1_score = metrics.f1_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid))\n",
    "    recall_1 = metrics.recall_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "    precision_1 = metrics.precision_score(my_valid, optuna_gradient_boosting_meta.predict(MX_valid),average=None,zero_division=0)[1]\n",
    "\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train, MX_valid, my_train, my_valid\n",
    "    gc.collect()\n",
    "\n",
    "  except func_timeout.FunctionTimedOut:\n",
    "    # удаляем крупные файлы чтобы высвободить память \n",
    "    del MX_train_balanced, my_train_balanced\n",
    "    gc.collect()\n",
    "    \n",
    "    # фиксируем пустые значения метрик\n",
    "    roc_train = 0\n",
    "    roc_valid = 0\n",
    "    f1_score = 0\n",
    "    recall_1 = 0\n",
    "    precision_1 = 0\n",
    "    pass\n",
    "\n",
    "  return roc_train, roc_valid, f1_score, recall_1, precision_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 19:23:05,011] A new study created in RDB with name: HistGradientBoostingClassifier_meta_first_stat\n"
     ]
    }
   ],
   "source": [
    "# ну случай удаления обучения\n",
    "# optuna.delete_study(study_name=\"HistGradientBoostingClassifier_meta_first_stat\", storage='sqlite:///optuna_studies.db')\n",
    "\n",
    "# cоздаем объект исследования\n",
    "# чтобы модель не переобучалась минимизируем roc_train \n",
    "# и максимизируем roc_valid\n",
    "optuna_study_gb_meta = optuna.create_study(study_name=\"HistGradientBoostingClassifier_meta_first_stat\", \n",
    "                               directions=['maximize','maximize','maximize','maximize','maximize'], \n",
    "                               sampler=optuna.samplers.TPESampler(),\n",
    "                               pruner='Hyperband',\n",
    "                               storage='sqlite:///optuna_studies.db',\n",
    "                               load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "optuna_study_gb_meta.optimize(optuna_gb_meta, n_trials=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Анализ гиперпараметров</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>roc_train</th>\n",
       "      <th>roc_valid</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_class_0_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>params_k_features</th>\n",
       "      <th>params_l2_regularization</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_max_depth</th>\n",
       "      <th>params_max_features</th>\n",
       "      <th>params_max_iter</th>\n",
       "      <th>params_max_leaf_nodes</th>\n",
       "      <th>params_min_samples_leaf</th>\n",
       "      <th>params_random_state</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>0.782753</td>\n",
       "      <td>0.735005</td>\n",
       "      <td>0.160974</td>\n",
       "      <td>0.311939</td>\n",
       "      <td>0.108476</td>\n",
       "      <td>2025-04-20 20:28:12.856070</td>\n",
       "      <td>2025-04-20 20:28:55.804379</td>\n",
       "      <td>0 days 00:00:42.948309</td>\n",
       "      <td>0.129293</td>\n",
       "      <td>...</td>\n",
       "      <td>342</td>\n",
       "      <td>0.791043</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>9</td>\n",
       "      <td>0.530224</td>\n",
       "      <td>243</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>940750</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>0.771059</td>\n",
       "      <td>0.734394</td>\n",
       "      <td>0.155815</td>\n",
       "      <td>0.201146</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>2025-04-20 20:28:55.809659</td>\n",
       "      <td>2025-04-20 20:29:48.277531</td>\n",
       "      <td>0 days 00:00:52.467872</td>\n",
       "      <td>0.137148</td>\n",
       "      <td>...</td>\n",
       "      <td>337</td>\n",
       "      <td>0.778605</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>9</td>\n",
       "      <td>0.519322</td>\n",
       "      <td>242</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>917998</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>0.784517</td>\n",
       "      <td>0.735330</td>\n",
       "      <td>0.142373</td>\n",
       "      <td>0.152627</td>\n",
       "      <td>0.133411</td>\n",
       "      <td>2025-04-20 20:29:48.283069</td>\n",
       "      <td>2025-04-20 20:30:39.932995</td>\n",
       "      <td>0 days 00:00:51.649926</td>\n",
       "      <td>0.130064</td>\n",
       "      <td>...</td>\n",
       "      <td>342</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>9</td>\n",
       "      <td>0.524345</td>\n",
       "      <td>240</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>936463</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0.785365</td>\n",
       "      <td>0.735699</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.117670</td>\n",
       "      <td>0.141903</td>\n",
       "      <td>2025-04-20 20:30:39.938552</td>\n",
       "      <td>2025-04-20 20:31:30.076055</td>\n",
       "      <td>0 days 00:00:50.137503</td>\n",
       "      <td>0.157550</td>\n",
       "      <td>...</td>\n",
       "      <td>335</td>\n",
       "      <td>0.810109</td>\n",
       "      <td>0.020111</td>\n",
       "      <td>9</td>\n",
       "      <td>0.498496</td>\n",
       "      <td>244</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>942875</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0.779582</td>\n",
       "      <td>0.735173</td>\n",
       "      <td>0.156448</td>\n",
       "      <td>0.365807</td>\n",
       "      <td>0.099501</td>\n",
       "      <td>2025-04-20 20:31:30.081065</td>\n",
       "      <td>2025-04-20 20:32:14.829955</td>\n",
       "      <td>0 days 00:00:44.748890</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>...</td>\n",
       "      <td>341</td>\n",
       "      <td>0.779593</td>\n",
       "      <td>0.017133</td>\n",
       "      <td>9</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>239</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>914266</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  roc_train  roc_valid  f1_score  recall_1  precision_1  \\\n",
       "295     295   0.782753   0.735005  0.160974  0.311939     0.108476   \n",
       "296     296   0.771059   0.734394  0.155815  0.201146     0.127159   \n",
       "297     297   0.784517   0.735330  0.142373  0.152627     0.133411   \n",
       "298     298   0.785365   0.735699  0.128655  0.117670     0.141903   \n",
       "299     299   0.779582   0.735173  0.156448  0.365807     0.099501   \n",
       "\n",
       "                datetime_start          datetime_complete  \\\n",
       "295 2025-04-20 20:28:12.856070 2025-04-20 20:28:55.804379   \n",
       "296 2025-04-20 20:28:55.809659 2025-04-20 20:29:48.277531   \n",
       "297 2025-04-20 20:29:48.283069 2025-04-20 20:30:39.932995   \n",
       "298 2025-04-20 20:30:39.938552 2025-04-20 20:31:30.076055   \n",
       "299 2025-04-20 20:31:30.081065 2025-04-20 20:32:14.829955   \n",
       "\n",
       "                  duration  params_class_0_weight  ...  params_k_features  \\\n",
       "295 0 days 00:00:42.948309               0.129293  ...                342   \n",
       "296 0 days 00:00:52.467872               0.137148  ...                337   \n",
       "297 0 days 00:00:51.649926               0.130064  ...                342   \n",
       "298 0 days 00:00:50.137503               0.157550  ...                335   \n",
       "299 0 days 00:00:44.748890               0.117100  ...                341   \n",
       "\n",
       "     params_l2_regularization  params_learning_rate  params_max_depth  \\\n",
       "295                  0.791043              0.019017                 9   \n",
       "296                  0.778605              0.011525                 9   \n",
       "297                  0.826667              0.018519                 9   \n",
       "298                  0.810109              0.020111                 9   \n",
       "299                  0.779593              0.017133                 9   \n",
       "\n",
       "     params_max_features  params_max_iter  params_max_leaf_nodes  \\\n",
       "295             0.530224              243                     58   \n",
       "296             0.519322              242                     57   \n",
       "297             0.524345              240                     59   \n",
       "298             0.498496              244                     56   \n",
       "299             0.554455              239                     58   \n",
       "\n",
       "     params_min_samples_leaf  params_random_state     state  \n",
       "295                        1               940750  COMPLETE  \n",
       "296                        4               917998  COMPLETE  \n",
       "297                        3               936463  COMPLETE  \n",
       "298                        1               942875  COMPLETE  \n",
       "299                        1               914266  COMPLETE  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# из полученного результат соврмируем Data Frame\n",
    "optuna_study_gb_meta_pd = optuna_study_gb_meta.trials_dataframe()\n",
    "# переименуем столбы\n",
    "optuna_study_gb_meta_pd.rename(columns={\n",
    "    'values_0': 'roc_train',\n",
    "    'values_1': 'roc_valid',\n",
    "    'values_2': 'f1_score',\n",
    "    'values_3': 'recall_1',\n",
    "    'values_4': 'precision_1'\n",
    "},inplace=True)\n",
    "optuna_study_gb_meta_pd.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение метрики ROC AUC на валидационном наборе: 0.736\n",
      "Среднее значение метрики ROC AUC на валидационном наборе: 0.729\n",
      "Максимальное значение метрики f1_score на валидационном наборе: 0.163\n",
      "Среднее значение метрики f1_score на валидационном наборе: 0.111\n",
      "Максимальное значение метрики recall_1 на валидационном наборе: 1.0\n",
      "Среднее значение метрики recall_1 на валидационном наборе: 0.418\n",
      "Максимальное значение метрики precision_1 на валидационном наборе: 0.462\n",
      "Среднее значение метрики precision_1 на валидационном наборе: 0.104\n"
     ]
    }
   ],
   "source": [
    "# покаждем статистику обучения\n",
    "print('Максимальное значение метрики ROC AUC на валидационном наборе:',round(optuna_study_gb_meta_pd['roc_valid'].max(),3))\n",
    "print('Среднее значение метрики ROC AUC на валидационном наборе:',round(optuna_study_gb_meta_pd['roc_valid'].mean(),3))\n",
    "print('Максимальное значение метрики f1_score на валидационном наборе:',round(optuna_study_gb_meta_pd['f1_score'].max(),3))\n",
    "print('Среднее значение метрики f1_score на валидационном наборе:',round(optuna_study_gb_meta_pd['f1_score'].mean(),3))\n",
    "print('Максимальное значение метрики recall_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['recall_1'].max(),3))\n",
    "print('Среднее значение метрики recall_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['recall_1'].mean(),3))\n",
    "print('Максимальное значение метрики precision_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['precision_1'].max(),3))\n",
    "print('Среднее значение метрики precision_1 на валидационном наборе:',round(optuna_study_gb_meta_pd['precision_1'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим график важности гиперпарметров\n",
    "optuna.visualization.plot_param_importances(optuna_study_gb_meta, target_name='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\10.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрики precision_1 от гиперпараметров\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_gb_meta_pd,\n",
    "    x='precision_1', #ось абсцисс\n",
    "    y=['recall_1','f1_score','roc_train','roc_valid'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision класса 1 от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision класса 1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\11.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим зависимость метрик качества модели от number\n",
    "\n",
    "fig = px.scatter(\n",
    "    data_frame=optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['roc_valid']>0.9999*optuna_study_gb_meta_pd['roc_valid'].max()],\n",
    "    x='number', #ось абсцисс\n",
    "    y=['roc_train','roc_valid','recall_1','precision_1'], #ось ординат\n",
    ")\n",
    "fig.update_layout(\n",
    "    title ={\n",
    "        'text':'Зависимость precision от гиперпараметров модели', # Имя рабочей плоскости\n",
    "        'font':{'size':35,'family':\"Times New Roman\"}, # размер и стиль написания имени рабочей плоскости\n",
    "        'x':0.5, # Смешение имени по оси \"x\" на половину рабочей плоскости\n",
    "        },\n",
    "    height =800,# Высота рабочей плоскости\n",
    "    width = 1350, # Ширина рабочей плоскости\n",
    "    bargap=0.2, # Добавил расстояния между столбами гистограммы\n",
    "    xaxis_title='Величина метрики precision_1',\n",
    "    yaxis_title='Величина параметра')\n",
    "fig.update_xaxes(showspikes=True)\n",
    "fig.update_yaxes(showspikes=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\12.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Вывод:  \n",
    "\n",
    "Их всех выбираем точку $number =272$.   \n",
    "В этой точке оптимальные значения метрик $recall_1$ и $precision_1$, а  \n",
    "также относительно высокая метрика $ROC AUC$.   \n",
    "Посмотрим каким значениям гиперпараметров соотвествует выбранная точка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best learning rate: 0.025\n",
      "best max iter: 232\n",
      "best max leaf nodes: 58\n",
      "best max depth: 9\n",
      "best min samples leaf: 7\n",
      "best max features: 0.346\n",
      "best l2 regularization: 0.688\n",
      "best class 0 weight: 0.233\n",
      "best class 1 weight: 0.616\n",
      "best class 1 percent: 0.15\n",
      "best k features: 334\n",
      "best random state: 975267\n",
      "time for best train: 0 minutes\n",
      "\n",
      "ROC AUC на обучающем наборе: 0.788\n",
      "ROC AUC на валидационном наборе: 0.736\n",
      "precision класса 1: 0.103\n",
      "recall класса 1: 0.333\n"
     ]
    }
   ],
   "source": [
    "# определим номер лучшге варианта\n",
    "best_optuna_number = 272\n",
    "\n",
    "# сформируем словарь лучших гипирпарметров HistGradientBoostingClassifier\n",
    "best_param_hgbc = {\n",
    "    'learning_rate' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_learning_rate'].iloc[0],\n",
    "    'max_iter' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_iter'].iloc[0],\n",
    "    'max_leaf_nodes' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_leaf_nodes'].iloc[0],\n",
    "    'max_depth' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_depth'].iloc[0],\n",
    "    'min_samples_leaf' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_min_samples_leaf'].iloc[0],\n",
    "    'max_features' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_max_features'].iloc[0],\n",
    "    'l2_regularization' : optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_l2_regularization'].iloc[0],\n",
    "    'class_weight' : {0: optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_0_weight'].iloc[0],\n",
    "                      1: optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_1_weight'].iloc[0],}\n",
    "    }\n",
    "\n",
    "# определим перменные для лучших значений параметров\n",
    "best_k_features = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_k_features'].iloc[0]\n",
    "best_random_state = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_random_state'].iloc[0]\n",
    "best_class_1_percent = optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['params_class_1_percent'].iloc[0]\n",
    "\n",
    "\n",
    "# Выведем принятые наилучшие параметры\n",
    "print('best learning rate:',round(best_param_hgbc['learning_rate'],3))\n",
    "print('best max iter:',best_param_hgbc['max_iter'])\n",
    "print('best max leaf nodes:',best_param_hgbc['max_leaf_nodes'])\n",
    "print('best max depth:',best_param_hgbc['max_depth'])\n",
    "print('best min samples leaf:',best_param_hgbc['min_samples_leaf'])\n",
    "print('best max features:',round(best_param_hgbc['max_features'],3))\n",
    "print('best l2 regularization:',round(best_param_hgbc['l2_regularization'],3))\n",
    "print('best class 0 weight:',round(best_param_hgbc['class_weight'][0],3))\n",
    "print('best class 1 weight:',round(best_param_hgbc['class_weight'][1],3))\n",
    "\n",
    "print('best class 1 percent:',round(best_class_1_percent,3))\n",
    "print('best k features:',best_k_features)\n",
    "print('best random state:',best_random_state)\n",
    "print('time for best train:',round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['duration'].iloc[0].seconds/60),'minutes')\n",
    "\n",
    "print()\n",
    "print('ROC AUC на обучающем наборе:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['roc_train'].iloc[0],3))\n",
    "print('ROC AUC на валидационном наборе:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['roc_valid'].iloc[0],3))\n",
    "print('precision класса 1:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['precision_1'].iloc[0],3))\n",
    "print('recall класса 1:', round(optuna_study_gb_meta_pd[optuna_study_gb_meta_pd['number']==best_optuna_number]['recall_1'].iloc[0],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:MediumSlateBlue\">Обучение модели с лучшими параметрами</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на обучающем наборе 0.788\n",
      "ROC AUC на валидационном наборе 0.736\n",
      "Основные метрики на валидационом наборе:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93    144390\n",
      "           1       0.10      0.33      0.16      5235\n",
      "\n",
      "    accuracy                           0.88    149625\n",
      "   macro avg       0.54      0.61      0.55    149625\n",
      "weighted avg       0.94      0.88      0.91    149625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загружаем обучающие наборы\n",
    "MX_train_pd = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()\n",
    "my_train_pd = pd.read_csv('features/firstmeta/mfy_train.csv')\n",
    "\n",
    "# поготовим данные my_train_pd к работе с функцией class_1_percent_samples\n",
    "my_train_pd.set_index('id',drop=False,inplace=True)\n",
    "\n",
    "# с помощью функции class_1_percent_samples зададим долю класса 1\n",
    "list_c1_percent_id = class_1_percent_samples(my_train_pd,best_class_1_percent,random_state=best_random_state)[:1000000]\n",
    "\n",
    "# подготовим данные для обучения\n",
    "MX_train_balanced = MX_train_pd.loc[list_c1_percent_id]\n",
    "my_train_balanced = my_train_pd.loc[list_c1_percent_id]['flag'].to_numpy()\n",
    "\n",
    "# освободим память от \"тяжелых\" и ненужных файлов\n",
    "del MX_train_pd, my_train_pd\n",
    "gc.collect()\n",
    "\n",
    "# с помощью класса SelectKBest получим список лучших признаков\n",
    "selector = SelectKBest(f_classif, k=best_k_features)\n",
    "selector.fit(MX_train_balanced, my_train_balanced)\n",
    "list_best_features = selector.get_feature_names_out()\n",
    "\n",
    "MX_train_balanced = MX_train_balanced[list_best_features].to_numpy()\n",
    "\n",
    "# обучаем модель HistGradientBoostingClassifier с наилучшеми параметрами\n",
    "gradient_boosting = HistGradientBoostingClassifier(\n",
    "        **best_param_hgbc,\n",
    "        random_state=best_random_state)\n",
    "gradient_boosting.fit(MX_train_balanced,my_train_balanced)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train_balanced, my_train_balanced\n",
    "gc.collect()\n",
    "\n",
    "# загружаем тестовые наборы\n",
    "MX_train = fp.ParquetFile('features/firstmeta/features_first_meta_stat_train').to_pandas()[list_best_features].to_numpy()\n",
    "MX_valid = fp.ParquetFile('features/firstmeta/features_first_meta_stat_valid').to_pandas()[list_best_features].to_numpy()\n",
    "my_train = pd.read_csv('features/firstmeta/mfy_train.csv')['flag'].to_numpy()\n",
    "my_valid = pd.read_csv('features/firstmeta/mfy_valid.csv')['flag'].to_numpy()\n",
    "\n",
    "# для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "my_train_gb_pred_proba = gradient_boosting.predict_proba(MX_train)[:,1]\n",
    "my_valid_gb_pred_proba = gradient_boosting.predict_proba(MX_valid)[:,1]\n",
    "\n",
    "# Делаем предсказание для валидационной выборки\n",
    "my_valid_gb_pred = gradient_boosting.predict(MX_valid)\n",
    "\n",
    "# удаляем крупные файлы чтобы высвободить память \n",
    "del MX_train, MX_valid\n",
    "gc.collect()\n",
    "\n",
    "#Выводим значения метрик\n",
    "print('ROC AUC на обучающем наборе', round(metrics.roc_auc_score(my_train, my_train_gb_pred_proba),3))\n",
    "print('ROC AUC на валидационном наборе', round(metrics.roc_auc_score(my_valid, my_valid_gb_pred_proba),3))\n",
    "\n",
    "print('Основные метрики на валидационом наборе:')\n",
    "print(metrics.classification_report(my_valid,my_valid_gb_pred,zero_division=0))\n",
    "\n",
    "# time: 1m 40s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
