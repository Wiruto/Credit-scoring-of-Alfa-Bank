{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\logo.png\" alt=\"drawing\" style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  \n",
    "\n",
    "<span style=\"background-size: 600px;background:White;color:REd;font-size: 60px;font-family: Comic Sans MS\">Кредитный скоринг Альфа банка</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Задача</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**\n",
    "\n",
    "Кредитный скоринг – важнейшая банковская задача. Стандартным подходом к ее решению   \n",
    "является построение классических моделей машинного обучения, таких как логистическая   \n",
    "регрессия и градиентный бустинг, на табличных данных, в том числе используя агрегации  \n",
    "от каких-нибудь последовательных данных, например, транзакционных историй клиентов.   \n",
    "Альтернативный подход заключается в использовании последовательных данных “как есть”,   \n",
    "подавая их на вход рекуррентной нейронной сети.\n",
    "\n",
    "В этом соревновании участникам предлагается решить задачу кредитного скоринга клиентов   \n",
    "Альфа-Банка, используя только данные кредитных историй. [Источник](https://www.kaggle.com/competitions/alfa-bank-pd-credit-history)\n",
    "\n",
    "**Данные**\n",
    "\n",
    "Датасет соревнования устроен таким образом, что кредиты для тренировочной выборки взяты   \n",
    "за период в М месяцев, а кредиты для тестовой выборки взяты за последующие K месяцев.\n",
    "\n",
    "Каждая запись кредитной истории содержит самую разнообразную информацию о прошлом кредите   \n",
    "клиента, например, сумму, отношение клиента к кредиту, дату открытия и закрытия, информацию   \n",
    "о просрочках по платежам и др. Все публикуемые данные тщательно анонимизированы.\n",
    "\n",
    "Целевая переменная – бинарная величина, принимающая значения 0 и 1, где 1 соответствует   \n",
    "дефолту клиента по кредиту.\n",
    "\n",
    "\n",
    "**Проверка решений**\n",
    "\n",
    "Метрика соревнования – ROC AUC. Подробнее про метрику можно почитать, например, [здесь](https://dyakonov.org/2017/07/28/auc-roc-площадь-под-кривой-ошибок/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Используемые библиотеки</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# работа с регулярными выражениями\n",
    "import re\n",
    "\n",
    "# библиотеки для работы с табличными данными\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastparquet as fp\n",
    "\n",
    "# генерация случайных чисел\n",
    "import random\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# библиотеки для построения графики\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import nbformat\n",
    "\n",
    "# библиотеки для математических преобразований с массивами данных\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# библиотеки для работы с функциями(частичная передача аргументов в функцию)\n",
    "from functools import partial\n",
    "\n",
    "# библиотеки для работы со статистическими характеристиками\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# библиотеки для работы с pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# проверка временного ряда на статичность\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Импортируем DBSCAN-кластеризацию\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# вставить картинку в Jupiter Notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# линейные модели машинного обучения\n",
    "from sklearn import linear_model\n",
    "\n",
    "# ансамбли моделей машинного обучения\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# поиск гиперпараметров модели\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "import optuna\n",
    "\n",
    " # метрики\n",
    "from sklearn import metrics\n",
    "\n",
    "# библиотека для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# сохранить полученные модели\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "# сборщик мусора\n",
    "import gc\n",
    "\n",
    "# для ограничения времени выполнения функции\n",
    "import signal\n",
    "import func_timeout\n",
    "\n",
    "# для отслеживания времени выполнения функции\n",
    "import time\n",
    "\n",
    "# очистить output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# испорт результирующего блендинга моделей\n",
    "from blending import blendingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:DodgerBlue\">Разработка инструментов преобразования данных</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция torow_transformer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция torow_transformer\n",
    "\n",
    "# Назначение: Преобразование признака столбца в признаки строки\n",
    "#               с сохранением обратной последовательности в признаке.\n",
    "#               (извлечение последних операций клиента)\n",
    "\n",
    "# Внешние переменные функции: DataFrame, n_last\n",
    "#   DataFrame - исоходый DataFrame\n",
    "#   n_last - необходимое счисло послдених операций клиента\n",
    "#   Структура DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: New DataFrame\n",
    "#   Признаки New DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1.1\n",
    "#       3. feature1.2\n",
    "#       4. feature1.3\n",
    "#       ...\n",
    "#       5. feature1.N\n",
    "#       6. feature2.1\n",
    "#       7. feature2.2\n",
    "#       ...\n",
    "#       8. feature2.N\n",
    "#       ...\n",
    "#   где featureX.1 - соотвествует последней операции клиента,\n",
    "#       featureX.2 - соотвествует предпоследней операции клиента,\n",
    "#       .....\n",
    "\n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных DataFrame\n",
    "# 3. формируем карту признаков dict_features:\n",
    "#    3.1. каждый признак кодируется следующим образом: 'fn',\n",
    "#         где n - порядковый номер признака\n",
    "#    3.2. полные имена признаков задаются следующим образом: 'feature_N',\n",
    "#         где N - порядок клиентской операции (большему N соотвествует, более ранняя операция)\n",
    "\n",
    "# 4. Преобразуем данные к массиву\n",
    "# 5. Группируем массив для каждого клиента\n",
    "# 6. К групированному массиву прменяем следующие преобразования:\n",
    "#    6.1. Обращаем порядок клиенских операций\n",
    "#    6.2. выбираем посление n_last операций\n",
    "#    6.3. Если число клиенски операций меньше чем n_last, дополняем их нулями\n",
    "# 7. Преобразуем полученные данные к DataFrame\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. pd_data - исходный DataFrame\n",
    "# 2. n_last - необходимое число последних операций клиента\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 5. dict_features - локальная карта признаков\n",
    "# 6. rn_id - список количества операций для каждого клиента\n",
    "# 6. array_data - данные преобразоанные к numpy-массиву\n",
    "# 6. split_array - сгрупированные по клиентам данные преобразованные\n",
    "\n",
    "# обьявлем функцию\n",
    "def torow_func(dict_params):\n",
    "    pd_data = dict_params['data']\n",
    "    n_last = dict_params['n_last']\n",
    "\n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "    \n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "\n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {}\n",
    "    \n",
    "    # заполним словарь dict_features\n",
    "    num_f=0\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for num_feature in range(1,n_last+1):\n",
    "            dict_features['f'+str(num_feature+num_f)] = feature+'_'+str(num_feature)\n",
    "        num_f+=n_last\n",
    "    \n",
    "\n",
    "    # формируем словарь rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)\n",
    "    \n",
    "    # определим порядок последующих преобразований в функции\n",
    "    def transform_array(array_id):\n",
    "        # обратим порядок клиентских операций \n",
    "        reverse_array_id = array_id[::,::-1]\n",
    "        # выбрем после n операций клиента\n",
    "        list_n_last = reverse_array_id[::,:n_last]\n",
    "        # если клиенских операций было меньше чем n_last\n",
    "        # дополним недастающие нулями и преобразуем данные к строке\n",
    "        if len(list_n_last[0])<n_last:\n",
    "            full_list_n_last = np.hstack((list_n_last,np.zeros((list_n_last.shape[0],n_last-len(list_n_last[0])),dtype='int64')))\n",
    "            # преобразуем список к строке\n",
    "            full_list_n_last = full_list_n_last.reshape(-1)\n",
    "        else:\n",
    "            full_list_n_last = list_n_last.reshape(-1)\n",
    "        return full_list_n_last\n",
    "\n",
    "    # применим transform_array преобразование к списку split_array\n",
    "    list_data = np.array(list(map(transform_array,split_array)))[:-1]\n",
    "    \n",
    "    # преобразуем полученные данные к dataframe\n",
    "    dataframe = pd.DataFrame(data=list_data, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "    \n",
    "    return dataframe, dict_features,rn_id\n",
    "\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "torow_transformer = FunctionTransformer(torow_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция features_from_transform_data_torow\n",
    "\n",
    "# Назначение: Извлечение из данных, над которомы совершено \n",
    "#             row_fich_transformer() преобразование, признаков  \n",
    "#             соотвествующих заданному числу последних  \n",
    "#             опреаций клиента n_last\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. n_last - необходимое число последних операций клиента\n",
    "#       2. n_groups - число групп признаков в transform_data_torow\n",
    "#       3. N_last - число последних операций клиента показанных в transform_data_torow\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. list_n_last_features - список признаков в transform_data_torow\n",
    "#    соотвествующий заданному числу n_last. \n",
    "    \n",
    "\n",
    "# обьявлем функцию\n",
    "def features_from_transform_data_torow(n_last,n_groups,N_last):\n",
    "    # создадим список под необходимые признаки\n",
    "    list_n_last_features = []\n",
    "    \n",
    "    # обьявим начальное значение в группе признаков\n",
    "    n_start = 0\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        for n in range(n_last):\n",
    "            list_n_last_features.append(n+n_start)\n",
    "        n_start+=N_last\n",
    "    \n",
    "    return list_n_last_features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция diff_feature</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция diff_feature\n",
    "\n",
    "# Назначение: Определение дифференциальных характеристик ряда \n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1.Series/np.array/list\n",
    "\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. diff_list - Список из значений:\n",
    "#                   1.1. speed - скорость изменения ряда;\n",
    "#                   1.2. accel - ускорение изменения ряда;\n",
    "#                   1.3. bias - смещение ряда;\n",
    "#                   1.4. pulse - импульс ряда;\n",
    "\n",
    "# обьявлем функцию\n",
    "def diff_feature(data):\n",
    "    # преобразуем данные к numpy массиву\n",
    "    data = np.array(data)\n",
    "    # расчитаем необходимые характеристики\n",
    "    speed = round(float(np.diff(data,1).mean()),2)\n",
    "    accel = round(float(np.diff(data,2).mean()),2)\n",
    "    bias = round(float(np.diff(data,1).sum()),2)\n",
    "    pulse = round(float(np.diff(data,2).sum()),2)\n",
    "    # сформируем из найденных значений в список\n",
    "    diff_list = [speed,accel,bias,pulse]\n",
    "    return diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция statistic_features</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция statistic_features\n",
    "\n",
    "# Назначение: Извлечение основных статистических характеристик\n",
    "#             из признаков в исходном DataFrame.\n",
    "\n",
    "# Внешние переменные функции: DataFrame\n",
    "#   Признаки DataFrame:\n",
    "#       1. id\n",
    "#       2. feature1\n",
    "#       3. feature2\n",
    "#       4. feature3\n",
    "#       ...\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. dataframe - таблица с данными. \n",
    "#    Признаки dataframe:\n",
    "#       1. id\n",
    "#       2. feature1_mean\n",
    "#       3. fearture1_hmean\n",
    "#       4. feature1_std\n",
    "#       5. feature1_min\n",
    "#       6. feature1_25%\n",
    "#       7. feature1_50%\n",
    "#       8. feature1_75%\n",
    "#       9. feature1_max\n",
    "#       10. feature1_mode\n",
    "#       11. feature1_frequency_mode\n",
    "#       12. feature2_mean\n",
    "#       ...\n",
    "    \n",
    "# 2. dict_features - словарь (карта) признаков,\n",
    "#    в котором отображается сокращения признаков и их расщифровка\n",
    "\n",
    "# Алгоритм работы функции:\n",
    "# 1. извлекаем признаки из данных\n",
    "# 2. формируем карту признаков:\n",
    "#    2.1. каждый признак кодируется следующим образом: 'fn' где n - порядковый номер признака\n",
    "#    2.2. полные имена признаков задаются следующим образом:\n",
    "#         2.2.1 если в исходном dataframe признак бинарный, то: \"Исходное имя признака\"+\"binary\"\n",
    "#         2.2.2 если в исходном dataframe признак не бинарный, то: \"Исходное имя признака\"+\"Статистическая характеристика\"\n",
    "# 3. для каждого клиента по каждому признаку из исходного dataframe расчитываем статистические характеристики\n",
    "# 4. записываем полученные значение в новый dataframe\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. dict_agg_function - словарь из агригирующих функций\n",
    "#       keys: имена для обращения к функциям:\n",
    "#       values: lamda-функция, соотвествующей статистической характристики\n",
    "# 2. list_features - список для хранения признаков исхдного dataframe внутри функции\n",
    "# 3. list_id - список для хранения id клиентов внутри функции\n",
    "# 4. dict_features - локальная карта признаков\n",
    "# 5. k - номер признака в dict_features на текущей итерации\n",
    "# 6. dataframe - результирующий dataframe\n",
    "\n",
    "# обьявлем функцию\n",
    "def statistic_features(pd_data):\n",
    "    # формируем список из функций для статистических преобразований\n",
    "    # предусмотрим работу функций на случай, если в массиве данных всего 1 строка\n",
    "    dict_agg_function = {\n",
    "    'ptp' : lambda x: 0 if len(x) <= 3 else np.ptp(x),\n",
    "    'mean': lambda x: 0 if len(x) <= 3 else x.mean(), \n",
    "    'gmean' : lambda x: stats.gmean(x),   \n",
    "    'hmean': lambda x: stats.gmean(x),\n",
    "    'pmean25': lambda x: stats.pmean(x,25),\n",
    "    'pmean50': lambda x: stats.pmean(x,50),\n",
    "    'pmean75': lambda x: stats.pmean(x,75),\n",
    "    'expectile25': lambda x: stats.expectile(x,0.25),\n",
    "    'expectile50': lambda x: stats.expectile(x),\n",
    "    'expectile75': lambda x: stats.expectile(x,0.75),\n",
    "    'moment': lambda x: stats.moment(x),\n",
    "    'std': lambda x: 0 if len(x) <= 3 else np.std(x),\n",
    "    'min': lambda x: min(x),\n",
    "    '20%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=20),\n",
    "    '30%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=30),\n",
    "    '40%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=40),\n",
    "    '50%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=50),\n",
    "    '60%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=60),\n",
    "    '70%': lambda x: x.mean() if len(x) <= 3 else np.percentile(x,q=70),\n",
    "    'max': lambda x: max(x),\n",
    "    'mode': lambda x: statistics.mean(statistics.multimode(x)),\n",
    "    'frequency_mode': lambda x: round(list(x).count(statistics.multimode(x)[0])*len(statistics.multimode(x))/len(x),2),\n",
    "    'cov' : lambda x: 0 if len(x) <= 3 else np.cov(x),\n",
    "    'histogram' : lambda x: 0 if len(x) <= 3 else np.histogram(x)[1].mean(), \n",
    "    'speed': lambda x: 0 if len(x) <= 3 else diff_feature(x)[0],\n",
    "    'accel': lambda x: 0 if len(x) <= 3 else diff_feature(x)[1],\n",
    "    'bias': lambda x: 0 if len(x) <= 3 else diff_feature(x)[2],\n",
    "    'pulse': lambda x: 0 if len(x) <= 3  else diff_feature(x)[3]\n",
    "    } \n",
    "\n",
    "    # напишем функцию для преобразования массива до статистических характеристик\n",
    "    def stat_func(array_data): \n",
    "        # сформируем лист под результаты преобразования\n",
    "        list_for_result = []\n",
    "        # запишем все статистические харкетристики из словаря dict_agg_function\n",
    "        for func in dict_agg_function.values():\n",
    "            list_for_result.append(func(array_data))\n",
    "        return np.array(list_for_result).round(3)\n",
    "\n",
    "    # напишем функцию для применения функции stat_func к списку\n",
    "    def submap(list_data):\n",
    "        # расчитаем количество операция клиента\n",
    "        max_rn = len(list_data[0])\n",
    "        # получим статистические характристики массива\n",
    "        list_stat_features = np.array(list(map(stat_func,list_data))).reshape(-1)\n",
    "        return np.hstack((max_rn,list_stat_features))\n",
    "    \n",
    "    # извлекаем список \"id\" клиентов\n",
    "    list_id = pd_data['id'].unique().tolist()\n",
    "\n",
    "    # извлекаем список признаков из данных        \n",
    "    list_features = pd_data.columns.drop(['id','rn'])\n",
    "    \n",
    "    # формируем словарь для зашифрованных признаков\n",
    "    dict_features = {'f1':'count'}\n",
    "    k=1 # порядковый номер защифрованного признака\n",
    "\n",
    "    # заполним словарь dict_features\n",
    "    for feature in list_features:\n",
    "        # шифруем признак: fk = \"feature_agg_function\"\n",
    "        for key_function in dict_agg_function.keys():\n",
    "            k+=1\n",
    "            dict_features['f'+str(k)] = feature+'_'+key_function\n",
    "\n",
    "    # формируем список rn_id\n",
    "    rn_id = pd_data.groupby('id')['id'].count().to_list()\n",
    "\n",
    "    # для улучшения производительности преобразуем DataFrame в array-массив\n",
    "    array_data = np.array(pd_data.iloc[:,2:]).transpose()\n",
    "\n",
    "    # \"порежем массив\" по длине кредитной истории клиента\n",
    "    split_array = np.array_split(array_data, np.cumsum(rn_id),axis=1)[:-1]\n",
    "\n",
    "    # получем статические характеристики признаков\n",
    "    stat_features = np.array(list(map(submap,split_array)))\n",
    "    \n",
    "    # Сформируем dataframe из полученных данных\n",
    "    dataframe = pd.DataFrame(data=stat_features, columns=dict_features.keys())\n",
    "\n",
    "    # добавим столбец id\n",
    "    dataframe.insert(0,'id',list_id)\n",
    "\n",
    "    return dataframe, dict_features\n",
    "\n",
    "# преобразуем функции в инструмент для преобразования данных (Transformer)\n",
    "stat_transformer = FunctionTransformer(statistic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция corr_transform_to_force</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция corr_transform_to_force\n",
    "\n",
    "# Назначение: из матрицы взаимных корреляций\n",
    "#             выделить не корелирующие признаки\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. df.corr() - матрица корреляций\n",
    "#           2. threshold - порог значимости корреляции:\n",
    "#               значение коэффициента корреляции, больше которого\n",
    "#               признаки считаются скоррелированными.\n",
    "\n",
    "# Пояснение: \n",
    "# Под силой корреляции будем понимать следующее: если коэффициент \n",
    "# коррелиции между признаками больше значения threshold, то принимаем,\n",
    "# что между признаками сильная корреляционная связь значение коэффициента \n",
    "# коррелияции заменяем на 1, иначе корреляционная связь слабая и значение \n",
    "# коээфициента корреляции заменяем на 0\n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. corr_matrix - матрица корреляций(отражает силу корреляции)\n",
    "# 2. list_ncorr_features - список не скореллированных признаков\n",
    "# 3. corr_force - сила корреляции всей матрицы: отношение числа скоррелированных \n",
    "# признаков к числу всех признаков в матрице\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. coor_force - функция преобазующая значение\n",
    "#        коэффициента корряляции в силу корреляции\n",
    "# 2. corr_matrix - матрица отражающая силу корряляции между признаками\n",
    "# 3. max_corr - максимальное число взаимных корреляций между признаками\n",
    "# 4. list_ncorr_features - список не коррелируемых признаков\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def corr_transform_to_force(matrix,threshold=0.7):\n",
    "    list_features = matrix.index.tolist()\n",
    "    \n",
    "    \n",
    "    # создадим функцию для разметки матрицы корреляции\n",
    "    # 1 - корреляция признаков выше порога значимости threshold\n",
    "    # 0 - корреляция признаков ниже порога значимости threshold\n",
    "    corr_force = lambda x: 1 if x >threshold else 0\n",
    "    # выполним разметку матрицы корреляции\n",
    "    corr_matrix = matrix.map(lambda x: corr_force(x))\n",
    "    \n",
    "    # алгоритм отбора не коррелиарных признаков:\n",
    "    #   1. Найдем признак с наибольшим числом взаимных корреляций\n",
    "    #   2. удалим найденный признак\n",
    "    #   3. составим матрицу корреляций из отсавшися признаков\n",
    "    #   4. повторяем пункты 1-3 до тех пор пока в матрице не останутся \n",
    "    #       не коррелированные признаки\n",
    "\n",
    "    # ищем наибольшее число взаимных корреляций среди признаков\n",
    "    max_corr = corr_matrix.sum().max()\n",
    "\n",
    "    while max_corr > 1:\n",
    "        # определяем признак с наибольшим числом взаимных корреляций\n",
    "        max_corr_feature = corr_matrix.sum()[corr_matrix.sum()==corr_matrix.sum().max()].index[0]\n",
    "        # удалем признак из матрицы корреляций\n",
    "        corr_matrix = corr_matrix.drop(max_corr_feature).drop(max_corr_feature,axis=1)\n",
    "        max_corr = corr_matrix.sum().max()\n",
    "    # запишем не скоррелированные признаки в список\n",
    "    list_ncorr_features = corr_matrix.index.tolist()\n",
    "    # найдем силу корреляции всей матрицы как отношение\n",
    "    # количества скоррелированных признаков к всмеу количеству признаков\n",
    "    corr_force = round(1-len(list_ncorr_features)/len(list_features),3)\n",
    "    return corr_matrix, list_ncorr_features, corr_force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция search_DBSCAN_parameters</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция search_DBSCAN_parameters\n",
    "\n",
    "# Назначение: Для подбора eps и min_samples параметров,\n",
    "#               функция \"прогоняет\" DBSCAN кластеризацию \n",
    "#               с параметрами eps и min_samples\n",
    "#               примающими значения из заданного диапазона.\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data - dataframe для кластеризации\n",
    "#           2. r1 - начало диапазона\n",
    "#           3. r2 - конец диапазона  \n",
    "#           4. n - предпалагамое число кластеров      \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. data_cluster - кластеризация данных при различных \n",
    "#       значениях параметров eps и min_samples\n",
    "\n",
    "# Описание локальных переменных функции:\n",
    "# 1. parametr_range - диапазон изменения параметров\n",
    "# 2. dataframe_columns - колонки в результирующем dataframe\n",
    "# 3. data_cluster - результрующий dataframe\n",
    "# 4. index_cluster - текущая позиция в data_cluster\n",
    "# 5. clustering - кластеризатор\n",
    "# 6. list_cluster_values - список для заполнения текущими \n",
    "#                          значениями data_cluster\n",
    "\n",
    "# обьявлем функцию\n",
    "def search_DBSCAN_parameters(dataframe,r1,r2,n=3):\n",
    "    # задаем диапозон измениния параметров\n",
    "    parameter_range = range(r1,r2)\n",
    "    # формируем заготовку для результирующего dataframe\n",
    "    dataframe_columns = ['eps','min_samples',-1,0,1]\n",
    "    # проверим что задано не меньше минимального количества кластеров\n",
    "    if n<=3: \n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    else: \n",
    "        for claster in range(4,n+1):\n",
    "            dataframe_columns.append(claster-2)\n",
    "        data_cluster = pd.DataFrame(columns=dataframe_columns)\n",
    "    # задаем начально значение индекса в data_cluster\n",
    "    index_cluster = 0\n",
    "\n",
    "    # для подсчета обьектов в кластерах создадим dataframe\n",
    "    dataframe_count = pd.DataFrame()\n",
    "    \n",
    "    # \"прогоняем\" DBSCAN кластеризациию по диапазону параметров\n",
    "    for eps in parameter_range:\n",
    "        \n",
    "        for min_samples in parameter_range:\n",
    "            print('current eps:',eps,'  current min_samples:', min_samples, end='\\r')\n",
    "            # запускаем кластеризацию с текущими параметрами\n",
    "            clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(dataframe)\n",
    "            # добавлем к данным столбец с разметкой\n",
    "            dataframe_count['clater'] = clustering.labels_\n",
    "            # формируем пустой список для заполнения\n",
    "            list_cluster_values = []\n",
    "            # добавлеям в список текущие параметры\n",
    "            list_cluster_values.append(eps)\n",
    "            list_cluster_values.append(min_samples)\n",
    "            # добавлем в список количество обьктов в каждом кластере\n",
    "            for column in dataframe_columns[2:]:\n",
    "                list_cluster_values.append(len(dataframe_count['clater'][dataframe_count['clater']==column]))\n",
    "                \n",
    "            # заполняем dataframe  текущими данными\n",
    "            data_cluster.loc[index_cluster] = list_cluster_values\n",
    "            index_cluster +=1\n",
    "            # сбрасываем dataframe_count\n",
    "            dataframe_count = pd.DataFrame()\n",
    "    return data_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция generate_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция generate_samples\n",
    "\n",
    "# Назначение: для генерации индексов выборок данных\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. max - определяет максимальное значение множества\n",
    "#               из которого формируются выборки\n",
    "#           2. n - количество выборок\n",
    "#           3. k - мощность одной выборки\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список с выбороками\n",
    "\n",
    "# алгоритм работы:\n",
    "# 1. задаем отрезок натурального ряда N мощностью max и добавлем в него 0.\n",
    "#       In = N U {0}, I = {0,1,2,3,4,5,..,max}\n",
    "# 2. если мощность множества In больше, необходимого количества элементов\n",
    "#       cardo(In) > n x k , то из множества In формируем n случайных выборок\n",
    "# размера k без повторения.\n",
    "# 3. если мощность множества In меньше, необходимого количества элементов\n",
    "#       cardo(In) < n x k , то из множества In формируем случайные выборки\n",
    "# размера k без повторения, до тех пор пока не закончится множество In.\n",
    "# После, добираем недостающее количество выборок случайными выборками \n",
    "# размера k из множества In с повторением (bootstrap метод).\n",
    "\n",
    "\n",
    "# обьявлем функцию\n",
    "def generate_samples(max,n,k,random_state = None):\n",
    "    # создадим список под результат\n",
    "    samples_list = []\n",
    "    # формируем множество натуральных числе от 0 до max\n",
    "    In = list(range(max+1))\n",
    "    # Будем выполнять код пока не наберем необходимого количества выборок\n",
    "    # нарушим порядок в множестве\n",
    "    In = shuffle(In,random_state=random_state)\n",
    "    # random.shuffle(In)\n",
    "\n",
    "    # задаим границы извлечения данных из In\n",
    "    In_start = 0\n",
    "    In_end = k\n",
    "    while len(samples_list) < n:\n",
    "        # сформируем список под одну выборку\n",
    "        sample = []\n",
    "        # первые списки будем наполнять значениеми из множества In\n",
    "        # без повторения, до тех пор пока все значения из множества In\n",
    "        # не распределяться по выборкам\n",
    "        if len(In)-In_end >= 0:\n",
    "            sample.extend(In[In_start:In_end])\n",
    "        else:                    \n",
    "            # если элементов во множестве In недостаточно,\n",
    "            # запоняем выборку \"остатками\" \n",
    "            sample.extend(In[In_start:])\n",
    "\n",
    "            # остальные данные заполняем методом bootstrap\n",
    "            # выполнем код пока не заполним выборку k значениями\n",
    "            while len(sample) < k:\n",
    "                # генерируем случайное число из диапазона от 0 до len(In)-1\n",
    "                random_index = randint(0,len(In)-1)\n",
    "                # добавляем значение из множества In с индексом random_index\n",
    "                # в список index_list\n",
    "                sample.append(In[random_index])\n",
    "\n",
    "        # после того как мы набрали значения в выборку отправлем ее в samples_list\n",
    "        samples_list.append(sample)\n",
    "        # переходим к следующим данным в множестве In\n",
    "        In_start+=k\n",
    "        In_end+=k\n",
    "\n",
    "    return samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция my_train_test_split</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X,y,random_state=42,train_size=0.8,):\n",
    "    # если разбиение без стратификации\n",
    "\n",
    "    # зададим число элементов в выборке train\n",
    "    len_train = round(len(y)*train_size)\n",
    "    # формируем множество натуральных чисел от 0 до max\n",
    "    list_random_index = list(range(len(y)))\n",
    "    # нарушим порядок в множестве\n",
    "    list_random_index = shuffle(list_random_index,random_state=random_state)\n",
    "    # формируем список индексов под train выборку\n",
    "    train_samples = list_random_index[:len_train]\n",
    "    # формируем список индексов под test выборку\n",
    "    test_samples = list_random_index[len_train:]\n",
    "    # выполнем код пока не заполним выборку k значениями\n",
    "   \n",
    "    X_train = X.iloc[train_samples]\n",
    "    y_train = y.iloc[train_samples]\n",
    "    X_test = X.iloc[test_samples]\n",
    "    y_test = y.iloc[test_samples]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RoyalBlue\">функция class_1_percent_samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция class_1_percent_samples\n",
    "\n",
    "# Назначение: для генерации индексов сбалансированных выборок\n",
    "\n",
    "# Внешние переменные функции: \n",
    "#           1. data_target - массив из id и значений класса\n",
    "#           2. class_1_percent - процент класса 1 в результирующей выборке\n",
    "#           3. random_state - параметр для обеспечения воспроизваодимости функции\n",
    "     \n",
    "\n",
    "# Результат работы функции: \n",
    "# 1. samples_list - список со сблансированными выбороками\n",
    "\n",
    "# обьявлем функцию\n",
    "def class_1_percent_samples(data_target,class_1_percent,random_state = None):\n",
    "    # приведем данные к нужной форме\n",
    "    data_target = pd.DataFrame(data=np.array(data_target),columns =['id','flag'])\n",
    "    \n",
    "    # разделим клиентов  по признаку flag\n",
    "    flag_0 = data_target[data_target['flag']==0].reset_index(drop=True)\n",
    "    flag_1 = data_target[data_target['flag']==1].reset_index(drop=True)\n",
    "\n",
    "    # определим класс большинства\n",
    "    if flag_1.shape[0] > flag_0.shape[0]:\n",
    "        majority_class = flag_1\n",
    "        minority_class = flag_0\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(class_1_percent)/(1-class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    else:\n",
    "        majority_class = flag_0\n",
    "        minority_class = flag_1\n",
    "        # расчитаем необходимую величину выборки majority_class\n",
    "        majority_class_size = round(minority_class.shape[0]*(1-class_1_percent)/(class_1_percent))\n",
    "        # с помощью функции generate_samples сформируем выборку для majority_class\n",
    "        samples_majority_class= generate_samples(majority_class.shape[0]-1,1,majority_class_size,random_state=random_state)\n",
    "    \n",
    "    # сформируем список выбороки с заданным процентом класс 1\n",
    "    samples_list_id = minority_class['id'].values.tolist()+majority_class['id'].iloc[samples_majority_class[0]].tolist()\n",
    "\n",
    "    return samples_list_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DeepSkyBlue\">Процесс машинного обучения (ML-Machine Learning)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи в рамках Machine Learning:\n",
    "1. Для решения задачи построем блендинг моделей. \n",
    "\n",
    "2. В качестве базовых и метамоделей рассмотрим следующие классические модели классификации:\n",
    "    - linear_model.LogisticRegression (Логистическая регрессия);\n",
    "    - RandomForestClassifier (Деревья решений);\n",
    "    - HistGradientBoostingClassifier (Градиентный бустинг).\n",
    "\n",
    "3. В результате, преобразования данных было получено два пространства признаков (torow и stat признаки),  \n",
    "состоящих из 6 подпространств:\n",
    "    - date features;\n",
    "    - late payments features; \n",
    "    - credit features;\n",
    "    - relative features;\n",
    "    - payments features;\n",
    "    - service features.\n",
    "\n",
    "4. На первом этапе построения потроения блендинга, сфокусируем обучение базовых моделей,   \n",
    "на каждом подпространстве в отдельности друго от друга.  \n",
    "\n",
    "5. На втором этапе построения блендинга, обучим несколько групп метамоделей.   \n",
    "Первая группа метамоделей в качестве метапризнаков использует предсказания базовых моделей,    \n",
    "обученных на пространстве признаков torow.  \n",
    "Вторая группа метамоделей в качестве метапризнаков использует предсказания базовых моделей,    \n",
    "обученных на пространстве признаков stat.\n",
    "\n",
    "6. На третьем этапе построения блендинга метамодель обучится на метапризнаках пространства \n",
    "torow и stat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\Blending.jpg\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:DodgerBlue\">Сбор результирующей модели в pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся модель блендинга делится на несколько частей:\n",
    "1. Преобразование исходных данных до признаков base models;\n",
    "2. Преобразование результатов предсказания base models до метапризнаков firstmeta models;\n",
    "3. Преобразование результатов предсказания firstmeta models до метапризнаков secondmeta models;\n",
    "4. Предсказание результирующей модели на метапризнаках от secondmeta models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для преобразования данных от исходных  \n",
    "# до признаков base models\n",
    "\n",
    "def to_base_model(dataset):\n",
    "    # сформируем списки признаков каждого подпространства\n",
    "    date_features = ['id','rn','pre_since_opened','pre_since_confirmed','pre_pterm','pre_fterm',\n",
    "                    'pre_till_pclose','pre_till_fclose','pclose_flag','fclose_flag']\n",
    "    late_features = ['id','rn','pre_loans5','pre_loans530','pre_loans3060','pre_loans6090',\n",
    "                    'pre_loans90','is_zero_loans5','is_zero_loans530','is_zero_loans3060',\n",
    "                    'is_zero_loans6090','is_zero_loans90','pre_loans_total_overdue','pre_loans_max_overdue_sum']\n",
    "    credit_features = ['id','rn','pre_loans_credit_limit','pre_loans_next_pay_summ','pre_loans_outstanding','pre_loans_credit_cost_rate']\n",
    "\n",
    "    relative_features = ['id','rn','pre_util','pre_over2limit','pre_maxover2limit','is_zero_util',\n",
    "                    'is_zero_over2limit','is_zero_maxover2limit']\n",
    "\n",
    "    payments_features = ['id','rn','enc_paym_0','enc_paym_1','enc_paym_2','enc_paym_3','enc_paym_4','enc_paym_5','enc_paym_6',\n",
    "                            'enc_paym_7','enc_paym_8','enc_paym_9','enc_paym_10','enc_paym_11','enc_paym_12','enc_paym_13',\n",
    "                            'enc_paym_14', 'enc_paym_15', 'enc_paym_16', 'enc_paym_17','enc_paym_18','enc_paym_19','enc_paym_20',\n",
    "                            'enc_paym_21','enc_paym_22', 'enc_paym_23','enc_paym_24']\n",
    "\n",
    "    service_features = ['id','rn','enc_loans_account_holder_type','enc_loans_credit_status','enc_loans_credit_type','enc_loans_account_cur']\n",
    "\n",
    "    # составим словарь небходимых данных и их признаков\n",
    "    dict_torow_data = {\n",
    "        'date_torow' : date_features,\n",
    "        'late_torow' : late_features,\n",
    "        'credit_torow' : credit_features,\n",
    "        'relative_torow' : relative_features,\n",
    "        'payments_torow': payments_features,\n",
    "        'service_torow': service_features}\n",
    "    # сформируем данные за последние 25 операций клиентов\n",
    "    for space_features in dict_torow_data.keys():\n",
    "        clear_output()\n",
    "        # добавим индекацию процесса\n",
    "        print('Current space features:',space_features)\n",
    "        # сформируем данные для преобразования\n",
    "        data_to_transform = dataset[dict_torow_data[space_features]]\n",
    "        print('Start transform')\n",
    "        data_torow = torow_transformer.transform({'data':data_to_transform,'n_last':25})[0]\n",
    "        # сохраним преобразованные данные на диск для быстрого воспроизведения\n",
    "        print('Start save')\n",
    "        fp.write('sample_submission/pipeline/'+space_features,data_torow)\n",
    "        # удалим использованные данные\n",
    "        del data_to_transform\n",
    "        gc.collect()\n",
    "\n",
    "    # составим словарь небходимых данных и их признаков\n",
    "    dict_stat_data = {\n",
    "        'date_stat' : date_features,\n",
    "        'late_stat' : late_features,\n",
    "        'credit_stat' : credit_features,\n",
    "        'relative_stat' : relative_features,\n",
    "        'payments_stat': payments_features,\n",
    "        'service_stat': service_features}\n",
    "    # сформируем данные за последние 25 операций клиентов\n",
    "    for space_features in dict_stat_data.keys():\n",
    "        clear_output()\n",
    "        # добавим индекацию процесса\n",
    "        print('Current space features:',space_features)\n",
    "        # сформируем данные для преобразования\n",
    "        data_to_transform = dataset[dict_stat_data[space_features]]\n",
    "        print('Start transform')\n",
    "        data_stat = stat_transformer.transform(data_to_transform)[0]\n",
    "        # сохраним преобразованные данные на диск для быстрого воспроизведения\n",
    "        print('Start save')\n",
    "        fp.write('sample_submission/pipeline/'+space_features,data_stat)\n",
    "        # удалим использованные данные\n",
    "        del data_to_transform, data_stat\n",
    "        gc.collect()\n",
    "    \n",
    "    # сформируем словарь из данных отложенных для тестирования модели\n",
    "    dict_datasets = {\n",
    "    'date_torow': fp.ParquetFile('sample_submission/pipeline/date_torow').to_pandas().set_index('id'),\n",
    "    'late_torow': fp.ParquetFile('sample_submission/pipeline/late_torow').to_pandas().set_index('id'),\n",
    "    'credit_torow': fp.ParquetFile('sample_submission/pipeline/credit_torow').to_pandas().set_index('id'),\n",
    "    'relative_torow': fp.ParquetFile('sample_submission/pipeline/relative_torow').to_pandas().set_index('id'),\n",
    "    'payments_torow': fp.ParquetFile('sample_submission/pipeline/payments_torow').to_pandas().set_index('id'),\n",
    "    'service_torow': fp.ParquetFile('sample_submission/pipeline/service_torow').to_pandas().set_index('id'),\n",
    "    'date_stat': fp.ParquetFile('sample_submission/pipeline/date_stat').to_pandas().set_index('id'),\n",
    "    'late_stat': fp.ParquetFile('sample_submission/pipeline/late_stat').to_pandas().set_index('id'),\n",
    "    'credit_stat': fp.ParquetFile('sample_submission/pipeline/credit_stat').to_pandas().set_index('id'),\n",
    "    'relative_stat': fp.ParquetFile('sample_submission/pipeline/relative_stat').to_pandas().set_index('id'),\n",
    "    'payments_stat': fp.ParquetFile('sample_submission/pipeline/payments_stat').to_pandas().set_index('id'),\n",
    "    'service_stat': fp.ParquetFile('sample_submission/pipeline/service_stat').to_pandas().set_index('id'),\n",
    "    }\n",
    "    return dict_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для преобразования данных от признаков base models\n",
    "# до метапризнаков first metamodels\n",
    "\n",
    "def to_first_meta(dict_datasets):\n",
    "  # формируем пространство для метапризнаков\n",
    "  list_spaces = ['date','late','credit','relative','payments','service']\n",
    "  torow_models = ['LRTR','RFTR','GBTR']\n",
    "  stat_models = ['LRST','RFST','GBST']\n",
    "  \n",
    "  # фомируем заготовки для результитрующих данных\n",
    "  features_first_meta_torow = pd.DataFrame(index=dict_datasets['credit_torow'].index)\n",
    "  features_first_meta_stat = pd.DataFrame(index=dict_datasets['credit_torow'].index)\n",
    "\n",
    "  # указываем путь к директории в которой находятся обученные base модели\n",
    "  directory = \"models/base/\"\n",
    "  # Получаем список моделей в директории\n",
    "  list_models = os.listdir(directory)\n",
    "\n",
    "  # формируем метапризнаки данных torow\n",
    "  for space in list_spaces:\n",
    "    for type_model in torow_models:\n",
    "      # для текущего подбпространства формируем список предобученных моделей \n",
    "      list_base_models = sorted([x for x in list_models if (re.search(space, x))and(re.search(type_model, x))])\n",
    "      # делаем предсказание на каждой предобученной модели\n",
    "      for name_model in list_base_models:\n",
    "        # добавим индекацию процесса\n",
    "        clear_output()\n",
    "        print('Current feature: torow')\n",
    "        print('Current space: ', space)\n",
    "        print('Current type_model: ', type_model)\n",
    "        print('Current name_model: ', name_model)\n",
    "        # загружаем предобученную модель\n",
    "        model_classifier = joblib.load('models/base/'+name_model)\n",
    "        # загружаем list_n_last_features\n",
    "        list_n_last_features = joblib.load('models/base/list_n_last_features'+'list_n_last_features_'+type_model+'_'+name_model[5:-7]+'.joblib')\n",
    "        if type_model=='LRTR':\n",
    "          # загужаемый предобученный scaler\n",
    "          scaler = joblib.load('models/base/scalers/'+'scaler_torow_'+name_model[5:-7]+'.joblib')\n",
    "          features_first_meta_torow[name_model[:-7]] = model_classifier.predict_proba(scaler.transform(dict_datasets[space+'_torow'].to_numpy()[:,list_n_last_features]))[:,1]\n",
    "        else:\n",
    "          features_first_meta_torow[name_model[:-7]] = model_classifier.predict_proba(dict_datasets[space+'_torow'].to_numpy()[:,list_n_last_features])[:,1]\n",
    "\n",
    "    del dict_datasets[space+'_torow']\n",
    "    gc.collect()\n",
    "\n",
    "  # формируем метапризнаки данных stat\n",
    "  for space in list_spaces:\n",
    "    for type_model in stat_models:\n",
    "      # для текущего подбпространства формируем список предобученных моделей \n",
    "      list_base_models = sorted([x for x in list_models if (re.search(space, x))and(re.search(type_model, x))])\n",
    "      # делаем предсказание на каждой предобученной модели\n",
    "      for name_model in list_base_models:\n",
    "        # добавим индекацию процесса\n",
    "        clear_output()\n",
    "        print('Current feature: stat')\n",
    "        print('Current space: ', space)\n",
    "        print('Current type_model: ', type_model)\n",
    "        print('Current name_model: ', name_model)\n",
    "        # загружаем предобученную модель\n",
    "        model_classifier = joblib.load('models/base/'+name_model)\n",
    "        # загружаем list_ncorr_features\n",
    "        list_ncorr_features = joblib.load('models/base/list_ncorr_features/'+'list_ncorr_features_'+type_model+'_'+name_model[5:-7]+'.joblib')\n",
    "        # загружаем \n",
    "        if type_model=='LRST':\n",
    "          # загужаемый предобученный scaler\n",
    "          scaler = joblib.load('models/base/scalers/'+'scaler_stat_'+name_model[5:-7]+'.joblib')\n",
    "          features_first_meta_stat[name_model[:-7]] = model_classifier.predict_proba(scaler.transform(dict_datasets[space+'_stat'][list_ncorr_features]))[:,1]\n",
    "        else:\n",
    "          features_first_meta_stat[name_model[:-7]] = model_classifier.predict_proba(dict_datasets[space+'_stat'][list_ncorr_features])[:,1]\n",
    "\n",
    "    del dict_datasets[space+'_stat']\n",
    "    gc.collect()\n",
    "\n",
    "  dict_meta_first = {\n",
    "    'features_first_meta_torow': features_first_meta_torow,\n",
    "    'features_first_meta_stat': features_first_meta_stat\n",
    "    }\n",
    "\n",
    "  return dict_meta_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для преобразования данных от метапризнаков first metamodels\n",
    "# до метапризнаков second metamodels\n",
    "\n",
    "def to_second_meta(dict_datasets):\n",
    "  # формируем пространство для метапризнаков\n",
    "  space_models = ['LRTR','RFTR','GBTR','LRST','RFST','GBST']\n",
    "  \n",
    "  # фомируем заготовку для результитрующих данных\n",
    "  features_second_meta = pd.DataFrame(index=dict_datasets['features_first_meta_stat'].index)\n",
    "\n",
    "  # указываем путь к директории в которой находятся обученные base модели\n",
    "  directory = \"models/firstmeta/\"\n",
    "  # Получаем список моделей в директории\n",
    "  list_models = os.listdir(directory)\n",
    "\n",
    "  # формируем метапризнаки данных torow\n",
    "\n",
    "  for model in space_models:\n",
    "    # для текущего подбпространства формируем список предобученных моделей \n",
    "    list_first_meta_models = sorted([x for x in list_models if re.search(model, x)])\n",
    "    # выберем неободимый dataset\n",
    "    choose_dataset_name = lambda torow, stat, model: torow if model[-2:] == 'TR' else stat\n",
    "    dataset_name = choose_dataset_name('features_first_meta_torow','features_first_meta_stat',model)\n",
    "\n",
    "    # делаем предсказание на каждой предобученной модели\n",
    "    for name_model in list_first_meta_models:\n",
    "      # добавим индекацию процесса\n",
    "      clear_output()\n",
    "      print('Current name_model: ', name_model)\n",
    "      # загружаем предобученную модель\n",
    "      model_classifier = joblib.load('models/firstmeta/'+name_model)\n",
    "      # загружаем list_best_features\n",
    "      list_best_features = joblib.load('features/firstmeta/list_best_features/'+'list_best_features_'+name_model[:-7]+'.joblib')\n",
    "      # запоняем dataframe мета признаками \n",
    "      features_second_meta[name_model[:-7]] = model_classifier.predict_proba(dict_datasets[dataset_name][list_best_features].to_numpy())[:,1]\n",
    "\n",
    "  return features_second_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для преобразования данных от метапризнаков second metamodels\n",
    "# до предсказания результирующей метамодели\n",
    "\n",
    "def best_model(data):\n",
    "  # загрузим загрузим модель показавшую наилучший результат\n",
    "  best_classifier = joblib.load('models/secondmeta/best_model.joblib')\n",
    "  # загрузим selector для лучшей модели\n",
    "  best_selector = joblib.load('models/secondmeta/selector/best_selector.joblib')\n",
    "  list_best_features = best_selector.get_feature_names_out()\n",
    "\n",
    "  # формируем данные для предсказания\n",
    "  MX_data = data[list_best_features].to_numpy()\n",
    "\n",
    "  # для метрик ROC AUC делаем предсказание модели в виде вероятности\n",
    "  my_pred_proba = best_classifier.predict_proba(MX_data)[:,1]\n",
    "\n",
    "  dataframe = pd.DataFrame(\n",
    "    data=np.array([data.index,my_pred_proba]).transpose(),\n",
    "    columns=['id','score'])\n",
    "  dataframe['id'] = dataframe['id'].astype('int')\n",
    "\n",
    "  dataframe.to_csv('prediction/prediction.csv',index=False)\n",
    "  \n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blendingClassifier(data):\n",
    "  # выполним преобразования для предсказания на базовых моделях \n",
    "  base_model_data = to_base_model(data)\n",
    "  del data\n",
    "  gc.collect()\n",
    "\n",
    "  # выполним преобразования для предсказания на метамоделях первого порядка \n",
    "  first_meta_data = to_first_meta(base_model_data)\n",
    "  del base_model_data\n",
    "  gc.collect()\n",
    "\n",
    "  # выполним преобразования для предсказания на метамоделях второго порядка \n",
    "  second_meta_data = to_second_meta(first_meta_data)\n",
    "  del first_meta_data\n",
    "\n",
    "  # выполним предсказание на результирующей метамодели \n",
    "  gc.collect()\n",
    "  prediction = best_model(second_meta_data)\n",
    "  \n",
    "  # вывод сообщения с директорией сохранненого файла\n",
    "  print('prediction saved to: prediction/prediction.csv')\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь функционал блендинга \"упакован\" в файл  $blending.py$.  \n",
    "Для выполнения предсказания на \"сырых\" данных, из файла $blending.py$,   \n",
    "необходимо импортировать функцию $blendingClassifier$.\n",
    "\n",
    "Процедура импорта выполнена в разделе $ИСПОЛЬЗУЕМЫЕ$ $БИБЛИОТЕКИ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:DodgerBlue\">Проверка модели на соревновательном dataset с kaggle</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузим данные о кредитной истории клиентов\n",
    "kd0 = pd.read_parquet('data_source/test_data_for_Kaggle/test_data_0.pq')\n",
    "kd1 = pd.read_parquet('data_source/test_data_for_Kaggle/test_data_1.pq')\n",
    "# Обьединим данные в один датасет\n",
    "kaggle_data = pd.concat([kd0,kd1],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current name_model:  GBST19.joblib\n",
      "prediction saved to: prediction/prediction.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000000</td>\n",
       "      <td>0.504303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000001</td>\n",
       "      <td>0.532028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000002</td>\n",
       "      <td>0.277015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000003</td>\n",
       "      <td>0.302926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000004</td>\n",
       "      <td>0.249442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3000005</td>\n",
       "      <td>0.395355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000006</td>\n",
       "      <td>0.326256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3000007</td>\n",
       "      <td>0.200252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000008</td>\n",
       "      <td>0.556577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3000009</td>\n",
       "      <td>0.147515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     score\n",
       "0  3000000  0.504303\n",
       "1  3000001  0.532028\n",
       "2  3000002  0.277015\n",
       "3  3000003  0.302926\n",
       "4  3000004  0.249442\n",
       "5  3000005  0.395355\n",
       "6  3000006  0.326256\n",
       "7  3000007  0.200252\n",
       "8  3000008  0.556577\n",
       "9  3000009  0.147515"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = blendingClassifier(kaggle_data)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним результат\n",
    "sample_submission.to_csv('sample_submission/sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000000</td>\n",
       "      <td>0.504303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000001</td>\n",
       "      <td>0.532028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000002</td>\n",
       "      <td>0.277015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000003</td>\n",
       "      <td>0.302926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000004</td>\n",
       "      <td>0.249442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3000005</td>\n",
       "      <td>0.395355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000006</td>\n",
       "      <td>0.326256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3000007</td>\n",
       "      <td>0.200252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000008</td>\n",
       "      <td>0.556577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3000009</td>\n",
       "      <td>0.147515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     score\n",
       "0  3000000  0.504303\n",
       "1  3000001  0.532028\n",
       "2  3000002  0.277015\n",
       "3  3000003  0.302926\n",
       "4  3000004  0.249442\n",
       "5  3000005  0.395355\n",
       "6  3000006  0.326256\n",
       "7  3000007  0.200252\n",
       "8  3000008  0.556577\n",
       "9  3000009  0.147515"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим корректность сохранненного файла\n",
    "sample_submission = pd.read_csv('sample_submission/sample_submission.csv')\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат модели на соревновательном dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = \"img\\kaggle.png\" alt=\"drawing\" style=\"width:1400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:DeepSkyBlue\"> Выводы</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Blue\">\n",
    "\n",
    "Качество полученной модели бленндинга по метрике $ROC AUC$:\n",
    "- на тренировочном наборе: $ROC AUC = 0.757$;\n",
    "- на валидационном наборе: $ROC AUC = 0.765$;\n",
    "- на отложенном наборе: $ROC AUC = 0.756$;\n",
    "- на соревновательном (kaggle) наборе: $ROC AUC = 0.743$;\n",
    "\n",
    "Мероприятия по возможному улучшению качества модели:\n",
    "\n",
    "1. Проанализировать данные на наличие выбросов и нерепрезентотивных данных. Скорее всего   \n",
    "есть клиенты, данные которых \"вводят в заблуждение\" модель.  \n",
    "\n",
    "2. Проанализировать влияние выбранных статистических характеристик, формирующих dataset STAT.  \n",
    "Возможно, использование некоторых характеристики наоборот ухудшает качество модели.   \n",
    "Возможно, есть иные характеристики числовых рядов которые были не учтены.  \n",
    "\n",
    "3. Проанализирровать влияние размера обучающего набора на каждом этапе на качество результирующей модели.   \n",
    "Возможно имеет смысл большую часть данных \"потратить\" на обучение базовых моделей, возможно наоборот.  \n",
    "\n",
    "4. Проанализировать влияние подпространств признаков на результаты first metamodels.  \n",
    "Некоторые подпространства показали низкое качество на базовых моделеях, Возможно имеет смысл    \n",
    "не использовать все подпространства признаков или разделить пространство признаков не на 6 выбранных     \n",
    "подпространств, а с помощью методов класстеризации.   \n",
    "\n",
    "5. Изучить вопрос выбранных базовых моделей. Возможно, имеет смысл сменить их или использовать не все.  \n",
    "В проссе обучения на первом этапе, модель логистической регресии показала наихудшие результаты.\n",
    "\n",
    "6. Проанализировать подбор гиперпараметров каждой модели. Сменить диапозоны параметров,  \n",
    "выбрать дополнительные гиперпермараметры и т.д.\n",
    "\n",
    "7. Сменить критерии \"лучших\" моделей. Возможно, имеет смысл использовать только один критерий для  \n",
    "определения \"лучших\" моделей. В построенном блендинге на каждом этапе использовались все \"лучшие\"   \n",
    "модели с предыдущего этапа. Возможно, есть смысл показывать метамодели результаты не от всех \"лучших\" моделей.\n",
    "\n",
    "8. Увеличить или уменьшить глубину блендинга.  \n",
    "\n",
    "9. Использовать не блендинг, а стекинг моделей.  \n",
    "\n",
    "10. Использовать не блендинг, а нейронную сеть.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
